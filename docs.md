# Table of Contents

- [ElevenLabs docs — ElevenLabs Documentation](#elevenlabs-docs-elevenlabs-documentation)
- [Developer quickstart — ElevenLabs Documentation](#developer-quickstart-elevenlabs-documentation)
- [Models — ElevenLabs Documentation](#models-elevenlabs-documentation)
- [January 16, 2025 — ElevenLabs Documentation](#january-16-2025-elevenlabs-documentation)
- [December 19, 2024 — ElevenLabs Documentation](#december-19-2024-elevenlabs-documentation)
- [October 27, 2024 — ElevenLabs Documentation](#october-27-2024-elevenlabs-documentation)
- [October 13, 2024 — ElevenLabs Documentation](#october-13-2024-elevenlabs-documentation)
- [December 3, 2024 — ElevenLabs Documentation](#december-3-2024-elevenlabs-documentation)
- [February 4, 2025 — ElevenLabs Documentation](#february-4-2025-elevenlabs-documentation)
- [January 2, 2025 — ElevenLabs Documentation](#january-2-2025-elevenlabs-documentation)
- [December 6, 2024 — ElevenLabs Documentation](#december-6-2024-elevenlabs-documentation)
- [October 20, 2024 — ElevenLabs Documentation](#october-20-2024-elevenlabs-documentation)
- [February 17, 2025 — ElevenLabs Documentation](#february-17-2025-elevenlabs-documentation)
- [November 29, 2024 — ElevenLabs Documentation](#november-29-2024-elevenlabs-documentation)
- [February 10, 2025 — ElevenLabs Documentation](#february-10-2025-elevenlabs-documentation)
- [Voice changer — ElevenLabs Documentation](#voice-changer-elevenlabs-documentation)
- [Dubbing — ElevenLabs Documentation](#dubbing-elevenlabs-documentation)
- [January 27, 2025 — ElevenLabs Documentation](#january-27-2025-elevenlabs-documentation)
- [Voice isolator — ElevenLabs Documentation](#voice-isolator-elevenlabs-documentation)
- [Voices — ElevenLabs Documentation](#voices-elevenlabs-documentation)
- [Sound effects — ElevenLabs Documentation](#sound-effects-elevenlabs-documentation)
- [Create sound effects from text — ElevenLabs Documentation](#create-sound-effects-from-text-elevenlabs-documentation)
- [Convert text to speech in real-time — ElevenLabs Documentation](#convert-text-to-speech-in-real-time-elevenlabs-documentation)
- [Using pronunciation dictionaries — ElevenLabs Documentation](#using-pronunciation-dictionaries-elevenlabs-documentation)
- [Changelog — ElevenLabs Documentation](#changelog-elevenlabs-documentation)
- [Latency optimization — ElevenLabs Documentation](#latency-optimization-elevenlabs-documentation)
- [Text to Speech — ElevenLabs Documentation](#text-to-speech-elevenlabs-documentation)
- [Stitching multiple requests — ElevenLabs Documentation](#stitching-multiple-requests-elevenlabs-documentation)
- [Sending generated audio through Twilio — ElevenLabs Documentation](#sending-generated-audio-through-twilio-elevenlabs-documentation)
- [Prompting — ElevenLabs Documentation](#prompting-elevenlabs-documentation)
- [Product guides overview — ElevenLabs Documentation](#product-guides-overview-elevenlabs-documentation)
- [Streaming text to speech — ElevenLabs Documentation](#streaming-text-to-speech-elevenlabs-documentation)
- [Dubbing audio — ElevenLabs Documentation](#dubbing-audio-elevenlabs-documentation)
- [Text to Speech (product guide) — ElevenLabs Documentation](#text-to-speech-product-guide-elevenlabs-documentation)
- [Sound effects (product guide) — ElevenLabs Documentation](#sound-effects-product-guide-elevenlabs-documentation)
- [Dubbing Overview — ElevenLabs Documentation](#dubbing-overview-elevenlabs-documentation)
- [Voice changer (product guide) — ElevenLabs Documentation](#voice-changer-product-guide-elevenlabs-documentation)
- [Studio overview — ElevenLabs Documentation](#studio-overview-elevenlabs-documentation)
- [Audio Native with Webflow — ElevenLabs Documentation](#audio-native-with-webflow-elevenlabs-documentation)
- [Dubbing studio — ElevenLabs Documentation](#dubbing-studio-elevenlabs-documentation)
- [Voice isolator & background sound remover (product guide) — ElevenLabs Documentation](#voice-isolator-background-sound-remover-product-guide-elevenlabs-documentation)
- [AI speech classifier — ElevenLabs Documentation](#ai-speech-classifier-elevenlabs-documentation)
- [Audio Native with Wix — ElevenLabs Documentation](#audio-native-with-wix-elevenlabs-documentation)
- [Account — ElevenLabs Documentation](#account-elevenlabs-documentation)
- [Billing — ElevenLabs Documentation](#billing-elevenlabs-documentation)
- [Voice design — ElevenLabs Documentation](#voice-design-elevenlabs-documentation)
- [Payouts — ElevenLabs Documentation](#payouts-elevenlabs-documentation)
- [Audio Native Overview — ElevenLabs Documentation](#audio-native-overview-elevenlabs-documentation)
- [Audio Native with Ghost — ElevenLabs Documentation](#audio-native-with-ghost-elevenlabs-documentation)
- [Workspaces — ElevenLabs Documentation](#workspaces-elevenlabs-documentation)
- [Libraries & SDKs — ElevenLabs Documentation](#libraries-sdks-elevenlabs-documentation)
- [User groups — ElevenLabs Documentation](#user-groups-elevenlabs-documentation)
- [Voice Cloning overview — ElevenLabs Documentation](#voice-cloning-overview-elevenlabs-documentation)
- [Instant Voice Cloning — ElevenLabs Documentation](#instant-voice-cloning-elevenlabs-documentation)
- [Audio Native with Squarespace — ElevenLabs Documentation](#audio-native-with-squarespace-elevenlabs-documentation)
- [Sharing resources — ElevenLabs Documentation](#sharing-resources-elevenlabs-documentation)
- [Single Sign-On (SSO) — ElevenLabs Documentation](#single-sign-on-sso-elevenlabs-documentation)
- [Error messages — ElevenLabs Documentation](#error-messages-elevenlabs-documentation)
- [Voice library — ElevenLabs Documentation](#voice-library-elevenlabs-documentation)
- [Audio Native with React — ElevenLabs Documentation](#audio-native-with-react-elevenlabs-documentation)
- [Professional Voice Cloning — ElevenLabs Documentation](#professional-voice-cloning-elevenlabs-documentation)
- [Conversational AI dashboard — ElevenLabs Documentation](#conversational-ai-dashboard-elevenlabs-documentation)
- [Zero retention mode (Enterprise) — ElevenLabs Documentation](#zero-retention-mode-enterprise-elevenlabs-documentation)
- [Troubleshooting — ElevenLabs Documentation](#troubleshooting-elevenlabs-documentation)
- [Audio Native with WordPress — ElevenLabs Documentation](#audio-native-with-wordpress-elevenlabs-documentation)
- [Audio Native with Framer — ElevenLabs Documentation](#audio-native-with-framer-elevenlabs-documentation)
- [Voiceover studio — ElevenLabs Documentation](#voiceover-studio-elevenlabs-documentation)
- [End call — ElevenLabs Documentation](#end-call-elevenlabs-documentation)

---

# ElevenLabs docs — ElevenLabs Documentation

## Most popular

[Developer quickstart\
\
Learn how to integrate ElevenLabs](/docs/quickstart)
[Conversational AI\
\
Deploy voice agents in minutes](/docs/conversational-ai/overview)
[Product guides\
\
Learn how to use ElevenLabs](/docs/product-guides/overview)
[API reference\
\
Dive into our API reference](/docs/api-reference/introduction)

## Meet the models

[Eleven Multilingual v2\
\
Our most lifelike, emotionally rich speech synthesis model\
\
Most natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Rich emotional expression](/docs/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](/docs/models#flash-v25)

[Explore all](/docs/models)

## Capabilities

[Text to Speech\
\
Convert text into lifelike speech](/docs/capabilities/text-to-speech)
[Voice changer\
\
Modify and transform voices](/docs/capabilities/voice-changer)
[Voice isolator\
\
Isolate voices from background noise](/docs/capabilities/voice-isolator)
[Dubbing\
\
Dub audio and videos seamlessly](/docs/capabilities/dubbing)
[Sound effects\
\
Create cinematic sound effects](/docs/capabilities/sound-effects)
[Voices\
\
Clone and design custom voices](/docs/capabilities/voices)
[Conversational AI\
\
Deploy intelligent voice agents](/docs/conversational-ai/overview)

## Product guides

[Product guides\
\
Explore our product guides for step-by-step guidance\
\
![Voice library](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-library.webp)](/docs/product-guides/overview)

† Excluding application & network latency

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Developer quickstart — ElevenLabs Documentation

The ElevenLabs API provides a simple interface to state-of-the-art audio [models](/docs/models)
and [features](/docs/api-reference/introduction)
. Follow this guide to learn how to create lifelike speech, generate and modify voices, produce immersive sound effects, isolate background noise from audio, and seamlessly dub audio/videos.

## Create an API key

[Create an API key in the dashboard here](https://elevenlabs.io/app/settings/api-keys)
, which you’ll use to securely [access the API](/docs/api-reference/authentication)
.

Store the key as a managed secret and pass it to the SDKs either as a environment variable via an `.env` file, or directly in your app’s configuration depending on your preference.

.env

`  |     |     | | --- | --- | | 1   | ELEVENLABS_API_KEY = 'your_api_key_here'; |    `

## Make your first request

You can either use the [REST API](/docs/api-reference/introduction)
directly with the HTTP client of your choice, or use one of our official SDKs as shown below. This guide will use the official SDKs to make requests.

PythonJavaScript

`  |     |     | | --- | --- | | $   | pip install elevenlabs |    `

To play the audio through your speakers, you may be prompted to install [MPV](https://mpv.io/)
and/or [ffmpeg](https://ffmpeg.org/)
.

###### Python

###### Javascript

The environment variables are loaded automatically when using the SDK, but we need to install the `python-dotenv` package to load them.

Install python-dotenv

`  |     |     | | --- | --- | | $   | pip install python-dotenv |    `

With the ElevenLabs SDK installed, create a file called `example.py` and copy one of the following examples into it:

###### Text to Speech

###### Voice changer

###### Sound effects

###### Voice isolator

###### Voice design

###### Dubbing

Convert text into life-like audio

`  |     |     | | --- | --- | | 1   | from dotenv import load_dotenv | | 2   | from elevenlabs.client import ElevenLabs | | 3   | from elevenlabs import play | | 4   |     | | 5   | load_dotenv() | | 6   |     | | 7   | client = ElevenLabs() | | 8   |     | | 9   | audio = client.text_to_speech.convert( | | 10  | text="The first move is what sets everything in motion.", | | 11  | voice_id="JBFqnCBsd6RMkjVDRZzb", | | 12  | model_id="eleven_multilingual_v2", | | 13  | output_format="mp3_44100_128", | | 14  | )   | | 15  |     | | 16  | play(audio) |    `

Execute the code with the command below. Within a few seconds you should hear the audio play through your speaker.

Run the script

`  |     |     | | --- | --- | | $   | python example.py |    `

To play the audio through your speakers, you may be prompted to install [MPV](https://mpv.io/)
and/or [ffmpeg](https://ffmpeg.org/)
.

## Next steps

Now that you’ve made your first ElevenLabs API request, you can explore the following resources:

[Tutorials\
\
Explore our developer guides](/docs/cookbooks)
[Text to Speech\
\
Turn text into lifelike spoken audio](/docs/capabilities/text-to-speech)
[Conversational AI\
\
Deploy conversational voice agents](/docs/conversational-ai/overview)
[API reference\
\
Dive into our API reference](/docs/api-reference/introduction)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Models — ElevenLabs Documentation

## Flagship models

[Eleven Multilingual v2\
\
Our most lifelike, emotionally rich speech synthesis model\
\
Most natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Rich emotional expression](/docs/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](/docs/models#flash-v25)

[Pricing](https://elevenlabs.io/pricing)

## Models overview

The ElevenLabs API offers a range of speech synthesis models optimized for different use cases, quality levels, and performance requirements.

| Model ID                     | Description                                                          | Languages                                                                                                                                                                     |
| ---------------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `eleven_multilingual_v2`     | Our most lifelike model with rich emotional expression               | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |
| `eleven_flash_v2_5`          | Ultra-fast model optimized for real-time use (~75ms†)                | All `eleven_multilingual_v2` languages plus: `hu`, `no`, `vi`                                                                                                                 |
| `eleven_flash_v2`            | Ultra-fast model optimized for real-time use (~75ms†)                | `en`                                                                                                                                                                          |
| `eleven_multilingual_sts_v2` | State-of-the-art multilingual voice changer model (Speech to Speech) | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |
| `eleven_english_sts_v2`      | English-only voice changer model (Speech to Speech)                  | `en`                                                                                                                                                                          |

###### Older Models

These models are maintained for backward compatibility but are not recommended for new projects.

| Model ID                 | Description                                                                 | Languages                                                                                                                                                                                       |
| ------------------------ | --------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `eleven_monolingual_v1`  | First generation TTS model (outclassed by v2 models)                        | `en`                                                                                                                                                                                            |
| `eleven_multilingual_v1` | First multilingual model (outclassed by v2 models)                          | `en`, `fr`, `de`, `hi`, `it`, `pl`, `pt`, `es`                                                                                                                                                  |
| `eleven_turbo_v2_5`      | High quality, low-latency model (~250ms-300ms) (outclassed by Flash models) | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru`, `hu`, `no`, `vi` |
| `eleven_turbo_v2`        | High quality, low-latency model (~250ms-300ms) (outclassed by Flash models) | `en`                                                                                                                                                                                            |

## Multilingual v2

Eleven Multilingual v2 is our most advanced, emotionally-aware speech synthesis model. It produces natural, lifelike speech with high emotional range and contextual understanding across multiple languages.

The model delivers consistent voice quality and personality across all supported languages while maintaining the speaker’s unique characteristics and accent.

This model excels in scenarios requiring high-quality, emotionally nuanced speech:

- **Audiobook Production**: Perfect for long-form narration with complex emotional delivery
- **Character Voiceovers**: Ideal for gaming and animation due to its emotional range
- **Professional Content**: Well-suited for corporate videos and e-learning materials
- **Multilingual Projects**: Maintains consistent voice quality across language switches

While it has a higher latency & cost per character than Flash models, it delivers superior quality for projects where lifelike speech is important.

## Flash v2.5

Eleven Flash v2.5 is our fastest speech synthesis model, designed for real-time applications and conversational AI. It delivers high-quality speech with ultra-low latency (~75ms†) across 32 languages.

The model balances speed and quality, making it ideal for interactive applications while maintaining natural-sounding output and consistent voice characteristics across languages.

This model is particularly well-suited for:

- **Conversational AI**: Perfect for real-time voice agents and chatbots
- **Interactive Applications**: Ideal for games and applications requiring immediate response
- **Large-Scale Processing**: Efficient for bulk text-to-speech conversion

With its lower price point and 75ms latency, Flash v2.5 is the cost-effective option for anyone needing fast, reliable speech synthesis across multiple languages.

## Model selection guide

###### Requirements

Quality

Use `eleven_multilingual_v2`

Best for high-fidelity audio output with rich emotional expression

Low-latency

Use Flash models

Optimized for real-time applications (~75ms latency)

Multilingual

Use either either `eleven_multilingual_v2` or `eleven_flash_v2_5`

Both support up to 32 languages

###### Use case

Content creation

Use `eleven_multilingual_v2`

Ideal for professional content, audiobooks & video narration.

Conversational AI

Use `eleven_flash_v2_5`, `eleven_flash_v2` or `eleven_multilingual_v2`

Perfect for real-time conversational applications

Voice changer

Use `eleven_multilingual_sts_v2`

Specialized for Speech-to-Speech conversion

### Supported languages

Our v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

## Character limits

The maximum number of characters supported in a single request varies by model.

| Model ID                 | Character limit | Approximate audio duration |
| ------------------------ | --------------- | -------------------------- |
| `eleven_flash_v2_5`      | 40,000          | ~40 minutes                |
| `eleven_flash_v2`        | 30,000          | ~30 minutes                |
| `eleven_multilingual_v2` | 10,000          | ~10 minutes                |
| `eleven_multilingual_v1` | 10,000          | ~10 minutes                |
| `eleven_english_sts_v2`  | 10,000          | ~10 minutes                |
| `eleven_english_sts_v1`  | 10,000          | ~10 minutes                |

For longer content, consider splitting the input into multiple requests.

## Concurrency and priority

Your subscription plan determines how many requests can be processed simultaneously and the priority level of your requests in the queue.

| Plan       | Concurrency limit | Priority level |
| ---------- | ----------------- | -------------- |
| Free       | 2                 | 3              |
| Starter    | 3                 | 4              |
| Creator    | 5                 | 5              |
| Pro        | 10                | 5              |
| Scale      | 15                | 5              |
| Business   | 15                | 5              |
| Enterprise | Elevated          | Highest        |

To increase your concurrency limit & queue priority, [upgrade your subscription plan](https://elevenlabs.io/pricing)
.

† Excluding application & network latency

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# January 16, 2025 — ElevenLabs Documentation

## Product

### Conversational AI

- **Additional languages**: Add a language dropdown to your widget so customers can launch conversations in their preferred language. Learn more [here](/docs/conversational-ai/customization/language)
  .
- **End call tool**: Let the agent automatically end the call with our new “End Call” tool. Learn more [here](/docs/conversational-ai/customization/tools)
- **Flash default**: Flash, our lowest latency model, is now the default for new agents. In your agent dashboard under “voice”, you can toggle between Turbo and Flash. Learn more about Flash [here](https://elevenlabs.io/blog/meet-flash)
  .
- **Privacy**: Set concurrent call and daily call limits, turn off audio recordings, add feedback collection, and define customer terms & conditions.
- **Increased tool limits**: Increase the number of tools available to your agent from 5 to 15. Learn more [here](/docs/conversational-ai/customization/tools)
  .

[January 2, 2025\
\
Up Next](/docs/changelog/2025/1/2)

---

# December 19, 2024 — ElevenLabs Documentation

## Model

- **Introducing Flash**: Our fastest text-to-speech model yet, generating speech in just 75ms. Access it via the API with model IDs `eleven_flash_v2` and `eleven_flash_v2_5`. Perfect for low-latency conversational AI applications. [Try it now](https://elevenlabs.io/docs/api-reference/text-to-speech)
  .

## Launches

- **[TalkToSanta.io](https://www.talktosanta.io/)
  **: Experience Conversational AI in action by talking to Santa this holiday season. For every conversation with santa we donate 2 dollars to [Bridging Voice](https://www.bridgingvoice.org/)
  (up to $11,000).
- **[AI Engineer Pack](https://aiengineerpack.com/)
  **: Get $50+ in credits from leading AI developer tools, including ElevenLabs.

[December 6, 2024\
\
Up Next](/docs/changelog/2024/12/6)

---

# October 27, 2024 — ElevenLabs Documentation

## API

- **u-law Audio Formats**: Added u-law audio formats to the Convai API for integrations with Twilio.
- **TTS Websocket Improvements**: TTS websocket improvements, flushes and generation work more intuitively now.
- **TTS Websocket Auto Mode**: A streamlined mode for using websockets. This setting reduces latency by disabling chunk scheduling and buffers. Note: Using partial sentences will result in significantly reduced quality.
- **Improvements to latency consistency**: Improvements to latency consistency for all models.

## Website

- **TTS Redesign**: The website TTS redesign is now in alpha!

[October 20, 2024\
\
Up Next](/docs/changelog/2024/10/20)

---

# October 13, 2024 — ElevenLabs Documentation

## Model

- **Stability Improvements**: Significant audio stability improvements across all models, most noticeable on `turbo_v2` and `turbo_v2.5`, when using:
  - Websockets
  - Projects
  - Reader app
  - TTS with request stitching
  - ConvAI
- **Latency Improvements**: Reduced time to first byte latency by approximately 20-30ms for all models.

## API

- **Remove Background Noise Voice Samples**: Added the ability to remove background noise from voice samples using our audio isolation model to improve quality for IVCs and PVCs at no additional cost.
- **Remove Background Noise STS Input**: Added the ability to remove background noise from STS audio input using our audio isolation model to improve quality at no additional cost.

## Feature

- **Conversational AI Beta**: Conversational AI is now in beta.

[Text to speech\
\
Up Next](/docs/capabilities/text-to-speech)

---

# December 3, 2024 — ElevenLabs Documentation

## API

- **Credit Usage Limits**: Set specific credit limits for API keys to control costs and manage usage across different use cases by setting “Access” or “No Access” to features like Dubbing, Audio Native, and more. [Check it out](https://elevenlabs.io/app/settings/api-keys)
- **Workspace API Keys**: Now support access permissions, such as “Read” or “Read and Write” for User, Workspace, and History resources.
- **Improved Key Management**:
  - Redesigned interface moving from modals to dedicated pages
  - Added detailed descriptions and key information
  - Enhanced visibility of key details and settings

[November 29, 2024\
\
Up Next](/docs/changelog/2024/11/29)

---

# February 4, 2025 — ElevenLabs Documentation

### Conversational AI

- **Agent monitoring**: Added a new dashboard for monitoring conversational AI agents’ activity. Check out your’s [here](/app/conversational-ai)
  .
- **Proactive conversations**: Enhanced capabilities with improved timeout retry logic. [Learn more](/docs/conversational-ai/customization/conversation-flow)
- **Tool calls**: Fixed timeout issues occurring during tool calls
- **Allowlist**: Fixed implementation of allowlist functionality.
- **Content summarization**: Added Gemini as a fallback model to ensure service reliability
- **Widget stability**: Fixed issue with dynamic variables causing the Conversational AI widget to fail

### Reader

- **Trending content**: Added carousel showcasing popular articles and trending content
- **New publications**: Introduced dedicated section for recent ElevenReader Publishing releases

### Studio (formerly Projects)

- **Projects is now Studio** and is now generally available to everyone
- **Chapter content editing**: Added support for editing chapter content through the public API, enabling programmatic updates to chapter text and metadata
- **GenFM public API**: Added public API support for podcast creation through GenFM. Key features include:
  - Conversation mode with configurable host and guest voices
  - URL-based content sourcing
  - Customizable duration and highlights
  - Webhook callbacks for status updates
  - Project snapshot IDs for audio downloads

### SDKs

- **Swift**: fixed an issue where resources were not being released after the end of a session
- **Python**: added uv support
- **Python**: fixed an issue where calls were not ending correctly

### API

###### View API changes

- Added POST `v1/workspace/invites/add-bulk` [endpoint](/docs/api-reference/workspace/invite-multiple-users)
  to enable inviting multiple users simultaneously
- Added POST `v1/projects/podcast/create` [endpoint](/docs/api-reference/projects/create-podcast)
  for programmatic podcast generation through GenFM
- Added ‘v1/convai/knowledge-base/:documentation_id’ [endpoints](/docs/api-reference/knowledge-base/)
  with CRUD operations for Conversational AI
- Added `v1/convai/tools` [tool management endpoints](/docs/api-reference/tools)
  for extending Conversational AI agent capabilities
- Added PATCH `v1/projects/:project_id/chapters/:chapter_id` [endpoint](/docs/api-reference/chapters/patch-chapter)
  for updating project chapter content and metadata
- Added `group_ids` parameter to [Workspace Invite endpoint](/docs/api-reference/workspace/invite-user)
  for group-based access control
- Added structured `content` property to [Chapter response objects](/docs/api-reference/chapters/get-chapter)
- Added `retention_days` and `delete_transcript_and_pii` data retention parameters to [Agent creation](/docs/api-reference/agents/create-agent)
- Added structured response to [AudioNative content](/docs/api-reference/audio-native/create#response.body.project_id)
- Added `convai_chars_per_minute` usage metric to [User endpoint](/docs/api-reference/user/get)
- Added `media_metadata` field to [Dubbing response objects](/docs/api-reference/dubbing/get)
- Added GDPR-compliant `deletion_settings` to [Conversation responses](/docs/api-reference/conversations/get-conversation#response.body.metadata.deletion_settings)
- Deprecated Knowledge Base legacy endpoints:
  - POST `/v1/convai/agents/{agent_id}/add-to-knowledge-base`
  - GET `/v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}`
- Updated Agent endpoints with consolidated [privacy control parameters](/docs/api-reference/agents/create-agent)

[January 27, 2025\
\
Up Next](/docs/changelog/2025/1/27)

---

# January 2, 2025 — ElevenLabs Documentation

## Product

- **Workspace Groups and Permissions**: Introduced new workspace group management features to enhance access control within organizations. [Learn more](https://elevenlabs.io/blog/workspace-groups-and-permissions)
  .

[December 19, 2024\
\
Up Next](/docs/changelog/2024/12/19)

---

# December 6, 2024 — ElevenLabs Documentation

## Product

- **GenFM Now on Web**: Access GenFM directly from the website in addition to the ElevenReader App, [try it now](https://elevenlabs.io/app/projects)
  .

[December 3, 2024\
\
Up Next](/docs/changelog/2024/12/3)

---

# October 20, 2024 — ElevenLabs Documentation

## API

- **Normalize Text with the API**: Added the option normalize the input text in the TTS API. The new parameter is called `apply_text_normalization` and works on all non-turbo & non-flash models.

## Product

- **Voice Design**: The Voice Design feature is now in beta!

[October 13, 2024\
\
Up Next](/docs/changelog/2024/10/13)

---

# February 17, 2025 — ElevenLabs Documentation

### Conversational AI

- **Tool calling fix**: Fixed an issue where tool calling was not working with agents using gpt-4o mini. This was due to a breaking change in the OpenAI API.
- **Tool calling improvements**: Added support for tool calling with dynamic variables inside objects and arrays.
- **Dynamic variables**: Fixed an issue where dynamic variables of a conversation were not being displayed correctly.

### Voice Isolator

- **Fixed**: Fixed an issue that caused the voice isolator to not work correctly temporarily.

### Workspace

- **Billing**: Improved billing visibility by differentiating rollover, cycle, gifted, and usage-based credits.
- **Usage Analytics**: Improved usage analytics load times and readability.
- **Fine grained fiat billing**: Added support for customizable pricing based on several factors.

### API

###### View API changes

- Added `phone_numbers` property to [Agent responses](/docs/api-reference/agents/get-agent)
- Added usage metrics to subscription_extras in [User endpoint](/docs/api-reference/user/get)
  :
  - `unused_characters_rolled_over_from_previous_period`
  - `overused_characters_rolled_over_from_previous_period`
  - `usage` statistics
- Added `enable_conversation_initiation_client_data_from_webhook` to [Agent creation](/docs/api-reference/agents/create-agent)
- Updated [Agent](/docs/api-reference/agents)
  endpoints with consolidated settings for:
  - `platform_settings`
  - `overrides`
  - `safety`
- Deprecated `with_settings` parameter in [Voice retrieval endpoint](/docs/api-reference/voices/get)

[February 10, 2025\
\
Up Next](/docs/changelog/2025/2/10)

---

# November 29, 2024 — ElevenLabs Documentation

## Product

- **GenFM**: Launched in the ElevenReader app. [Learn more](https://elevenlabs.io/blog/genfm-on-elevenreader)
- **Conversational AI**: Now generally available to all customers. [Try it now](https://elevenlabs.io/conversational-ai)
- **TTS Redesign**: The website TTS redesign is now rolled out to all customers.
- **Auto-regenerate**: Now available in Projects. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)
- **Reader Platform Improvements**:

  - Improved content sharing with enhanced landing pages and social media previews.
  - Added podcast rating system and improved voice synchronization.

- **Projects revamp**:

  - Restore past generations, lock content, assign speakers to sentence fragments, and QC at 2x speed. [Learn more](https://elevenlabs.io/blog/narrate-any-project)
  - Auto-regeneration identifies mispronunciations and regenerates audio at no extra cost. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)

## API

- **Conversational AI**: [SDKs and APIs](https://elevenlabs.io/docs/conversational-ai/docs/introduction)
  now available.

[October 27, 2024\
\
Up Next](/docs/changelog/2024/10/27)

---

# February 10, 2025 — ElevenLabs Documentation

## Conversational AI

- **Updated Pricing**: Updated self-serve pricing for Conversational AI with [reduced cost and a more generous free tier](/docs/conversational-ai/overview#pricing-tiers)
  .
- **Knowledge Base UI**: Created a new page to easily manage your [knowledge base](/app/conversational-ai/knowledge-base)
  .
- **Live calls**: Added number of live calls in progress in the user [dashboard](/app/conversational-ai)
  and as a new endpoint.
- **Retention**: Added ability to customize transcripts and audio recordings [retention settings](/docs/conversational-ai/customization/privacy/retention)
  .
- **Audio recording**: Added a new option to [disable audio recordings](/docs/conversational-ai/customization/privacy/audio-saving)
  .
- **8k PCM support**: Added support for 8k PCM audio for both input and output.

## Studio

- **GenFM**: Updated the create podcast endpoint to accept [multiple input sources](/docs/api-reference/projects/create-podcast)
  .
- **GenFM**: Fixed an issue where GenFM was creating empty podcasts.

## Enterprise

- **New workspace group endpoints**: Added new endpoints to manage [workspace groups](/docs/api-reference/workspace/search-user-groups)
  .

### API

###### Deprecated Endpoints

**Studio (formerly Projects)**

All `/v1/projects/*` endpoints have been deprecated in favor of the new `/v1/studio/projects/*` endpoints. The following endpoints are now deprecated:

- All operations on `/v1/projects/`
- All operations related to chapters, snapshots, and content under `/v1/projects/*`

**Conversational AI**

- `POST /v1/convai/add-tool` - Use `POST /v1/convai/tools` instead

###### Breaking Changes

- `DELETE /v1/convai/agents/{agent_id}` - Response type is no longer an object
- `GET /v1/convai/tools` - Response type changed from array to object with a `tools` property

###### Modified Endpoints

**Conversational AI Updates**

- `GET /v1/convai/agents/{agent_id}` - Updated conversation configuration and agent properties
- `PATCH /v1/convai/agents/{agent_id}` - Added `use_tool_ids` parameter for tool management
- `POST /v1/convai/agents/create` - Added tool integration via `use_tool_ids`

**Knowledge Base & Tools**

- `GET /v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
- `GET /v1/convai/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
- `GET /v1/convai/tools/{tool_id}` - Added `dependent_agents` property
- `PATCH /v1/convai/tools/{tool_id}` - Added `dependent_agents` property

**GenFM**

- `POST /v1/projects/podcast/create` - Added support for multiple input sources

###### New Endpoints

**Studio (formerly Projects)**

New endpoints replacing the deprecated `/v1/projects/*` endpoints

- `GET /v1/studio/projects`: List all projects
- `POST /v1/studio/projects`: Create a project
- `GET /v1/studio/projects/{project_id}`: Get project details
- `DELETE /v1/studio/projects/{project_id}`: Delete a project

**Knowledge Base Management**

- `GET /v1/convai/knowledge-base`: List all knowledge base documents
- `DELETE /v1/convai/knowledge-base/{documentation_id}`: Delete a knowledge base
- `GET /v1/convai/knowledge-base/{documentation_id}/dependent-agents`: List agents using this knowledge base

**Workspace Groups** - New enterprise features for team management

- `GET /v1/workspace/groups/search`: Search workspace groups
- `POST /v1/workspace/groups/{group_id}/members`: Add members to a group
- `POST /v1/workspace/groups/{group_id}/members/remove`: Remove members from a group

**Tools**

- `POST /v1/convai/tools`: Create new tools for agents

## Socials

- **ElevenLabs Developers**: Follow our new developers account on X [@ElevenLabsDevs](https://x.com/ElevenLabsDevs)

[February 4, 2025\
\
Up Next](/docs/changelog/2025/2/4)

---

# Voice changer — ElevenLabs Documentation

## Overview

ElevenLabs [voice changer](/docs/api-reference/speech-to-speech/convert)
API lets you transform any source audio (recorded or uploaded) into a different, fully cloned voice without losing the performance nuances of the original. It’s capable of capturing whispers, laughs, cries, accents, and subtle emotional cues to achieve a highly realistic, human feel and can be used to:

- Change any voice while preserving emotional delivery and nuance
- Create consistent character voices across multiple languages and recording sessions
- Fix or replace specific words and phrases in existing recordings

Your browser does not support the video tag.

Explore our [voice library](https://elevenlabs.io/community)
to find the perfect voice for your project.

[Developer quickstart\
\
Learn how to integrate voice changer into your application.](/docs/quickstart)
[Product guide\
\
Step-by-step guide for using voice changer in ElevenLabs.](/docs/product-guides/playground/voice-changer)

## Supported languages

Our v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

The `eleven_english_sts_v2` model only supports English.

## Best practices

### Audio quality

- Record in a quiet environment to minimize background noise
- Maintain appropriate microphone levels - avoid too quiet or peaked audio
- Use `remove_background_noise=true` if environmental sounds are present

### Recording guidelines

- Keep segments under 5 minutes for optimal processing
- Feel free to include natural expressions (laughs, sighs, emotions)
- The source audio’s accent and language will be preserved in the output

### Parameters

- **Style**: Set to 0% when input audio is already expressive
- **Stability**: Use 100% for maximum voice consistency
- **Language**: Choose source audio that matches your desired accent and language

## FAQ

###### Can I convert more than 5 minutes of audio?

Yes, but you must split it into smaller chunks (each under 5 minutes). This helps ensure stability and consistent output.

###### Can I use my own custom/cloned voice for output?

Absolutely. Provide your custom voice’s `voice_id` and specify the correct `model_id`.

###### How is billing handled?

You’re charged at 1000 characters’ worth of usage per minute of processed audio. There’s no additional fee based on file size.

###### Does the model reproduce background noise?

Possibly. Use `remove_background_noise=true` or the Voice Isolator tool to minimize environmental sounds in the final output.

###### Which model is best for English audio?

Though `eleven_english_sts_v2` is available, our `eleven_multilingual_sts_v2` model often outperforms it, even for English material.

###### How does style & stability work?

“Style” adds interpretative flair; “stability” enforces consistency. For high-energy performances in the source audio, turn style down and stability up.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Dubbing — ElevenLabs Documentation

## Overview

ElevenLabs [dubbing](/docs/api-reference/dubbing/dub-a-video-or-an-audio-file)
API translates audio and video across 32 languages while preserving the emotion, timing, tone and unique characteristics of each speaker. Our model separates each speaker’s dialogue from the soundtrack, allowing you to recreate the original delivery in another language. It can be used to:

- Grow your addressable audience by 4x to reach international audiences
- Adapt existing material for new markets while preserving emotional nuance
- Offer content in multiple languages without re-recording voice talent

Your browser does not support the video tag.

We also offer a [fully managed dubbing service](https://elevenlabs.io/elevenstudios)
for video and podcast creators.

## Usage

ElevenLabs dubbing can be used in two ways:

- **Dubbing Studio** in the user interface for fast, interactive control and editing
- **Programmatic integration** via our [API](/docs/api-reference/dubbing/dub-a-video-or-an-audio-file)
  for large-scale or automated workflows

The UI supports files up to **500MB** and **45 minutes**. The API supports files up to **1GB** and **2.5 hours**.

[Developer quickstart\
\
Learn how to integrate dubbing into your application.](/docs/quickstart)
[Product guide\
\
Edit transcripts and translate videos step by step in Dubbing Studio.](/docs/product-guides/products/dubbing)

### Key features

**Speaker separation**  
Automatically detect multiple speakers, even with overlapping speech.

**Multi-language output**  
Generate localized tracks in 32 languages.

**Preserve original voices**  
Retain the speaker’s identity and emotional tone.

**Keep background audio**  
Avoid re-mixing music, effects, or ambient sounds.

**Customizable transcripts**  
Manually edit translations and transcripts as needed.

**Supported file types**  
Videos and audio can be dubbed from various sources, including YouTube, X, TikTok, Vimeo, direct URLs, or file uploads.

**Video transcript and translation editing**  
Our AI video translator lets you manually edit transcripts and translations to ensure your content is properly synced and localized. Adjust the voice settings to tune delivery, and regenerate speech segments until the output sounds just right.

A Creator plan or higher is required to dub audio files. For videos, a watermark option is available to reduce credit usage.

### Cost

To reduce credit usage, you can:

- Dub only a selected portion of your file
- Use watermarks on video output (not available for audio)
- Fine-tune transcripts and regenerate individual segments instead of the entire clip

Refer to our [pricing page](https://elevenlabs.io/pricing)
for detailed credit costs.

## FAQ

###### What content can I dub?

Dubbing can be performed on all types of short and long form video and audio content. We recommend dubbing content with a maximum of 9 unique speakers at a time to ensure a high-quality dub.

###### Does dubbing preserve the speaker's natural intonation?

Yes. Our models analyze each speaker’s original delivery to recreate the same tone, pace, and style in your target language.

###### What about overlapping speakers or background noise?

We use advanced source separation to isolate individual voices from ambient sound. Multiple overlapping speakers can be split into separate tracks.

###### Are there file size limits?

Via the user interface, the maximum file size is 500MB up to 45 minutes. Through the API, you can process files up to 1GB and 2.5 hours.

###### How do I handle fine-tuning or partial translations?

You can choose to dub only certain portions of your video/audio or tweak translations/voices in our interactive Dubbing Studio.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# January 27, 2025 — ElevenLabs Documentation

### Docs

- **Shipped our new docs**: we’re keen to hear your thoughts, you can reach out by opening an issue on [GitHub](https://github.com/elevenlabs/elevenlabs-docs)
  or chatting with us on [Discord](https://discord.gg/elevenlabs)

### Conversational AI

- **Dynamic variables**: Available in the dashboard and SDKs. [Learn more](/docs/conversational-ai/customization/dynamic-variables)
- **Interruption handling**: Now possible to ignore user interruptions in Conversational AI. [Learn more](/docs/conversational-ai/customization/interruptions)
- **Twilio integration**: Shipped changes to increase audio quality when integrating with Twilio
- **Latency optimization**: Published detailed blog post on latency optimizations. [Read more](/blog/how-do-you-optimize-latency-for-conversational-ai)
- **PCM 8000**: Added support for PCM 8000 to Conversational AI agents
- **Websocket improvements**: Fixed unexpected websocket closures

### Projects

- **Auto-regenerate**: Auto-regeneration now available by default at no extra cost
- **Content management**: Added `updateContent` method for dynamic content updates
- **Audio conversion**: New auto-convert and auto-publish flags for seamless workflows

### API

###### View API changes

- Added `Update Project` endpoint for [project editing](/docs/api-reference/projects/edit-project#:~:text=List%20projects-,POST,Update%20project,-GET)
- Added `Update Content` endpoint for [AudioNative content management](/docs/api-reference/audio-native/update-content)
- Deprecated `quality_check_on` parameter in [project operations](/docs/api-reference/projects/add-project#request.body.quality_check_on)
  . It is now enabled for all users at no extra cost
- Added `apply_text_normalization` parameter to project creation with modes ‘auto’, ‘on’, ‘apply_english’ and ‘off’ for controlling text normalization during [project creation](/docs/api-reference/projects/add-project#request.body.apply_text_normalization)
- Added alpha feature `auto_assign_voices` in [project creation](/docs/api-reference/projects/add-project#request.body.auto_assign_voices)
  to automatically assign voices to phrases
- Added `auto_convert` flag to project creation to automatically convert [projects to audio](/docs/api-reference/audio-native/create#request.body.auto_convert)
- Added support for creating Conversational AI agents with [dynamic variables](/docs/api-reference/agents/create-agent#request.body.conversation_config.agent.dynamic_variables)
- Added `voice_slots_used` to `Subscription` model to track number of custom voices used in a workspace to the `User` [endpoint](/docs/api-reference/user/get-subscription#response.body.voice_slots_used)
- Added `user_id` field to `User` [endpoint](/docs/api-reference/user/get#response.body.user_id)
- Marked legacy AudioNative creation parameters (`image`, `small`, `sessionization`) as deprecated [parameters](/docs/api-reference/audio-native/create#request.body.image)
- Agents platform now supports `call_limits` containing either `agent_concurrency_limit` or `daily_limit` or both parameters to control simultaneous and daily conversation limits for [agents](/docs/changelog/2025/1/docs/api-reference/agents/create-agent#request.body.platform_settings.call_limits)
- Added support for `language_presets` in `conversation_config` to customize language-specific [settings](/docs/api-reference/agents/create-agent#request.body.conversation_config.language_presets)

### SDKs

- **Cross-Runtime Support**: Now compatible with **Bun 1.1.45+** and **Deno 2.1.7+**
- **Regenerated SDKs**: We regenerated our SDKs to be up to date with the latest API spec. Check out the latest [Python SDK release](https://github.com/elevenlabs/elevenlabs-python/releases/tag/1.50.5)
  and [JS SDK release](https://github.com/elevenlabs/elevenlabs-js/releases/tag/v1.50.4)
- **Dynamic Variables**: Fixed an issue where dynamic variables were not being handled correctly, they are now correctly handled in all SDKs

[January 16, 2025\
\
Up Next](/docs/changelog/2025/1/16)

---

# Voice isolator — ElevenLabs Documentation

## Overview

ElevenLabs [voice isolator](/docs/api-reference/audio-isolation/audio-isolation)
API transforms audio recordings with background noise into clean, studio-quality speech. This is particularly useful for audio recorded in noisy environments, or recordings containing unwanted ambient sounds, music, or other background interference.

Listen to a sample:

## Usage

The voice isolator model extracts speech from background noise in both audio and video files.

[Developer quickstart\
\
Learn how to integrate voice isolator into your application.](/docs/quickstart)
[Product guide\
\
Step-by-step guide for using voice isolator in ElevenLabs.](/docs/product-guides/audio-tools/voice-isolator)

### Supported file types

- **Audio**: AAC, AIFF, OGG, MP3, OPUS, WAV, FLAC, M4A
- **Video**: MP4, AVI, MKV, MOV, WMV, FLV, WEBM, MPEG, 3GPP

## FAQ

- **Cost**: Voice isolator costs 1000 characters for every minute of audio.
- **File size and length**: Supports files up to 500MB and 1 hour in length.
- **Music vocals**: Not specifically optimized for isolating vocals from music, but may work depending on the content.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voices — ElevenLabs Documentation

## Overview

ElevenLabs provides models for voice creation & customization. The platform supports a wide range of voice options, including voices from our extensive [voice library](https://elevenlabs.io/app/voice-library)
, voice cloning, and artificially designed voices using text prompts.

### Voice categories

- **Community**: Voices shared by the community from the ElevenLabs [voice library](/docs/product-guides/voices/voice-library)
  .
- **Cloned**: Custom voices created using instant or professional [voice cloning](/docs/product-guides/voices/voice-cloning)
  .
- **Voice design**: Artificially designed voices created with the [voice design](/docs/product-guides/voices/voice-design)
  tool.
- **Default**: Pre-designed, high-quality voices optimized for general use.

#### Community

The [voice library](/docs/product-guides/voices/voice-library)
contains over 5,000 voices shared by the ElevenLabs community. Use it to:

- Discover unique voices shared by the ElevenLabs community.
- Add voices to your personal collection.
- Share your own voice clones for cash rewards when others use it.

Share your voice with the community, set your terms, and earn cash rewards when others use it. We’ve paid out over **$1M** already.

[Product guide\
\
Learn how to use voices from the voice library](/docs/product-guides/voices/voice-library)

#### Cloned

Clone your own voice from 30-second samples with Instant Voice Cloning, or create hyper-realistic voices using Professional Voice Cloning.

- **Instant Voice Cloning**: Quickly replicate a voice from short audio samples.
- **Professional Voice Cloning**: Generate professional-grade voice clones with extended training audio.

Voice-captcha technology is used to verify that **all** voice clones are created from your own voice samples.

A Creator plan or higher is required to create voice clones.

[Product guide\
\
Learn how to create instant & professional voice clones](/docs/product-guides/voices/voice-cloning)

#### Voice design

With [Voice Design](/docs/voice-design)
, you can create entirely new voices by specifying attributes like age, gender, accent, and tone. Generated voices are ideal for:

- Realistic voices with nuanced characteristics.
- Creative character voices for games and storytelling.

The voice design tool creates 3 voice previews, simply provide:

- A **voice description** between 20 and 1000 characters.
- Some **text** to preview the voice between 100 and 1000 characters.

[Developer quickstart\
\
Integrate voice design into your application.](/docs/quickstart)
[Product guide\
\
Learn how to craft voices from a single prompt.](/docs/product-guides/voices/voice-design)

#### Default

Our curated set of default voices is optimized for core use cases. These voices are:

- **Reliable**: Available long-term.
- **Consistent**: Carefully crafted and quality-checked for performance.
- **Model-ready**: Fine-tuned on new models upon release.

Default voices are available to all users via the **my voices** tab in the [voice lab dashboard](https://elevenlabs.io/app/voice-lab)
. Default voices were previously referred to as `premade` voices. The latter term is still used when accessing default voices via the API.

### Managing voices

All voices can be managed through **My Voices**, where you can:

- Search, filter, and categorize voices
- Add descriptions and custom tags
- Organize voices for quick access

Learn how to manage your voice collection in [My Voices documentation](/docs/my-voices)
.

- **Search and Filter**: Find voices using keywords or tags.
- **Preview Samples**: Listen to voice demos before adding them to **My Voices**.
- **Add to Collection**: Save voices for easy access in your projects.

> **Tip**: Try searching by specific accents or genres, such as “Australian narration” or “child-like character.”

### Supported languages

All ElevenLabs voices support multiple languages. Experiment by converting phrases like `Hello! こんにちは! Bonjour!` into speech to hear how your own voice sounds across different languages.

ElevenLabs supports voice creation in 32 languages. Match your voice selection to your target region for the most natural results.

- **Default Voices**: Optimized for multilingual use.
- **Generated and Cloned Voices**: Accent fidelity depends on input samples or selected attributes.

Our v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

[Learn more about our models](/docs/models)

## FAQ

###### Can I create a custom voice?

Yes, you can create custom voices with Voice Design or clone voices using Instant or Professional Voice Cloning. Both options are accessible in **My Voices**.

###### What is the difference between Instant and Professional Voice Cloning?

Instant Voice Cloning uses short audio samples for near-instantaneous voice creation. Professional Voice Cloning requires longer samples but delivers hyper-realistic, high-quality results.

###### Can I share my created voices?

Professional Voice Clones can be shared privately or publicly in the Voice Library. Generated voices and Instant Voice Clones cannot currently be shared.

###### How do I manage my voices?

Use **My Voices** to search, filter, and organize your voice collection. You can also delete, tag, and categorize voices for easier management.

###### How can I ensure my cloned voice matches the original?

Use clean and consistent audio samples. For Professional Voice Cloning, provide a variety of recordings in the desired speaking style.

###### Can I share voices I create?

Yes, Professional Voice Clones can be shared in the Voice Library. Instant Voice Clones and Generated Voices cannot currently be shared.

###### What are some common use cases for Generated Voices?

Generated Voices are ideal for unique characters in games, animations, and creative storytelling.

###### How do I access the Voice Library?

Go to **Voices > Voice Library** in your dashboard or access it via API.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Sound effects — ElevenLabs Documentation

## Overview

ElevenLabs [sound effects](/docs/api-reference/text-to-sound-effects/convert)
API turns text descriptions into high-quality audio effects with precise control over timing, style and complexity. The model understands both natural language and audio terminology, enabling you to:

- Generate cinematic sound design for films & trailers
- Create custom sound effects for games & interactive media
- Produce Foley and ambient sounds for video content

Listen to an example:

## Usage

Sound effects are generated using text descriptions & two optional parameters:

- **Duration**: Set a specific length for the generated audio (in seconds)

  - Default: Automatically determined based on the prompt
  - Range: 0.1 to 30 seconds
  - Cost: 40 characters per second when duration is specified

- **Prompt influence**: Control how strictly the model follows the prompt

  - High: More literal interpretation of the prompt
  - Low: More creative interpretation with added variations

[Developer quickstart\
\
Learn how to integrate sound effects into your application.](/docs/quickstart)
[Product guide\
\
Step-by-step guide for using sound effects in ElevenLabs.](/docs/product-guides/playground/sound-effects)

### Prompting guide

#### Simple effects

For basic sound effects, use clear, concise descriptions:

- “Glass shattering on concrete"
- "Heavy wooden door creaking open"
- "Thunder rumbling in the distance”

#### Complex sequences

For multi-part sound effects, describe the sequence of events:

- “Footsteps on gravel, then a metallic door opens"
- "Wind whistling through trees, followed by leaves rustling"
- "Sword being drawn, then clashing with another blade”

#### Musical elements

The API also supports generation of musical components:

- ”90s hip-hop drum loop, 90 BPM"
- "Vintage brass stabs in F minor"
- "Atmospheric synth pad with subtle modulation”

#### Audio Terminology

Common terms that can enhance your prompts:

- **Impact**: Collision or contact sounds between objects, from subtle taps to dramatic crashes
- **Whoosh**: Movement through air effects, ranging from fast and ghostly to slow-spinning or rhythmic
- **Ambience**: Background environmental sounds that establish atmosphere and space
- **One-shot**: Single, non-repeating sound
- **Loop**: Repeating audio segment
- **Stem**: Isolated audio component
- **Braam**: Big, brassy cinematic hit that signals epic or dramatic moments, common in trailers
- **Glitch**: Sounds of malfunction, jittering, or erratic movement, useful for transitions and sci-fi
- **Drone**: Continuous, textured sound that creates atmosphere and suspense

## FAQ

###### What's the maximum duration for generated effects?

The maximum duration is 30 seconds per generation. For longer sequences, generate multiple effects and combine them.

###### Can I generate music with this API?

Yes, you can generate musical elements like drum loops, bass lines, and melodic samples. However, for full music production, consider combining multiple generated elements.

###### How do I ensure consistent quality?

Use detailed prompts, appropriate duration settings, and high prompt influence for more predictable results. For complex sounds, generate components separately and combine them.

###### What audio formats are supported?

Generated audio is provided in MP3 format with professional-grade quality (44.1kHz, 128-192kbps).

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Create sound effects from text — ElevenLabs Documentation

## Introduction

Our [text to sound effects](https://elevenlabs.io/sound-effects)
model enables you to create high-quality sound effects from a short description. These sound effects could be used in a variety of applications, including game development and building apps for music production.

In this tutorial, we will use the text to sound effects API to generate a sound effect from a short description using the Python SDK. We’ll then save this sound effect to a file.

For general tips on prompting, see the [sound effects product docs](/docs/product/sound-effects/overview)
. And for information on the API configuration visit [the API reference](/docs/api-reference/sound-generation)
.

## How to generate a sound effect with the API

### Requirements

Before proceeding, please ensure that you have the following:

- An ElevenLabs account with an API key (here’s how to [find your API key](/docs/api-reference/authentication)
  )
- Python installed on your machine

Then, install the ElevenLabs SDK as shown below

Python

`  |     |     | | --- | --- | | $   | pip install elevenlabs |    `

Install the necessary packages to manage your environmental variables:

Python

`  |     |     | | --- | --- | | $   | pip install python-dotenv |    `

Next, create a `.env` file in your project directory and fill it with your credentials like so:

.env

`  |     |     | | --- | --- | | $   | ELEVENLABS_API_KEY=your_elevenlabs_api_key_here |    `

### Using the sound effects SDK

Now we can use the SDK to generate a sound effect from a short description and save it to a file as shown below.

`  |     |     | | --- | --- | | 1   | import os | | 2   | from elevenlabs.client import ElevenLabs | | 3   |     | | 4   | from dotenv import load_dotenv | | 5   |     | | 6   | load_dotenv() | | 7   |     | | 8   | elevenlabs = ElevenLabs(api_key=os.getenv("ELEVENLABS_API_KEY")) | | 9   |     | | 10  |     | | 11  | def generate_sound_effect(text: str, output_path: str): | | 12  | print("Generating sound effects...") | | 13  |     | | 14  | result = elevenlabs.text_to_sound_effects.convert( | | 15  | text=text, | | 16  | duration_seconds=10,  # Optional, if not provided will automatically determine the correct length | | 17  | prompt_influence=0.3,  # Optional, if not provided will use the default value of 0.3 | | 18  | )   | | 19  |     | | 20  | with open(output_path, "wb") as f: | | 21  | for chunk in result: | | 22  | f.write(chunk) | | 23  |     | | 24  | print(f"Audio saved to {output_path}") | | 25  |     | | 26  |     | | 27  | if __name__ == "__main__": | | 28  | generate_sound_effect("Dog barking", "output.mp3") |    `

## Configuration

- `duration_seconds`: The duration of the sound effect in seconds. If not provided, the API will automatically determine the correct length. The maximum value is 22
- `prompt_influence`: The amount of influence the prompt has on the generated sound effect. If not provided, the API will use the default value of 0.3

### API pricing

The API is charged at 100 characters per generation with automatic duration or 25 characters per second with a set duration.

### Next steps

We’re excited to see what you build with the API. Here are some ideas of what you might want to use it for:

- Adding sound effect generation to a video editing application
- Enabling users to create on-demand samples for their music production
- A new type of video game where every sound is generated dynamically

For higher rate limits of volume based discounts please [contact sales](https://elevenlabs.io/contact-sales)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Convert text to speech in real-time — ElevenLabs Documentation

Websocket streaming is a method of sending and receiving data over a single, long-lived connection. This method is useful for real-time applications where you need to stream audio data as it becomes available.

If you want to quickly test out the latency (time to first byte) of a websocket connection to the ElevenLabs text-to-speech API, you can install `elevenlabs-latency` via `npm` and follow the instructions [here](https://www.npmjs.com/package/elevenlabs-latency?activeTab=readme)
.

## Requirements

- An ElevenLabs account with an API key (here’s how to [find your API key](/docs/api-reference/authentication)
  ).
- Python or Node.js/Typescript installed on your machine

## Setup

Install dotenv package to manage your environmental variables:

PythonTypeScript

`  |     |     | | --- | --- | | $   | pip install python-dotenv | | >   | pip install websockets |    `

Next, create a `.env` file in your project directory and fill it with your credentials like so:

.env

`  |     |     | | --- | --- | | $   | ELEVENLABS_API_KEY=your_elevenlabs_api_key_here |    `

Last, create a new file to write the code in. You can name it `text-to-speech-websocket.py` for Python or `text-to-speech-websocket.ts` for Typescript.

## Initiate the websocket connection

Pick a voice from the voice library and a text-to-speech model; Then initiate a websocket connection to the text-to-speech API.

text-to-speech-websocket.py (Python)text-to-speech-websocket.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import os | | 2   | from dotenv import load_dotenv | | 3   | import websockets | | 4   |     | | 5   | # Load the API key from the .env file | | 6   | load_dotenv() | | 7   | ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY") | | 8   |     | | 9   | voice_id = 'kmSVBPu7loj4ayNinwWM' | | 10  | model_id = 'eleven_flash_v2_5' | | 11  |     | | 12  | async def text_to_speech_ws_streaming(voice_id, model_id): | | 13  | uri = f"wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id={model_id}" | | 14  |     | | 15  | async with websockets.connect(uri) as websocket: | | 16  | ... |    `

For TypeScript, create a write stream ahead for saving the audio into mp3 which can be passed to the websocket listener.

text-to-speech-websocket.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import * as fs from 'node:fs'; | | 2   |     | | 3   | const outputDir = './output'; | | 4   | try { | | 5   | fs.accessSync(outputDir, fs.constants.R_OK \| fs.constants.W_OK); | | 6   | } catch (err) { | | 7   | fs.mkdirSync(outputDir); | | 8   | }   | | 9   | const writeStream = fs.createWriteStream(outputDir + '/test.mp3', { | | 10  | flags: 'a', | | 11  | }); |    `

## Send the input text

Once the websocket connection is open, set up voice settings first. Next, send the text message to the API.

text-to-speech-websocket.py (Python)text-to-speech-websocket.ts (Typescript)

`  |     |     | | --- | --- | | 1   | async def text_to_speech_ws_streaming(voice_id, model_id): | | 2   | async with websockets.connect(uri) as websocket: | | 3   | await websocket.send(json.dumps({ | | 4   | "text": " ", | | 5   | "voice_settings": {"stability": 0.5, "similarity_boost": 0.8, "use_speaker_boost": False}, | | 6   | "generation_config": { | | 7   | "chunk_length_schedule": [120, 160, 250, 290] | | 8   | },  | | 9   | "xi_api_key": ELEVENLABS_API_KEY, | | 10  | })) | | 11  |     | | 12  | text = "The twilight sun cast its warm golden hues upon the vast rolling fields, saturating the landscape with an ethereal glow. Silently, the meandering brook continued its ceaseless journey, whispering secrets only the trees seemed privy to." | | 13  | await websocket.send(json.dumps({"text": text})) | | 14  |     | | 15  | // Send empty string to indicate the end of the text sequence which will close the websocket connection | | 16  | await websocket.send(json.dumps({"text": ""})) |    `

## Save the audio to file

Read the incoming message from the websocket connection and write the audio chunks to a local file.

text-to-speech-websocket.py (Python)text-to-speech-websocket.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import asyncio | | 2   |     | | 3   | async def write_to_local(audio_stream): | | 4   | """Write the audio encoded in base64 string to a local mp3 file.""" | | 5   |     | | 6   | with open(f'./output/test.mp3', "wb") as f: | | 7   | async for chunk in audio_stream: | | 8   | if chunk: | | 9   | f.write(chunk) | | 10  |     | | 11  | async def listen(websocket): | | 12  | """Listen to the websocket for audio data and stream it.""" | | 13  |     | | 14  | while True: | | 15  | try: | | 16  | message = await websocket.recv() | | 17  | data = json.loads(message) | | 18  | if data.get("audio"): | | 19  | yield base64.b64decode(data["audio"]) | | 20  | elif data.get('isFinal'): | | 21  | break | | 22  |     | | 23  | except websockets.exceptions.ConnectionClosed: | | 24  | print("Connection closed") | | 25  | break | | 26  |     | | 27  | async def text_to_speech_ws_streaming(voice_id, model_id): | | 28  | async with websockets.connect(uri) as websocket: | | 29  | ... | | 30  | # Add listen task to submit the audio chunks to the write_to_local function | | 31  | listen_task = asyncio.create_task(write_to_local(listen(websocket))) | | 32  |     | | 33  | await listen_task | | 34  |     | | 35  | asyncio.run(text_to_speech_ws_streaming(voice_id, model_id)) |    `

## Run the script

You can run the script by executing the following command in your terminal. An mp3 audio file will be saved in the `output` directory.

PlaintextPlaintext 1

`python Python python text-to-speech-websocket.py`

## Understanding buffering

A key concept to understand when using websockets is buffering. The API only runs model generations when a certain amount of text above a threshold has been sent. This is to optimize the quality of the generated audio by maximising the amount of context available to the model while balancing latency.

For example, if the threshold is set to 120 characters and you send ‘Hello, how are you?’, the audio won’t be generated immediately. This is because the sent message has only 19 characters which is below the threshold. However, if you keep sending text, the API will generate audio once the total text sent since the last generation has at least 120 characters.

In the case that you want force the immediate return of the audio, you can use `flush=true` to clear out the buffer and force generate any buffered text. This can be useful, for example, when you have reached the end of a document and want to generate audio for the final section.

In addition, closing the websocket will automatically force generate any buffered text.

## Best practice

- We suggest using the default setting for `chunk_length_schedule` in `generation_config`. Avoid using `try_trigger_generation` as it is deprecated.
- When developing a real-time conversational AI application, we advise using `flush=true` along with the text at the end of conversation turn to ensure timely audio generation.
- If the default setting doesn’t provide optimal latency for your use case, you can modify the `chunk_length_schedule`. However, be mindful that reducing latency through this adjustment may come at the expense of quality.

## Tips

- The API maintains a internal buffer so that it only runs model generations when a certain amount of text above a threshold has been sent. For short texts with a character length smaller than the value set in `chunk_length_schedule`, you can use `flush=true` to clear out the buffer and force generate any buffered text.
- The websocket connection will automatically close after 20 seconds of inactivity. To keep the connection open, you can send a single space character `" "`. Please note that this string must include a space, as sending a fully empty string, `""`, will close the websocket.
- Send an empty string to close the websocket connection after sending the last text message.
- You can use `alignment` to get the word-level timestamps for each word in the text. This can be useful for aligning the audio with the text in a video or for other applications that require precise timing.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Using pronunciation dictionaries — ElevenLabs Documentation

In this tutorial, you’ll learn how to use a pronunciation dictionary with the ElevenLabs Python SDK. Pronunciation dictionaries are useful for controlling the specific pronunciation of words. We support both [IPA](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)
and [CMU](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)
alphabets. It is useful for correcting rare or specific pronunciations, such as names or companies. For example, the word `nginx` could be pronounced incorrectly. Instead, we can add our version of pronunciation. Based on IPA, `nginx` is pronounced as `/ˈɛndʒɪnˈɛks/`. Finding IPA or CMU of words manually can be difficult. Instead, LLMs like ChatGPT can help you to make the search easier.

We’ll start by adding rules to the pronunciation dictionary from a file and comparing the text-to-speech results that use and do not use the dictionary. After that, we’ll discuss how to add and remove specific rules to existing dictionaries.

If you want to jump straight to the finished repo you can find it [here](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/pronunciation-dictionaries/python)

Phoneme tags only work with the `eleven_turbo_v2` & `eleven_monolingual_v1` models. If you use phoneme tags with other models, they will silently skip the word.

## Requirements

- An ElevenLabs account with an API key (here’s how to [find your API key](/docs/api-reference/text-to-speech#authentication)
  ).
- Python installed on your machine
- FFMPEG to play audio

## Setup

### Installing our SDK

Before you begin, make sure you have installed the necessary SDKs and libraries. You will need the ElevenLabs SDK for the updating pronunciation dictionary and using text-to-speech conversion. You can install it using pip:

`  |     |     | | --- | --- | | $   | pip install elevenlabs |    `

Additionally, install `python-dotenv` to manage your environmental variables:

`  |     |     | | --- | --- | | $   | pip install python-dotenv |    `

Next, create a `.env` file in your project directory and fill it with your credentials like so:

`  ELEVENLABS_API_KEY=your_elevenlabs_api_key_here  `

## Initiate the Client SDK

We’ll start by initializing the client SDK.

`  |     |     | | --- | --- | | 1   | import os | | 2   | from elevenlabs.client import ElevenLabs | | 3   |     | | 4   | ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY") | | 5   | client = ElevenLabs( | | 6   | api_key=ELEVENLABS_API_KEY, | | 7   | )   |    `

## Create a Pronunciation Dictionary From a File

To create a pronunciation dictionary from a File, we’ll create a `.pls` file for our rules.

This rule will use the “IPA” alphabet and update the pronunciation for `tomato` and `Tomato` with a different pronunciation. PLS files are case sensitive which is why we include it both with and without a capital “T”. Save it as `dictionary.pls`.

dictionary.pls

`  |     |     | | --- | --- | | 1   | <?xml version="1.0" encoding="UTF-8"?> | | 2   | <lexicon version="1.0" | | 3   | xmlns="http://www.w3.org/2005/01/pronunciation-lexicon" | | 4   | xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" | | 5   | xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon | | 6   | http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd" | | 7   | alphabet="ipa" xml:lang="en-US"> | | 8   | <lexeme> | | 9   | <grapheme>tomato</grapheme> | | 10  | <phoneme>/tə'meɪtoʊ/</phoneme> | | 11  | </lexeme> | | 12  | <lexeme> | | 13  | <grapheme>Tomato</grapheme> | | 14  | <phoneme>/tə'meɪtoʊ/</phoneme> | | 15  | </lexeme> | | 16  | </lexicon> |    `

In the following snippet, we start by adding rules from a file and get the uploaded result. Finally, we generate and play two different text-to-speech audio to compare the custom pronunciation dictionary.

`  |     |     | | --- | --- | | 1   | import requests | | 2   | from elevenlabs import play, PronunciationDictionaryVersionLocator | | 3   |     | | 4   | with open("dictionary.pls", "rb") as f: | | 5   | # this dictionary changes how tomato is pronounced | | 6   | pronunciation_dictionary = client.pronunciation_dictionary.add_from_file( | | 7   | file=f.read(), name="example" | | 8   | )   | | 9   |     | | 10  | audio_1 = client.generate( | | 11  | text="Without the dictionary: tomato", | | 12  | voice="Rachel", | | 13  | model="eleven_turbo_v2", | | 14  | )   | | 15  |     | | 16  | audio_2 = client.generate( | | 17  | text="With the dictionary: tomato", | | 18  | voice="Rachel", | | 19  | model="eleven_turbo_v2", | | 20  | pronunciation_dictionary_locators=[ | | 21  | PronunciationDictionaryVersionLocator( | | 22  | pronunciation_dictionary_id=pronunciation_dictionary.id, | | 23  | version_id=pronunciation_dictionary.version_id, | | 24  | )   | | 25  | ],  | | 26  | )   | | 27  |     | | 28  | # play the audio | | 29  | play(audio_1) | | 30  | play(audio_2) |    `

## Remove Rules From a Pronunciation Dictionary

To remove rules from a pronunciation dictionary, we can simply call `remove_rules_from_the_pronunciation_dictionary` method in the pronunciation dictionary module. In the following snippet, we start by removing rules based on the rule string and get the updated result. Finally, we generate and play another text-to-speech audio to test the difference. In the example, we take pronunciation dictionary version id from `remove_rules_from_the_pronunciation_dictionary` response because every changes to pronunciation dictionary will create a new version, so we need to use the latest version returned from the response. The old version also still available.

`  |     |     | | --- | --- | | 1   | pronunciation_dictionary_rules_removed = ( | | 2   | client.pronunciation_dictionary.remove_rules_from_the_pronunciation_dictionary( | | 3   | pronunciation_dictionary_id=pronunciation_dictionary.id, | | 4   | rule_strings=["tomato", "Tomato"], | | 5   | )   | | 6   | )   | | 7   |     | | 8   | audio_3 = client.generate( | | 9   | text="With the rule removed: tomato", | | 10  | voice="Rachel", | | 11  | model="eleven_turbo_v2", | | 12  | pronunciation_dictionary_locators=[ | | 13  | PronunciationDictionaryVersionLocator( | | 14  | pronunciation_dictionary_id=pronunciation_dictionary_rules_removed.id, | | 15  | version_id=pronunciation_dictionary_rules_removed.version_id, | | 16  | )   | | 17  | ],  | | 18  | )   | | 19  |     | | 20  | play(audio_3) |    `

## Add Rules to Pronunciation Dictionary

We can add rules directly to the pronunciation dictionary with `PronunciationDictionaryRule_Phoneme` class and call `add_rules_to_the_pronunciation_dictionary` from the pronunciation dictionary. The snippet will demonstrate adding rules with the class and get the updated result. Finally, we generate and play another text-to-speech audio to test the difference. This example also use pronunciation dictionary version returned from `add_rules_to_the_pronunciation_dictionary` to ensure we use the latest dictionary version.

`  |     |     | | --- | --- | | 1   | from elevenlabs import PronunciationDictionaryRule_Phoneme | | 2   |     | | 3   | pronunciation_dictionary_rules_added = client.pronunciation_dictionary.add_rules_to_the_pronunciation_dictionary( | | 4   | pronunciation_dictionary_id=pronunciation_dictionary_rules_removed.id, | | 5   | rules=[ | | 6   | PronunciationDictionaryRule_Phoneme( | | 7   | type="phoneme", | | 8   | alphabet="ipa", | | 9   | string_to_replace="tomato", | | 10  | phoneme="/tə'meɪtoʊ/", | | 11  | ),  | | 12  | PronunciationDictionaryRule_Phoneme( | | 13  | type="phoneme", | | 14  | alphabet="ipa", | | 15  | string_to_replace="Tomato", | | 16  | phoneme="/tə'meɪtoʊ/", | | 17  | ),  | | 18  | ],  | | 19  | )   | | 20  |     | | 21  | audio_4 = client.generate( | | 22  | text="With the rule added again: tomato", | | 23  | voice="Rachel", | | 24  | model="eleven_turbo_v2", | | 25  | pronunciation_dictionary_locators=[ | | 26  | PronunciationDictionaryVersionLocator( | | 27  | pronunciation_dictionary_id=pronunciation_dictionary_rules_added.id, | | 28  | version_id=pronunciation_dictionary_rules_added.version_id, | | 29  | )   | | 30  | ],  | | 31  | )   | | 32  |     | | 33  | play(audio_4) |    `

## Conclusion

You know how to use a pronunciation dictionary for generating text-to-speech audio. These functionailities open up opportunities to generate text-to-speech audio based on your pronunciation dictionary, making it more flexible for your use case.

For more details, visit our [example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/pronunciation-dictionaries/python)
to see the full project files which give a clear structure for setting up your application:

- `env.example`: Template for your environment variables.
- `main.py`: The complete code for snippets above.
- `dictionary.pls`: Custom dictionary example with XML format.
- `requirements.txt`: List of python package used for this example.

If you have any questions please create an issue on the [elevenlabs-doc Github](https://github.com/elevenlabs/elevenlabs-docs/issues)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Changelog — ElevenLabs Documentation

---

[February 17, 2025](/docs/changelog/2025/2/17)

### Conversational AI

- **Tool calling fix**: Fixed an issue where tool calling was not working with agents using gpt-4o mini. This was due to a breaking change in the OpenAI API.
- **Tool calling improvements**: Added support for tool calling with dynamic variables inside objects and arrays.
- **Dynamic variables**: Fixed an issue where dynamic variables of a conversation were not being displayed correctly.

### Voice Isolator

- **Fixed**: Fixed an issue that caused the voice isolator to not work correctly temporarily.

### Workspace

- **Billing**: Improved billing visibility by differentiating rollover, cycle, gifted, and usage-based credits.
- **Usage Analytics**: Improved usage analytics load times and readability.
- **Fine grained fiat billing**: Added support for customizable pricing based on several factors.

### API

###### View API changes

- Added `phone_numbers` property to [Agent responses](/docs/api-reference/agents/get-agent)
- Added usage metrics to subscription_extras in [User endpoint](/docs/api-reference/user/get)
  :
  - `unused_characters_rolled_over_from_previous_period`
  - `overused_characters_rolled_over_from_previous_period`
  - `usage` statistics
- Added `enable_conversation_initiation_client_data_from_webhook` to [Agent creation](/docs/api-reference/agents/create-agent)
- Updated [Agent](/docs/api-reference/agents)
  endpoints with consolidated settings for:
  - `platform_settings`
  - `overrides`
  - `safety`
- Deprecated `with_settings` parameter in [Voice retrieval endpoint](/docs/api-reference/voices/get)

---

[February 10, 2025](/docs/changelog/2025/2/10)

## Conversational AI

- **Updated Pricing**: Updated self-serve pricing for Conversational AI with [reduced cost and a more generous free tier](/docs/conversational-ai/overview#pricing-tiers)
  .
- **Knowledge Base UI**: Created a new page to easily manage your [knowledge base](/app/conversational-ai/knowledge-base)
  .
- **Live calls**: Added number of live calls in progress in the user [dashboard](/app/conversational-ai)
  and as a new endpoint.
- **Retention**: Added ability to customize transcripts and audio recordings [retention settings](/docs/conversational-ai/customization/privacy/retention)
  .
- **Audio recording**: Added a new option to [disable audio recordings](/docs/conversational-ai/customization/privacy/audio-saving)
  .
- **8k PCM support**: Added support for 8k PCM audio for both input and output.

## Studio

- **GenFM**: Updated the create podcast endpoint to accept [multiple input sources](/docs/api-reference/projects/create-podcast)
  .
- **GenFM**: Fixed an issue where GenFM was creating empty podcasts.

## Enterprise

- **New workspace group endpoints**: Added new endpoints to manage [workspace groups](/docs/api-reference/workspace/search-user-groups)
  .

### API

###### Deprecated Endpoints

**Studio (formerly Projects)**

All `/v1/projects/*` endpoints have been deprecated in favor of the new `/v1/studio/projects/*` endpoints. The following endpoints are now deprecated:

- All operations on `/v1/projects/`
- All operations related to chapters, snapshots, and content under `/v1/projects/*`

**Conversational AI**

- `POST /v1/convai/add-tool` - Use `POST /v1/convai/tools` instead

###### Breaking Changes

- `DELETE /v1/convai/agents/{agent_id}` - Response type is no longer an object
- `GET /v1/convai/tools` - Response type changed from array to object with a `tools` property

###### Modified Endpoints

**Conversational AI Updates**

- `GET /v1/convai/agents/{agent_id}` - Updated conversation configuration and agent properties
- `PATCH /v1/convai/agents/{agent_id}` - Added `use_tool_ids` parameter for tool management
- `POST /v1/convai/agents/create` - Added tool integration via `use_tool_ids`

**Knowledge Base & Tools**

- `GET /v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
- `GET /v1/convai/knowledge-base/{documentation_id}` - Added `name` and `access_level` properties
- `GET /v1/convai/tools/{tool_id}` - Added `dependent_agents` property
- `PATCH /v1/convai/tools/{tool_id}` - Added `dependent_agents` property

**GenFM**

- `POST /v1/projects/podcast/create` - Added support for multiple input sources

###### New Endpoints

**Studio (formerly Projects)**

New endpoints replacing the deprecated `/v1/projects/*` endpoints

- `GET /v1/studio/projects`: List all projects
- `POST /v1/studio/projects`: Create a project
- `GET /v1/studio/projects/{project_id}`: Get project details
- `DELETE /v1/studio/projects/{project_id}`: Delete a project

**Knowledge Base Management**

- `GET /v1/convai/knowledge-base`: List all knowledge base documents
- `DELETE /v1/convai/knowledge-base/{documentation_id}`: Delete a knowledge base
- `GET /v1/convai/knowledge-base/{documentation_id}/dependent-agents`: List agents using this knowledge base

**Workspace Groups** - New enterprise features for team management

- `GET /v1/workspace/groups/search`: Search workspace groups
- `POST /v1/workspace/groups/{group_id}/members`: Add members to a group
- `POST /v1/workspace/groups/{group_id}/members/remove`: Remove members from a group

**Tools**

- `POST /v1/convai/tools`: Create new tools for agents

## Socials

- **ElevenLabs Developers**: Follow our new developers account on X [@ElevenLabsDevs](https://x.com/ElevenLabsDevs)

---

[February 4, 2025](/docs/changelog/2025/2/4)

### Conversational AI

- **Agent monitoring**: Added a new dashboard for monitoring conversational AI agents’ activity. Check out your’s [here](/app/conversational-ai)
  .
- **Proactive conversations**: Enhanced capabilities with improved timeout retry logic. [Learn more](/docs/conversational-ai/customization/conversation-flow)
- **Tool calls**: Fixed timeout issues occurring during tool calls
- **Allowlist**: Fixed implementation of allowlist functionality.
- **Content summarization**: Added Gemini as a fallback model to ensure service reliability
- **Widget stability**: Fixed issue with dynamic variables causing the Conversational AI widget to fail

### Reader

- **Trending content**: Added carousel showcasing popular articles and trending content
- **New publications**: Introduced dedicated section for recent ElevenReader Publishing releases

### Studio (formerly Projects)

- **Projects is now Studio** and is now generally available to everyone
- **Chapter content editing**: Added support for editing chapter content through the public API, enabling programmatic updates to chapter text and metadata
- **GenFM public API**: Added public API support for podcast creation through GenFM. Key features include:
  - Conversation mode with configurable host and guest voices
  - URL-based content sourcing
  - Customizable duration and highlights
  - Webhook callbacks for status updates
  - Project snapshot IDs for audio downloads

### SDKs

- **Swift**: fixed an issue where resources were not being released after the end of a session
- **Python**: added uv support
- **Python**: fixed an issue where calls were not ending correctly

### API

###### View API changes

- Added POST `v1/workspace/invites/add-bulk` [endpoint](/docs/api-reference/workspace/invite-multiple-users)
  to enable inviting multiple users simultaneously
- Added POST `v1/projects/podcast/create` [endpoint](/docs/api-reference/projects/create-podcast)
  for programmatic podcast generation through GenFM
- Added ‘v1/convai/knowledge-base/:documentation_id’ [endpoints](/docs/api-reference/knowledge-base/)
  with CRUD operations for Conversational AI
- Added `v1/convai/tools` [tool management endpoints](/docs/api-reference/tools)
  for extending Conversational AI agent capabilities
- Added PATCH `v1/projects/:project_id/chapters/:chapter_id` [endpoint](/docs/api-reference/chapters/patch-chapter)
  for updating project chapter content and metadata
- Added `group_ids` parameter to [Workspace Invite endpoint](/docs/api-reference/workspace/invite-user)
  for group-based access control
- Added structured `content` property to [Chapter response objects](/docs/api-reference/chapters/get-chapter)
- Added `retention_days` and `delete_transcript_and_pii` data retention parameters to [Agent creation](/docs/api-reference/agents/create-agent)
- Added structured response to [AudioNative content](/docs/api-reference/audio-native/create#response.body.project_id)
- Added `convai_chars_per_minute` usage metric to [User endpoint](/docs/api-reference/user/get)
- Added `media_metadata` field to [Dubbing response objects](/docs/api-reference/dubbing/get)
- Added GDPR-compliant `deletion_settings` to [Conversation responses](/docs/api-reference/conversations/get-conversation#response.body.metadata.deletion_settings)
- Deprecated Knowledge Base legacy endpoints:
  - POST `/v1/convai/agents/{agent_id}/add-to-knowledge-base`
  - GET `/v1/convai/agents/{agent_id}/knowledge-base/{documentation_id}`
- Updated Agent endpoints with consolidated [privacy control parameters](/docs/api-reference/agents/create-agent)

---

[January 27, 2025](/docs/changelog/2025/1/27)

### Docs

- **Shipped our new docs**: we’re keen to hear your thoughts, you can reach out by opening an issue on [GitHub](https://github.com/elevenlabs/elevenlabs-docs)
  or chatting with us on [Discord](https://discord.gg/elevenlabs)

### Conversational AI

- **Dynamic variables**: Available in the dashboard and SDKs. [Learn more](/docs/conversational-ai/customization/dynamic-variables)
- **Interruption handling**: Now possible to ignore user interruptions in Conversational AI. [Learn more](/docs/conversational-ai/customization/interruptions)
- **Twilio integration**: Shipped changes to increase audio quality when integrating with Twilio
- **Latency optimization**: Published detailed blog post on latency optimizations. [Read more](/blog/how-do-you-optimize-latency-for-conversational-ai)
- **PCM 8000**: Added support for PCM 8000 to Conversational AI agents
- **Websocket improvements**: Fixed unexpected websocket closures

### Projects

- **Auto-regenerate**: Auto-regeneration now available by default at no extra cost
- **Content management**: Added `updateContent` method for dynamic content updates
- **Audio conversion**: New auto-convert and auto-publish flags for seamless workflows

### API

###### View API changes

- Added `Update Project` endpoint for [project editing](/docs/api-reference/projects/edit-project#:~:text=List%20projects-,POST,Update%20project,-GET)
- Added `Update Content` endpoint for [AudioNative content management](/docs/api-reference/audio-native/update-content)
- Deprecated `quality_check_on` parameter in [project operations](/docs/api-reference/projects/add-project#request.body.quality_check_on)
  . It is now enabled for all users at no extra cost
- Added `apply_text_normalization` parameter to project creation with modes ‘auto’, ‘on’, ‘apply_english’ and ‘off’ for controlling text normalization during [project creation](/docs/api-reference/projects/add-project#request.body.apply_text_normalization)
- Added alpha feature `auto_assign_voices` in [project creation](/docs/api-reference/projects/add-project#request.body.auto_assign_voices)
  to automatically assign voices to phrases
- Added `auto_convert` flag to project creation to automatically convert [projects to audio](/docs/api-reference/audio-native/create#request.body.auto_convert)
- Added support for creating Conversational AI agents with [dynamic variables](/docs/api-reference/agents/create-agent#request.body.conversation_config.agent.dynamic_variables)
- Added `voice_slots_used` to `Subscription` model to track number of custom voices used in a workspace to the `User` [endpoint](/docs/api-reference/user/get-subscription#response.body.voice_slots_used)
- Added `user_id` field to `User` [endpoint](/docs/api-reference/user/get#response.body.user_id)
- Marked legacy AudioNative creation parameters (`image`, `small`, `sessionization`) as deprecated [parameters](/docs/api-reference/audio-native/create#request.body.image)
- Agents platform now supports `call_limits` containing either `agent_concurrency_limit` or `daily_limit` or both parameters to control simultaneous and daily conversation limits for [agents](/docs/docs/api-reference/agents/create-agent#request.body.platform_settings.call_limits)
- Added support for `language_presets` in `conversation_config` to customize language-specific [settings](/docs/api-reference/agents/create-agent#request.body.conversation_config.language_presets)

### SDKs

- **Cross-Runtime Support**: Now compatible with **Bun 1.1.45+** and **Deno 2.1.7+**
- **Regenerated SDKs**: We regenerated our SDKs to be up to date with the latest API spec. Check out the latest [Python SDK release](https://github.com/elevenlabs/elevenlabs-python/releases/tag/1.50.5)
  and [JS SDK release](https://github.com/elevenlabs/elevenlabs-js/releases/tag/v1.50.4)
- **Dynamic Variables**: Fixed an issue where dynamic variables were not being handled correctly, they are now correctly handled in all SDKs

---

[January 16, 2025](/docs/changelog/2025/1/16)

## Product

### Conversational AI

- **Additional languages**: Add a language dropdown to your widget so customers can launch conversations in their preferred language. Learn more [here](/docs/conversational-ai/customization/language)
  .
- **End call tool**: Let the agent automatically end the call with our new “End Call” tool. Learn more [here](/docs/conversational-ai/customization/tools)
- **Flash default**: Flash, our lowest latency model, is now the default for new agents. In your agent dashboard under “voice”, you can toggle between Turbo and Flash. Learn more about Flash [here](https://elevenlabs.io/blog/meet-flash)
  .
- **Privacy**: Set concurrent call and daily call limits, turn off audio recordings, add feedback collection, and define customer terms & conditions.
- **Increased tool limits**: Increase the number of tools available to your agent from 5 to 15. Learn more [here](/docs/conversational-ai/customization/tools)
  .

---

[January 2, 2025](/docs/changelog/2025/1/2)

## Product

- **Workspace Groups and Permissions**: Introduced new workspace group management features to enhance access control within organizations. [Learn more](https://elevenlabs.io/blog/workspace-groups-and-permissions)
  .

---

[December 19, 2024](/docs/changelog/2024/12/19)

## Model

- **Introducing Flash**: Our fastest text-to-speech model yet, generating speech in just 75ms. Access it via the API with model IDs `eleven_flash_v2` and `eleven_flash_v2_5`. Perfect for low-latency conversational AI applications. [Try it now](https://elevenlabs.io/docs/api-reference/text-to-speech)
  .

## Launches

- **[TalkToSanta.io](https://www.talktosanta.io/)
  **: Experience Conversational AI in action by talking to Santa this holiday season. For every conversation with santa we donate 2 dollars to [Bridging Voice](https://www.bridgingvoice.org/)
  (up to $11,000).
- **[AI Engineer Pack](https://aiengineerpack.com/)
  **: Get $50+ in credits from leading AI developer tools, including ElevenLabs.

---

[December 6, 2024](/docs/changelog/2024/12/6)

## Product

- **GenFM Now on Web**: Access GenFM directly from the website in addition to the ElevenReader App, [try it now](https://elevenlabs.io/app/projects)
  .

---

[December 3, 2024](/docs/changelog/2024/12/3)

## API

- **Credit Usage Limits**: Set specific credit limits for API keys to control costs and manage usage across different use cases by setting “Access” or “No Access” to features like Dubbing, Audio Native, and more. [Check it out](https://elevenlabs.io/app/settings/api-keys)
- **Workspace API Keys**: Now support access permissions, such as “Read” or “Read and Write” for User, Workspace, and History resources.
- **Improved Key Management**:
  - Redesigned interface moving from modals to dedicated pages
  - Added detailed descriptions and key information
  - Enhanced visibility of key details and settings

---

[November 29, 2024](/docs/changelog/2024/11/29)

## Product

- **GenFM**: Launched in the ElevenReader app. [Learn more](https://elevenlabs.io/blog/genfm-on-elevenreader)
- **Conversational AI**: Now generally available to all customers. [Try it now](https://elevenlabs.io/conversational-ai)
- **TTS Redesign**: The website TTS redesign is now rolled out to all customers.
- **Auto-regenerate**: Now available in Projects. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)
- **Reader Platform Improvements**:

  - Improved content sharing with enhanced landing pages and social media previews.
  - Added podcast rating system and improved voice synchronization.

- **Projects revamp**:

  - Restore past generations, lock content, assign speakers to sentence fragments, and QC at 2x speed. [Learn more](https://elevenlabs.io/blog/narrate-any-project)
  - Auto-regeneration identifies mispronunciations and regenerates audio at no extra cost. [Learn more](https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects)

## API

- **Conversational AI**: [SDKs and APIs](https://elevenlabs.io/docs/conversational-ai/docs/introduction)
  now available.

[Older posts\
\
Previous](/docs/changelog#page-2)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Latency optimization — ElevenLabs Documentation

This guide covers the core principles for improving text-to-speech latency.

While there are many individual techniques, we’ll group them into **four principles**.

#### Four principles

1.  [Use Flash models](/docs/best-practices/latency-optimization#use-flash-models)
2.  [Leverage streaming](/docs/best-practices/latency-optimization#leverage-streaming)
3.  [Consider geographic proximity](/docs/best-practices/latency-optimization#consider-geographic-proximity)
4.  [Choose appropriate voices](/docs/best-practices/latency-optimization#choose-appropriate-voices)

Enterprise customers benefit from increased concurrency limits and priority access to our rendering queue. [Contact sales](https://elevenlabs.io/contact-sales)
to learn more about our enterprise plans.

## Use Flash models

[Flash models](/docs/models#flash-v25)
deliver ~75ms inference speeds, making them ideal for real-time applications. The trade-off is a slight reduction in audio quality compared to [Multilingual v2](/docs/models#multilingual-v2)
.

75ms refers to model inference time only. Actual end-to-end latency will vary with factors such as your location & endpoint type used.

## Leverage streaming

There are three types of text-to-speech endpoints available in our [API Reference](/docs/api-reference)
:

- **Regular endpoint**: Returns a complete audio file in a single response.
- **Streaming endpoint**: Returns audio chunks progressively using [Server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events)
  .
- **Websockets endpoint**: Enables bidirectional streaming for real-time audio generation.

### Streaming

Streaming endpoints progressively return audio as it is being generated in real-time, reducing the time-to-first-byte. This endpoint is recommended for cases where the input text is available up-front.

Streaming is supported for the [Text to Speech](/docs/api-reference/text-to-speech/convert-as-stream)
API, [Voice Changer](/docs/api-reference/speech-to-speech/convert-as-stream)
API & [Audio Isolation](/docs/api-reference/audio-isolation/audio-isolation-stream)
API.

### Websockets

The [text-to-speech websocket endpoint](/docs/api-reference#text-to-speech-websocket)
supports bidirectional streaming making it perfect for applications with real-time text input (e.g. LLM outputs).

Setting `auto_mode` to true automatically handles generation triggers, removing the need to manually manage chunk strategies.

If `auto_mode` is disabled, the model will wait for enough text to match the chunk schedule before starting to generate audio.

For instance, if you set a chunk schedule of 125 characters but only 50 arrive, the model stalls until additional characters come in—potentially increasing latency.

For implementation details, see the [text-to-speech websocket guide](/docs/api-reference#text-to-speech-websocket)
.

## Consider geographic proximity

Because our models are served in the US, your geographic location will affect the network latency you experience.

For example, using Flash models with Websockets, you can expect the following TTFB latencies:

| Region          | TTFB      |
| --------------- | --------- |
| US              | 150-200ms |
| EU              | ~230ms    |
| North East Asia | 250-350ms |
| South Asia      | 380-440ms |

We are actively working on deploying our models in EU and Asia. These deployments will bring speeds closer to those experienced by US customers.

## Choose appropriate voices

We have observed that in some cases, voice selection can impact latency. Here’s the order from fastest to slowest:

1.  Default voices (formerly premade), Synthetic voices, and Instant Voice Clones (IVC)
2.  Professional Voice Clones (PVC)

Higher audio quality output formats can increase latency. Be sure to balance your latency requirements with audio fidelity needs.

We are actively working on optimizing PVC latency for Flash v2.5.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Text to Speech — ElevenLabs Documentation

## Overview

ElevenLabs [Text to Speech (TTS)](/docs/api-reference/text-to-speech)
API turns text into lifelike audio with nuanced intonation, pacing and emotional awareness. [Our models](/docs/models)
adapt to textual cues across 32 languages and multiple voice styles and can be used to:

- Narrate global media campaigns & ads
- Produce audiobooks in multiple languages with complex emotional delivery
- Stream real-time audio from text

Listen to a sample:

Explore our [voice library](https://elevenlabs.io/community)
to find the perfect voice for your project.

[Developer quickstart\
\
Learn how to integrate text to speech into your application.](/docs/quickstart)
[Product guide\
\
Step-by-step guide for using text to speech in ElevenLabs.](/docs/product-guides/playground/text-to-speech)

### Voice quality

For real-time applications, Flash v2.5 provides ultra-low 75ms latency, while Multilingual v2 delivers the highest quality audio with more nuanced expression.

[Eleven Multilingual v2\
\
Our most lifelike, emotionally rich speech synthesis model\
\
Most natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Rich emotional expression](/docs/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](/docs/models#flash-v25)

[Explore all](/docs/models)

### Voice options

ElevenLabs offers thousands of voices across 32 languages through multiple creation methods:

- [Voice library](/docs/capabilities/voices)
  with 3,000+ community-shared voices
- [Professional voice cloning](/docs/capabilities/voices#cloned)
  for highest-fidelity replicas
- [Instant voice cloning](/docs/capabilities/voices#cloned)
  for quick voice replication
- [Voice design](/docs/capabilities/voices#voice-design)
  to generate custom voices from text descriptions

Learn more about our [voice options](/docs/capabilities/voices)
.

### Supported formats

The default response format is “mp3”, but other formats like “PCM”, & “μ-law” are available.

- **MP3**
  - Sample rates: 22.05kHz - 44.1kHz
  - Bitrates: 32kbps - 192kbps
- **PCM (S16LE)**
  - Sample rates: 16kHz - 44.1kHz
- **μ-law**
  - 8kHz sample rate
  - Optimized for telephony applications

Higher quality audio options are only available on paid tiers - see our [pricing page](https://elevenlabs.io/pricing)
for details.

### Supported languages

Our v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

Simply input text in any of our supported languages and select a matching voice from our [voice library](https://elevenlabs.io/community)
. For the most natural results, choose a voice with an accent that matches your target language and region.

### Prompting

The models interpret emotional context directly from the text input. For example, adding descriptive text like “she said excitedly” or using exclamation marks will influence the speech emotion. Voice settings like Stability and Similarity help control the consistency, while the underlying emotion comes from textual cues.

Read the [prompting guide](/docs/best-practices/prompting)
for more details.

Descriptive text will be spoken out by the model and must be manually trimmed or removed from the audio if desired.

## FAQ

###### Can I clone my own voice?

Yes, you can create [instant voice clones](/docs/capabilities/voices#cloned)
of your own voice from short audio clips. For high-fidelity clones, check out our [professional voice cloning](/docs/capabilities/voices#cloned)
feature.

###### Do I own the audio output?

Yes. You retain ownership of any audio you generate. However, commercial usage rights are only available with paid plans. With a paid subscription, you may use generated audio for commercial purposes and monetize the outputs if you own the IP rights to the input content.

###### What qualifies as a free regeneration?

A free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:

- You can regenerate each piece of content up to 2 times for free
- The content must be exactly the same as the previous generation. Any changes to the text, voice settings, or other parameters will require a new, paid generation

Free regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs’ internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.

###### How do I reduce latency for real-time cases?

Use the low-latency Flash [models](/docs/models)
(Flash v2 or v2.5) optimized for near real-time conversational or interactive scenarios. See our [latency optimization guide](/docs/best-practices/latency-optimization)
for more details.

###### Why is my output sometimes inconsistent?

The models are nondeterministic. For consistency, use the optional [seed parameter](/docs/api-reference/text-to-speech/convert#request.body.seed)
, though subtle differences may still occur.

###### What's the best practice for large text conversions?

Split long text into segments and use streaming for real-time playback and efficient processing. To maintain natural prosody flow between chunks, include [previous/next text or previous/next request id parameters](/docs/api-reference/text-to-speech/convert#request.body.previous_text)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Stitching multiple requests — ElevenLabs Documentation

## What is Request Stitching?

When one has a large text to convert into audio and sends the text in chunks without further context there can be abrupt changes in prosody from one chunk to another.

It would be much better to give the model context on what was already generated and what will be generated in the future, this is exactly what Request Stitching does.

As you can see below the difference between not using Request Stitching and using it is subtle but noticeable:

#### Without Request Stitching:

#### With Request Stitching:

## Conditioning on text

We will use Pydub for concatenating multiple audios together, you can install it using:

`  |     |     | | --- | --- | | $   | pip install pydub |    `

One of the two ways on how to give the model context is to provide the text before and / or after the current chunk by using the ‘previous_text’ and ‘next_text’ parameters:

`  |     |     | | --- | --- | | 1   | import os | | 2   | import requests | | 3   | from pydub import AudioSegment | | 4   | import io | | 5   |     | | 6   | YOUR_XI_API_KEY = "<insert your xi-api-key here>" | | 7   | VOICE_ID = "21m00Tcm4TlvDq8ikWAM"  # Rachel | | 8   | PARAGRAPHS = [ | | 9   | "The advent of technology has transformed countless sectors, with education " | | 10  | "standing out as one of the most significantly impacted fields.", | | 11  | "In recent years, educational technology, or EdTech, has revolutionized the way " | | 12  | "teachers deliver instruction and students absorb information.", | | 13  | "From interactive whiteboards to individual tablets loaded with educational software, " | | 14  | "technology has opened up new avenues for learning that were previously unimaginable.", | | 15  | "One of the primary benefits of technology in education is the accessibility it provides.", | | 16  | ]   | | 17  | segments = [] | | 18  |     | | 19  | for i, paragraph in enumerate(PARAGRAPHS): | | 20  | is_last_paragraph = i == len(PARAGRAPHS) - 1 | | 21  | is_first_paragraph = i == 0 | | 22  | response = requests.post( | | 23  | f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream", | | 24  | json={ | | 25  | "text": paragraph, | | 26  | "model_id": "eleven_multilingual_v2", | | 27  | "previous_text": None if is_first_paragraph else " ".join(PARAGRAPHS[:i]), | | 28  | "next_text": None if is_last_paragraph else " ".join(PARAGRAPHS[i + 1:]) | | 29  | },  | | 30  | headers={"xi-api-key": YOUR_XI_API_KEY}, | | 31  | )   | | 32  |     | | 33  | if response.status_code != 200: | | 34  | print(f"Error encountered, status: {response.status_code}, " | | 35  | f"content: {response.text}") | | 36  | quit() | | 37  |     | | 38  | print(f"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}") | | 39  | segments.append(AudioSegment.from_mp3(io.BytesIO(response.content))) | | 40  |     | | 41  | segment = segments[0] | | 42  | for new_segment in segments[1:]: | | 43  | segment = segment + new_segment | | 44  |     | | 45  | audio_out_path = os.path.join(os.getcwd(), "with_text_conditioning.wav") | | 46  | segment.export(audio_out_path, format="wav") | | 47  | print(f"Success! Wrote audio to {audio_out_path}") |    `

## Conditioning on past generations

Text conditioning works well when there has been no previous or next chunks generated yet. If there have been however, it works much better to provide the actual past generations to the model instead of just the text. This is done by using the previous_request_ids and next_request_ids parameters.

Every text-to-speech request has an associated request-id which is obtained by reading from the response header. Below is an example on how to use this request_id in order to condition requests on the previous generations.

`  |     |     | | --- | --- | | 1   | import os | | 2   | import requests | | 3   | from pydub import AudioSegment | | 4   | import io | | 5   |     | | 6   | YOUR_XI_API_KEY = "<insert your xi-api-key here>" | | 7   | VOICE_ID = "21m00Tcm4TlvDq8ikWAM"  # Rachel | | 8   | PARAGRAPHS = [ | | 9   | "The advent of technology has transformed countless sectors, with education " | | 10  | "standing out as one of the most significantly impacted fields.", | | 11  | "In recent years, educational technology, or EdTech, has revolutionized the way " | | 12  | "teachers deliver instruction and students absorb information.", | | 13  | "From interactive whiteboards to individual tablets loaded with educational software, " | | 14  | "technology has opened up new avenues for learning that were previously unimaginable.", | | 15  | "One of the primary benefits of technology in education is the accessibility it provides.", | | 16  | ]   | | 17  | segments = [] | | 18  | previous_request_ids = [] | | 19  |     | | 20  | for i, paragraph in enumerate(PARAGRAPHS): | | 21  | response = requests.post( | | 22  | f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream", | | 23  | json={ | | 24  | "text": paragraph, | | 25  | "model_id": "eleven_multilingual_v2", | | 26  | # A maximum of three next or previous history item ids can be send | | 27  | "previous_request_ids": previous_request_ids[-3:], | | 28  | },  | | 29  | headers={"xi-api-key": YOUR_XI_API_KEY}, | | 30  | )   | | 31  |     | | 32  | if response.status_code != 200: | | 33  | print(f"Error encountered, status: {response.status_code}, " | | 34  | f"content: {response.text}") | | 35  | quit() | | 36  |     | | 37  | print(f"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}") | | 38  | previous_request_ids.append(response.headers["request-id"]) | | 39  | segments.append(AudioSegment.from_mp3(io.BytesIO(response.content))) | | 40  |     | | 41  | segment = segments[0] | | 42  | for new_segment in segments[1:]: | | 43  | segment = segment + new_segment | | 44  |     | | 45  | audio_out_path = os.path.join(os.getcwd(), "with_previous_request_ids_conditioning.wav") | | 46  | segment.export(audio_out_path, format="wav") | | 47  | print(f"Success! Wrote audio to {audio_out_path}") |    `

**Note that the order matters here**: When one converts a text split into 5 chunks and has already converted chunks 1, 2, 4 and 5 and now wants to convert chunk 3 the previous_request_ids one neeeds to send would be \[request_id_chunk_1, request_id_chunk_2\] and the next_request_ids would be \[request_id_chunk_4, request_id_chunk_5\].

## Conditioning both on text and past generations

The best possible results are achieved when conditioning both on text and past generations so lets combine the two by providing previous_text, next_text and previous_request_ids in one request:

`  |     |     | | --- | --- | | 1   | import os | | 2   | import requests | | 3   | from pydub import AudioSegment | | 4   | import io | | 5   |     | | 6   | YOUR_XI_API_KEY = "<insert your xi-api-key here>" | | 7   | VOICE_ID = "21m00Tcm4TlvDq8ikWAM"  # Rachel | | 8   | PARAGRAPHS = [ | | 9   | "The advent of technology has transformed countless sectors, with education " | | 10  | "standing out as one of the most significantly impacted fields.", | | 11  | "In recent years, educational technology, or EdTech, has revolutionized the way " | | 12  | "teachers deliver instruction and students absorb information.", | | 13  | "From interactive whiteboards to individual tablets loaded with educational software, " | | 14  | "technology has opened up new avenues for learning that were previously unimaginable.", | | 15  | "One of the primary benefits of technology in education is the accessibility it provides.", | | 16  | ]   | | 17  | segments = [] | | 18  | previous_request_ids = [] | | 19  |     | | 20  | for i, paragraph in enumerate(PARAGRAPHS): | | 21  | is_first_paragraph = i == 0 | | 22  | is_last_paragraph = i == len(PARAGRAPHS) - 1 | | 23  | response = requests.post( | | 24  | f"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream", | | 25  | json={ | | 26  | "text": paragraph, | | 27  | "model_id": "eleven_multilingual_v2", | | 28  | # A maximum of three next or previous history item ids can be send | | 29  | "previous_request_ids": previous_request_ids[-3:], | | 30  | "previous_text": None if is_first_paragraph else " ".join(PARAGRAPHS[:i]), | | 31  | "next_text": None if is_last_paragraph else " ".join(PARAGRAPHS[i + 1:]) | | 32  | },  | | 33  | headers={"xi-api-key": YOUR_XI_API_KEY}, | | 34  | )   | | 35  |     | | 36  | if response.status_code != 200: | | 37  | print(f"Error encountered, status: {response.status_code}, " | | 38  | f"content: {response.text}") | | 39  | quit() | | 40  |     | | 41  | print(f"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}") | | 42  | previous_request_ids.append(response.headers["request-id"]) | | 43  | segments.append(AudioSegment.from_mp3(io.BytesIO(response.content))) | | 44  |     | | 45  | segment = segments[0] | | 46  | for new_segment in segments[1:]: | | 47  | segment = segment + new_segment | | 48  |     | | 49  | audio_out_path = os.path.join(os.getcwd(), "with_full_conditioning.wav") | | 50  | segment.export(audio_out_path, format="wav") | | 51  | print(f"Success! Wrote audio to {audio_out_path}") |    `

## Things to note

1.  Providing wrong previous_request_ids and next_request_ids will not result in an error.
2.  In order to use the request_id of a request for conditioning it needs to have processed completely. In case of streaming this means the audio has to be read completely from the response body.
3.  How well Request Stitching works varies greatly dependent on the model, voice and voice settings used.
4.  previous_request_ids and next_request_ids should contain request_ids which are not too old. When the request_ids are older than two hours it will diminish the effect of conditioning.
5.  Enterprises with increased privacy requirements will have Request Stitching disabled.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Sending generated audio through Twilio — ElevenLabs Documentation

In this guide, you’ll learn how to send an AI generated message through a phone call using Twilio and ElevenLabs. This process allows you to send high-quality voice messages directly to your callers.

## Create accounts with Twilio and ngrok

We’ll be using Twilio and ngrok for this guide, so go ahead and create accounts with them.

- [twilio.com](https://www.twilio.com/)
- [ngrok.com](https://ngrok.com/)

## Get the code

If you want to get started quickly, you can get the entire code for this guide on [GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/twilio/call)

## Create the server with Express

### Initialize your project

Create a new folder for your project

`  |     | | --- | | mkdir elevenlabs-twilio | | cd elevenlabs-twilio | | npm init -y |    `

### Install dependencies

`  npm install elevenlabs express express-ws twilio  `

### Install dev dependencies

`  npm i @types/node @types/express @types/express-ws @types/ws dotenv tsx typescript  `

### Create your files

``  |     |     | | --- | --- | | 1   | // src/app.ts | | 2   | import 'dotenv/config'; | | 3   | import { ElevenLabsClient } from 'elevenlabs'; | | 4   | import express, { Response } from 'express'; | | 5   | import ExpressWs from 'express-ws'; | | 6   | import { Readable } from 'stream'; | | 7   | import VoiceResponse from 'twilio/lib/twiml/VoiceResponse'; | | 8   | import { type WebSocket } from 'ws'; | | 9   |     | | 10  | const app = ExpressWs(express()).app; | | 11  | const PORT: number = parseInt(process.env.PORT \| '5000'); | | 12  |     | | 13  | const elevenlabs = new ElevenLabsClient(); | | 14  | const voiceId = '21m00Tcm4TlvDq8ikWAM'; | | 15  | const outputFormat = 'ulaw_8000'; | | 16  | const text = 'This is a test. You can now hang up. Thank you.'; | | 17  |     | | 18  | function startApp() { | | 19  | app.post('/call/incoming', (_, res: Response) => { | | 20  | const twiml = new VoiceResponse(); | | 21  |     | | 22  | twiml.connect().stream({ | | 23  | url: `wss://${process.env.SERVER_DOMAIN}/call/connection`, | | 24  | }); | | 25  |     | | 26  | res.writeHead(200, { 'Content-Type': 'text/xml' }); | | 27  | res.end(twiml.toString()); | | 28  | }); | | 29  |     | | 30  | app.ws('/call/connection', (ws: WebSocket) => { | | 31  | ws.on('message', async (data: string) => { | | 32  | const message: { | | 33  | event: string; | | 34  | start?: { streamSid: string; callSid: string }; | | 35  | } = JSON.parse(data); | | 36  |     | | 37  | if (message.event === 'start' && message.start) { | | 38  | const streamSid = message.start.streamSid; | | 39  | const response = await elevenlabs.textToSpeech.convert(voiceId, { | | 40  | model_id: 'eleven_flash_v2_5', | | 41  | output_format: outputFormat, | | 42  | text, | | 43  | }); | | 44  |     | | 45  | const readableStream = Readable.from(response); | | 46  | const audioArrayBuffer = await streamToArrayBuffer(readableStream); | | 47  |     | | 48  | ws.send( | | 49  | JSON.stringify({ | | 50  | streamSid, | | 51  | event: 'media', | | 52  | media: { | | 53  | payload: Buffer.from(audioArrayBuffer as any).toString('base64'), | | 54  | },  | | 55  | })  | | 56  | );  | | 57  | }   | | 58  | }); | | 59  |     | | 60  | ws.on('error', console.error); | | 61  | }); | | 62  |     | | 63  | app.listen(PORT, () => { | | 64  | console.log(`Local: http://localhost:${PORT}`); | | 65  | console.log(`Remote: https://${process.env.SERVER_DOMAIN}`); | | 66  | }); | | 67  | }   | | 68  |     | | 69  | function streamToArrayBuffer(readableStream: Readable) { | | 70  | return new Promise((resolve, reject) => { | | 71  | const chunks: Buffer[] = []; | | 72  |     | | 73  | readableStream.on('data', (chunk) => { | | 74  | chunks.push(chunk); | | 75  | }); | | 76  |     | | 77  | readableStream.on('end', () => { | | 78  | resolve(Buffer.concat(chunks).buffer); | | 79  | }); | | 80  |     | | 81  | readableStream.on('error', reject); | | 82  | }); | | 83  | }   | | 84  |     | | 85  | startApp(); |    ``

`  |     |     | | --- | --- | | 1   | # .env | | 2   | SERVER_DOMAIN= | | 3   | ELEVENLABS_API_KEY= |    `

## Understanding the code

### Handling the incoming call

When you call your number, Twilio makes a POST request to your endpoint at `/call/incoming`. We then use twiml.connect to tell Twilio that we want to handle the call via our websocket by setting the url to our `/call/connection` endpoint.

``  |     |     | | --- | --- | | 1   | function startApp() { | | 2   | app.post('/call/incoming', (_, res: Response) => { | | 3   | const twiml = new VoiceResponse(); | | 4   |     | | 5   | twiml.connect().stream({ | | 6   | url: `wss://${process.env.SERVER_DOMAIN}/call/connection`, | | 7   | }); | | 8   |     | | 9   | res.writeHead(200, { 'Content-Type': 'text/xml' }); | | 10  | res.end(twiml.toString()); | | 11  | }); |    ``

### Creating the text to speech

Here we listen for messages that Twilio sends to our websocket endpoint. When we receive a `start` message event, we generate audio using the ElevenLabs [TypeScript SDK](https://github.com/elevenlabs/elevenlabs-js)
.

`  |     |     | | --- | --- | | 1   | app.ws('/call/connection', (ws: WebSocket) => { | | 2   | ws.on('message', async (data: string) => { | | 3   | const message: { | | 4   | event: string; | | 5   | start?: { streamSid: string; callSid: string }; | | 6   | } = JSON.parse(data); | | 7   |     | | 8   | if (message.event === 'start' && message.start) { | | 9   | const streamSid = message.start.streamSid; | | 10  | const response = await elevenlabs.textToSpeech.convert(voiceId, { | | 11  | model_id: 'eleven_flash_v2_5', | | 12  | output_format: outputFormat, | | 13  | text, | | 14  | }); |    `

### Sending the message

Upon receiving the audio back from ElevenLabs, we convert it to an array buffer and send the audio to Twilio via the websocket.

`  |     |     | | --- | --- | | 1   | const readableStream = Readable.from(response); | | 2   | const audioArrayBuffer = await streamToArrayBuffer(readableStream); | | 3   |     | | 4   | ws.send( | | 5   | JSON.stringify({ | | 6   | streamSid, | | 7   | event: 'media', | | 8   | media: { | | 9   | payload: Buffer.from(audioArrayBuffer as any).toString('base64'), | | 10  | },  | | 11  | })  | | 12  | );  |    `

## Point ngrok to your application

Twilio requires a publicly accessible URL. We’ll use ngrok to forward the local port of our application and expose it as a public URL.

Run the following command in your terminal:

`  ngrok http 5000  `

Copy the ngrok domain (without https://) to use in your environment variables.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/ngrok.webp)

## Update your environment variables

Update the `.env` file with your ngrok domain and ElevenLabs API key.

`  |     | | --- | | # .env | | SERVER_DOMAIN=*******.ngrok.app | | ELEVENLABS_API_KEY=************************* |    `

## Start the application

Run the following command to start the app:

`  npm run dev  `

## Set up Twilio

Follow Twilio’s guides to create a new number. Once you’ve created your number, navigate to the “Configure” tab in Phone Numbers -> Manage -> Active numbers

In the “A call comes in” section, enter the full URL to your application (make sure to add the`/call/incoming` path):

E.g. https://**\*\*\***ngrok.app/call/incoming

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/twilio.webp)

## Make a phone call

Make a call to your number. You should hear a message using the ElevenLabs voice.

## Tips for deploying to production

When running the application in production, make sure to set the `SERVER_DOMAIN` environment variable to that of your server. Be sure to also update the URL in Twilio to point to your production server.

## Conclusion

You should now have a basic understanding of integrating Twilio with ElevenLabs voices. If you have any further questions, or suggestions on how to improve this blog post, please feel free to select the “Suggest edits” or “Raise issue” button below.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Prompting — ElevenLabs Documentation

We are actively working on _Director’s Mode_ to give you even greater control over outputs.

This guide provides techniques to enhance text-to-speech outputs using ElevenLabs models. Experiment with these methods to discover what works best for your needs. These techniques provide a practical way to achieve nuanced results until advanced features like _Director’s Mode_ are rolled out.

## Pauses

Use `<break time="x.xs" />` for natural pauses up to 3 seconds.

Using too many break tags in a single generation can cause instability. The AI might speed up, or introduce additional noises or audio artifacts. We are working on resolving this.

Example

`  "Hold on, let me think." <break time="1.5s" /> "Alright, I’ve got it."  `

- **Consistency:** Use `<break>` tags consistently to maintain natural speech flow. Excessive use can lead to instability.
- **Voice-Specific Behavior:** Different voices may handle pauses differently, especially those trained with filler sounds like “uh” or “ah.”

Alternatives to `<break>` include dashes (- or —) for short pauses or ellipses (…) for hesitant tones. However, these are less consistent.

Example

`  "It… well, it might work." "Wait — what’s that noise?"  `

## Pronunciation

### Phoneme Tags

Specify pronunciation using [SSML phoneme tags](https://en.wikipedia.org/wiki/Speech_Synthesis_Markup_Language)
. Supported alphabets include [CMU](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)
Arpabet and the [International Phonetic Alphabet (IPA)](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)
.

Phoneme tags are only compatible with “Eleven English v1” and “Eleven Turbo v2” [models](/docs/models)
.

CMU Arpabet ExampleIPA Example

`  |     |     | | --- | --- | | 1   | <phoneme alphabet="cmu-arpabet" ph="M AE1 D IH0 S AH0 N"> | | 2   | Madison | | 3   | </phoneme> |    `

We recommend using CMU Arpabet for consistent and predictable results with current AI models. While IPA can be effective, CMU Arpabet generally offers more reliable performance.

Phoneme tags only work for individual words. If for example you have a name with a first and last name that you want to be pronounced a certain way, you will need to create a phoneme tag for each word.

Ensure correct stress marking for multi-syllable words to maintain accurate pronunciation. For example:

Correct usageIncorrect usage

`  |     |     | | --- | --- | | 1   | <phoneme alphabet="cmu-arpabet" ph="P R AH0 N AH0 N S IY EY1 SH AH0 N"> | | 2   | pronunciation | | 3   | </phoneme> |    `

### Alias Tags

For models that don’t support phoneme tags, you can try writing words more phonetically. You can also employ various tricks such as capital letters, dashes, apostrophes, or even single quotation marks around a single letter or letters.

As an example, a word like “trapezii” could be spelt “trapezIi” to put more emphasis on the “ii” of the word.

You can either replace the word directly in your text, or if you want to specify pronunciation using other words or phrases when using a pronunciation dictionary, you can use alias tags for this. This can be useful if you’re generating using Multilingual v2 or Turbo v2.5, which don’t support phoneme tags. You can use pronunciation dictionaries with Studio, Dubbing Studio and Speech Synthesis via the API.

For example, if your text includes a name that has an unusual pronunciation that the AI might struggle with, you could use an alias tag to specify how you would like it to be pronounced:

`|     | | --- | | <lexeme> | | <grapheme>Claughton</grapheme> | | <alias>Cloffton</alias> | | </lexeme> |`

If you want to make sure that an acronym is always delivered in a certain way whenever it is incountered in your text, you can use an alias tag to specify this:

`|     | | --- | | <lexeme> | | <grapheme>UN</grapheme> | | <alias>United Nations</alias> | | </lexeme> |`

### Pronunciation Dictionaries

Some of our tools, such as Studio and Dubbing Studio, allow you to create and upload a pronunciation dictionary. These allow you to specify the pronunciation of certain words, such as character or brand names, or to specify how acronyms should be read.

Pronunciation dictionaries allow this functionality by enabling you to upload a lexicon or dictionary file that specifies pairs of words and how they should be pronounced, either using a phonetic alphabet or word substitutions.

Whenever one of these words is encountered in a project, the AI model will pronounce the word using the specified replacement.

To provide a pronunciation dictionary file, open the settings for a project and upload a file in either TXT or the [.PLS format](https://www.w3.org/TR/pronunciation-lexicon/)
. When a dictionary is added to a project it will automatically recalculate which pieces of the project will need to be re-converted using the new dictionary file and mark these as unconverted.

Currently we only support pronunciation dictionaries that specify replacements using phoneme or alias tags.

Both phonemes and aliases are sets of rules that specify a word or phrase they are looking for, referred to as a grapheme, and what it will be replaced with. Please note that searches are case sensitive. When checking for a replacement word in a pronunciation dictionary, the dictionary is checked from start to end and only the very first replacement is used.

### Pronunciation Dictionary Example

Here is an example pronunciation dictionary that specifies in IPA the pronunciation of “Apple” with IPA of “ˈæpl̩” and “UN” with an alias of “United Nations”:

`  |     | | --- | | <?xml version="1.0" encoding="UTF-8"?> | | <lexicon version="1.0" | | xmlns="http://www.w3.org/2005/01/pronunciation-lexicon" | | xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" | | xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon | | http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd" | | alphabet="ipa" xml:lang="en-GB"> | | <lexeme> | | <grapheme>Apple</grapheme> | | <phoneme>ˈæpl̩</phoneme> | | </lexeme> | | <lexeme> | | <grapheme>UN</grapheme> | | <alias>United Nations</alias> | | </lexeme> | | </lexicon> |    `

To generate a pronunciation dictionary `.pls` file, there are a few open source tools available:

- [Sequitur G2P](https://github.com/sequitur-g2p/sequitur-g2p)
  - Open-source tool that learns pronunciation rules from data and can generate phonetic transcriptions.
- [Phonetisaurus](https://github.com/AdolfVonKleist/Phonetisaurus)
  - Open-source G2P system trained on existing dictionaries like CMUdict.
- [eSpeak](https://github.com/espeak-ng/espeak-ng)
  - Speech synthesizer that can generate phoneme transcriptions from text.
- [CMU Pronouncing Dictionary](https://github.com/cmusphinx/cmudict)
  - A pre-built English dictionary with phonetic transcriptions.

## Emotion

Convey emotions through narrative context or explicit dialogue tags. This approach helps the AI understand the tone and emotion to emulate.

Example

`  You’re leaving?" she asked, her voice trembling with sadness. "That’s it!" he exclaimed triumphantly.  `

Explicit dialogue tags yield more predictable results than relying solely on context, however the model will still speak out the emotional delivery guides. These can be removed in post-production using an audio editor if unwanted.

## Pace

Pacing can be controlled by writing in a natural, narrative style. For voice cloning, longer, continuous samples are recommended to avoid pacing issues like unnaturally fast speech.

Example

`  "I… I thought you’d understand," he said, his voice slowing with disappointment.  `

Sample Length: Use longer, continuous samples for voice cloning to avoid pacing issues.

Narrative Style: Write in a narrative style to naturally control pacing and emotion, similar to scriptwriting.

## Tips

###### Common Issues

- Inconsistent pauses: Ensure `<break time=“x.xs” />` syntax is used for pauses.
- Pronunciation errors: Use CMU Arpabet or IPA phoneme tags for precise pronunciation.
- Emotion mismatch: Add narrative context or explicit tags to guide emotion. **Remember to remove any emotional guidance text in post-production.**

###### Tips for Improving Output

Experiment with alternative phrasing to achieve desired pacing or emotion. For complex sound effects, break prompts into smaller, sequential elements and combine results manually.

## Creative control

While we are actively developing a “Director’s Mode” to give users even greater control over outputs, here are some interim techniques to maximize creativity and precision:

[1](/docs/best-practices/prompting#narrative-styling)

### Narrative styling

Write prompts in a narrative style, similar to scriptwriting, to guide tone and pacing effectively.

[2](/docs/best-practices/prompting#layered-outputs)

### Layered outputs

Generate sound effects or speech in segments and layer them together using audio editing software for more complex compositions.

[3](/docs/best-practices/prompting#phonetic-experimentation)

### Phonetic experimentation

If pronunciation isn’t perfect, experiment with alternate spellings or phonetic approximations to achieve desired results.

[4](/docs/best-practices/prompting#manual-adjustments)

### Manual adjustments

Combine individual sound effects manually in post-production for sequences that require precise timing.

[5](/docs/best-practices/prompting#feedback-iteration)

### Feedback iteration

Iterate on results by tweaking descriptions, tags, or emotional cues.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Product guides overview — ElevenLabs Documentation

![Product guides overview](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/overview.png)

This section covers everything from account creation to advanced voice cloning, speech synthesis techniques, dubbing, and expert voiceover.

## Product guides

[Create speech from text\
\
Discover how to create speech from text with text to speech](/docs/product-guides/playground/text-to-speech)
[Voice changer\
\
Discover how to transform your voice with voice changer](/docs/product-guides/playground/voice-changer)
[Sound effects\
\
Discover how to create cinematic sound effects from text](/docs/product-guides/playground/sound-effects)
[Projects\
\
Manage long-form content with projects](/docs/product-guides/products/projects)
[Dubbing\
\
Discover how to dub your videos in multiple languages](/docs/product-guides/products/dubbing)
[Conversational AI\
\
Discover how to create conversational AI agents](/docs/conversational-ai/overview)
[Voice cloning\
\
Discover how to create instant & professional voice clones](/docs/product-guides/voices/voice-cloning)
[Voice library\
\
Discover our voice library with over 5,000 community voices](/docs/product-guides/voices/voice-library)
[Voice design\
\
Discover how to craft voices from a single prompt](/docs/product-guides/voices/voice-design)
[Payouts\
\
Discover how to get paid when your voice is used](/docs/product-guides/voices/payouts)
[Audio native\
\
Discover how to get paid when your voice is used](/docs/product-guides/audio-tools/audio-native)
[Voiceover studio\
\
Manage long-form audio generation with voiceover studio](/docs/product-guides/audio-tools/voiceover-studio)
[Voice isolator\
\
Isolate voices from background noise](/docs/product-guides/audio-tools/voice-isolator)
[AI speech classifier\
\
Classify AI-generated speech](/docs/product-guides/audio-tools/ai-speech-classifier)

## Administration

[Account\
\
Learn how to manage your account settings](/docs/product-guides/administration/account)
[Billing\
\
Learn how to manage your billing information](/docs/product-guides/administration/billing)
[Workspaces\
\
Learn how to manage your enterprise workspaces](/docs/product-guides/administration/workspaces)
[SSO\
\
Learn how to enable single sign-on for your enterprise](/docs/product-guides/administration/sso)

---

## Troubleshooting

1.  Explore our troubleshooting section for common issues and solutions.
2.  Get help from the Conversational AI widget in the bottom right corner.
3.  Ask for help in our [Discord community](https://discord.gg/elevenlabs)
    .
4.  Contact our [support team](https://help.elevenlabs.io/hc/en-us/requests/new?ticket_form_id=13145996177937)
    .

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Streaming text to speech — ElevenLabs Documentation

In this tutorial, you’ll learn how to convert [text to speech](https://elevenlabs.io/text-to-speech)
with the ElevenLabs SDK. We’ll start by talking through how to generate speech and receive a file and then how to generate speech and stream the response back. Finally, as a bonus we’ll show you how to upload the generated audio to an AWS S3 bucket, and share it through a signed URL. This signed URL will provide temporary access to the audio file, making it perfect for sharing with users by SMS or embedding into an application.

If you want to jump straight to an example you can find them in the [Python](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/python)
and [Node.js](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/node)
example repositories.

## Requirements

- An ElevenLabs account with an API key (here’s how to [find your API key](/docs/developer-guides/quickstart#authentication)
  ).
- Python or Node installed on your machine
- (Optionally) an AWS account with access to S3.

## Setup

### Installing our SDK

Before you begin, make sure you have installed the necessary SDKs and libraries. You will need the ElevenLabs SDK for the text to speech conversion. You can install it using pip:

PythonTypeScript

`  |     |     | | --- | --- | | $   | pip install elevenlabs |    `

Additionally, install necessary packages to manage your environmental variables:

PythonTypeScript

`  |     |     | | --- | --- | | $   | pip install python-dotenv |    `

Next, create a `.env` file in your project directory and fill it with your credentials like so:

.env

`  |     |     | | --- | --- | | $   | ELEVENLABS_API_KEY=your_elevenlabs_api_key_here |    `

## Convert text to speech (file)

To convert text to speech and save it as a file, we’ll use the `convert` method of the ElevenLabs SDK and then it locally as a `.mp3` file.

text_to_speech_file.py (Python)text_to_speech_file.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import os | | 2   | import uuid | | 3   | from elevenlabs import VoiceSettings | | 4   | from elevenlabs.client import ElevenLabs | | 5   |     | | 6   | ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY") | | 7   | client = ElevenLabs( | | 8   | api_key=ELEVENLABS_API_KEY, | | 9   | )   | | 10  |     | | 11  |     | | 12  | def text_to_speech_file(text: str) -> str: | | 13  | # Calling the text_to_speech conversion API with detailed parameters | | 14  | response = client.text_to_speech.convert( | | 15  | voice_id="pNInz6obpgDQGcFmaJgB", # Adam pre-made voice | | 16  | output_format="mp3_22050_32", | | 17  | text=text, | | 18  | model_id="eleven_turbo_v2_5", # use the turbo model for low latency | | 19  | voice_settings=VoiceSettings( | | 20  | stability=0.0, | | 21  | similarity_boost=1.0, | | 22  | style=0.0, | | 23  | use_speaker_boost=True, | | 24  | ),  | | 25  | )   | | 26  |     | | 27  | # uncomment the line below to play the audio back | | 28  | # play(response) | | 29  |     | | 30  | # Generating a unique file name for the output MP3 file | | 31  | save_file_path = f"{uuid.uuid4()}.mp3" | | 32  |     | | 33  | # Writing the audio to a file | | 34  | with open(save_file_path, "wb") as f: | | 35  | for chunk in response: | | 36  | if chunk: | | 37  | f.write(chunk) | | 38  |     | | 39  | print(f"{save_file_path}: A new audio file was saved successfully!") | | 40  |     | | 41  | # Return the path of the saved audio file | | 42  | return save_file_path |    `

You can then run this function with:

PythonTypeScript

`  |     |     | | --- | --- | | 1   | text_to_speech_file("Hello World") |    `

## Convert text to speech (streaming)

If you prefer to stream the audio directly without saving it to a file, you can use our streaming feature.

text_to_speech_stream.py (Python)text_to_speech_stream.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import os | | 2   | from typing import IO | | 3   | from io import BytesIO | | 4   | from elevenlabs import VoiceSettings | | 5   | from elevenlabs.client import ElevenLabs | | 6   |     | | 7   | ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY") | | 8   | client = ElevenLabs( | | 9   | api_key=ELEVENLABS_API_KEY, | | 10  | )   | | 11  |     | | 12  |     | | 13  | def text_to_speech_stream(text: str) -> IO[bytes]: | | 14  | # Perform the text-to-speech conversion | | 15  | response = client.text_to_speech.convert( | | 16  | voice_id="pNInz6obpgDQGcFmaJgB", # Adam pre-made voice | | 17  | output_format="mp3_22050_32", | | 18  | text=text, | | 19  | model_id="eleven_multilingual_v2", | | 20  | voice_settings=VoiceSettings( | | 21  | stability=0.0, | | 22  | similarity_boost=1.0, | | 23  | style=0.0, | | 24  | use_speaker_boost=True, | | 25  | ),  | | 26  | )   | | 27  |     | | 28  | # Create a BytesIO object to hold the audio data in memory | | 29  | audio_stream = BytesIO() | | 30  |     | | 31  | # Write each chunk of audio data to the stream | | 32  | for chunk in response: | | 33  | if chunk: | | 34  | audio_stream.write(chunk) | | 35  |     | | 36  | # Reset stream position to the beginning | | 37  | audio_stream.seek(0) | | 38  |     | | 39  | # Return the stream for further use | | 40  | return audio_stream |    `

You can then run this function with:

PythonTypeScript

`  |     |     | | --- | --- | | 1   | text_to_speech_stream("This is James") |    `

## Bonus - Uploading to AWS S3 and getting a secure sharing link

Once your audio data is created as either a file or a stream you might want to share this with your users. One way to do this is to upload it to an AWS S3 bucket and generate a secure sharing link.

###### Creating your AWS credentials

To upload the data to S3 you’ll need to add your AWS access key ID, secret access key and AWS region name to your `.env` file. Follow these steps to find the credentials:

1.  Log in to your AWS Management Console: Navigate to the AWS home page and sign in with your account.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_console_login.webp)

AWS Console Login

2.  Access the IAM (Identity and Access Management) Dashboard: You can find IAM under “Security, Identity, & Compliance” on the services menu. The IAM dashboard manages access to your AWS services securely.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_iam_dashboard.webp)

AWS IAM Dashboard

3.  Create a New User (if necessary): On the IAM dashboard, select “Users” and then “Add user”. Enter a user name.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_iam_add_user.webp)

Add AWS IAM User

4.  Set the permissions: attach policies directly to the user according to the access level you wish to grant. For S3 uploads, you can use the AmazonS3FullAccess policy. However, it’s best practice to grant least privilege, or the minimal permissions necessary to perform a task. You might want to create a custom policy that specifically allows only the necessary actions on your S3 bucket.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_iam_set_permission.webp)

Set Permission for AWS IAM User

5.  Review and create the user: Review your settings and create the user. Upon creation, you’ll be presented with an access key ID and a secret access key. Be sure to download and securely save these credentials; the secret access key cannot be retrieved again after this step.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_access_secret_key.webp)

AWS Access Secret Key

6.  Get AWS region name: ex. us-east-1

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_region_name.webp)

AWS Region Name

If you do not have an AWS S3 bucket, you will need to create a new one by following these steps:

1.  Access the S3 dashboard: You can find S3 under “Storage” on the services menu.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_s3_dashboard.webp)

AWS S3 Dashboard

2.  Create a new bucket: On the S3 dashboard, click the “Create bucket” button.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_s3_create_bucket.webp)

Click Create Bucket Button

3.  Enter a bucket name and click on the “Create bucket” button. You can leave the other bucket options as default. The newly added bucket will appear in the list.

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_s3_enter_bucket_name.webp)

Enter a New S3 Bucket Name

![](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/docs/pages/cookbooks/legacy/images/aws_s3_bucket_list.webp)

S3 Bucket List

###### Installing the AWS SDK and adding the credentials

Install `boto3` for interacting with AWS services using `pip` and `npm`.

PythonTypeScript

`  |     |     | | --- | --- | | $   | pip install boto3 |    `

Then add the environment variables to `.env` file like so:

`  |     | | --- | | AWS_ACCESS_KEY_ID=your_aws_access_key_id_here | | AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here | | AWS_REGION_NAME=your_aws_region_name_here | | AWS_S3_BUCKET_NAME=your_s3_bucket_name_here |    `

###### Uploading to AWS S3 and generating the signed URL

Add the following functions to upload the audio stream to S3 and generate a signed URL.

s3_uploader.py (Python)s3_uploader.ts (TypeScript)

`  |     |     | | --- | --- | | 1   | import os | | 2   | import boto3 | | 3   | import uuid | | 4   |     | | 5   | AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID") | | 6   | AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY") | | 7   | AWS_REGION_NAME = os.getenv("AWS_REGION_NAME") | | 8   | AWS_S3_BUCKET_NAME = os.getenv("AWS_S3_BUCKET_NAME") | | 9   |     | | 10  | session = boto3.Session( | | 11  | aws_access_key_id=AWS_ACCESS_KEY_ID, | | 12  | aws_secret_access_key=AWS_SECRET_ACCESS_KEY, | | 13  | region_name=AWS_REGION_NAME, | | 14  | )   | | 15  | s3 = session.client("s3") | | 16  |     | | 17  |     | | 18  | def generate_presigned_url(s3_file_name: str) -> str: | | 19  | signed_url = s3.generate_presigned_url( | | 20  | "get_object", | | 21  | Params={"Bucket": AWS_S3_BUCKET_NAME, "Key": s3_file_name}, | | 22  | ExpiresIn=3600, | | 23  | )  # URL expires in 1 hour | | 24  | return signed_url | | 25  |     | | 26  |     | | 27  | def upload_audiostream_to_s3(audio_stream) -> str: | | 28  | s3_file_name = f"{uuid.uuid4()}.mp3"  # Generates a unique file name using UUID | | 29  | s3.upload_fileobj(audio_stream, AWS_S3_BUCKET_NAME, s3_file_name) | | 30  |     | | 31  | return s3_file_name |    `

You can then call uploading function with the audio stream from the text.

PythonTypeScript

`  |     |     | | --- | --- | | 1   | s3_file_name = upload_audiostream_to_s3(audio_stream) |    `

After uploading the audio file to S3, generate a signed URL to share access to the file. This URL will be time-limited, meaning it will expire after a certain period, making it secure for temporary sharing.

You can now generate a URL from a file with:

PythonTypeScript

`  |     |     | | --- | --- | | 1   | signed_url = generate_presigned_url(s3_file_name) | | 2   | print(f"Signed URL to access the file: {signed_url}") |    `

If you want to use the file multiple times, you should store the s3 file path in your database and then regenerate the signed URL each time you need rather than saving the signed URL directly as it will expire.

###### Putting it all together

To put it all together, you can use the following script:

main.py (Python)index.ts (Typescript)

`  |     |     | | --- | --- | | 1   | import os | | 2   |     | | 3   | from dotenv import load_dotenv | | 4   |     | | 5   | load_dotenv() | | 6   |     | | 7   | from text_to_speech_stream import text_to_speech_stream | | 8   | from s3_uploader import upload_audiostream_to_s3, generate_presigned_url | | 9   |     | | 10  |     | | 11  | def main(): | | 12  | text = "This is James" | | 13  |     | | 14  | audio_stream = text_to_speech_stream(text) | | 15  | s3_file_name = upload_audiostream_to_s3(audio_stream) | | 16  | signed_url = generate_presigned_url(s3_file_name) | | 17  |     | | 18  | print(f"Signed URL to access the file: {signed_url}") | | 19  |     | | 20  |     | | 21  | if __name__ == "__main__": | | 22  | main() |    `

## Conclusion

You now know how to convert text into speech and generate a signed URL to share the audio file. This functionality opens up numerous opportunities for creating and sharing content dynamically.

Here are some examples of what you could build with this.

1.  **Educational Podcasts**: Create personalized educational content that can be accessed by students on demand. Teachers can convert their lessons into audio format, upload them to S3, and share the links with students for a more engaging learning experience outside the traditional classroom setting.
2.  **Accessibility Features for Websites**: Enhance website accessibility by offering text content in audio format. This can make information on websites more accessible to individuals with visual impairments or those who prefer auditory learning.
3.  **Automated Customer Support Messages**: Produce automated and personalized audio messages for customer support, such as FAQs or order updates. This can provide a more engaging customer experience compared to traditional text emails.
4.  **Audio Books and Narration**: Convert entire books or short stories into audio format, offering a new way for audiences to enjoy literature. Authors and publishers can diversify their content offerings and reach audiences who prefer listening over reading.
5.  **Language Learning Tools**: Develop language learning aids that provide learners with audio lessons and exercises. This makes it possible to practice pronunciation and listening skills in a targeted way.

For more details, visit the following to see the full project files which give a clear structure for setting up your application:

For Python: [example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/python)

For TypeScript: [example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/node)

If you have any questions please create an issue on the [elevenlabs-doc Github](https://github.com/elevenlabs/elevenlabs-docs/issues)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Dubbing audio — ElevenLabs Documentation

## Introduction

Dubbing videos and audio files from one language to another can be a great way to reach a wider audience. The ElevenLabs API provides a convenient way to automatically dub media files using state-of-the-art technology. In this guide, we will walk you through how to upload a video or audio file, dub it, and download the translated video. We’ll also discuss how to directly dub a link such as a YouTube, TikTok, or Twitter video.

If you’re looking to jump straight into the action, the complete code is available on the following repos:

- [Python example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/dubbing/python)
  .

On the 8th of May 2024 we launched the Dubbing API for all ElevenLabs tiers

## How to upload and dub a video or audio file

### Requirements

Before proceeding, please ensure that you have the following:

- An ElevenLabs account with an API key (here’s how to [find your API key](/docs/api-reference/authentication)
  ).
- Python or Node.js installed on your machine

Then, install the ElevenLabs SDK as shown below

Python

`  |     |     | | --- | --- | | $   | pip install elevenlabs |    `

Install the necessary packages to manage your environmental variables:

Python

`  |     |     | | --- | --- | | $   | pip install python-dotenv |    `

Next, create a `.env` file in your project directory and fill it with your credentials like so:

.env

`  |     |     | | --- | --- | | $   | ELEVENLABS_API_KEY=your_elevenlabs_api_key_here |    `

### Start the dubbing

First we want to send the file to the ElevenLabs dubbing service

Python

`  |     |     | | --- | --- | | 1   | def create_dub_from_file( | | 2   | input_file_path: str, | | 3   | file_format: str, | | 4   | source_language: str, | | 5   | target_language: str, | | 6   | ) -> Optional[str]: | | 7   | """ | | 8   | Dubs an audio or video file from one language to another and saves the output. | | 9   |     | | 10  | Args: | | 11  | input_file_path (str): The file path of the audio or video to dub. | | 12  | file_format (str): The file format of the input file. | | 13  | source_language (str): The language of the input file. | | 14  | target_language (str): The target language to dub into. | | 15  |     | | 16  | Returns: | | 17  | Optional[str]: The file path of the dubbed file or None if operation failed. | | 18  | """ | | 19  | if not os.path.isfile(input_file_path): | | 20  | raise FileNotFoundError(f"The input file does not exist: {input_file_path}") | | 21  |     | | 22  | with open(input_file_path, "rb") as audio_file: | | 23  | response = client.dubbing.dub_a_video_or_an_audio_file( | | 24  | file=(os.path.basename(input_file_path), audio_file, file_format), # Optional file | | 25  | target_lang=target_language, # The target language to dub the content into. Can be none if dubbing studio editor is enabled and running manual mode | | 26  | mode="automatic", # automatic or manual. | | 27  | source_lang=source_language, # Source language | | 28  | num_speakers=1, # Number of speakers to use for the dubbing. | | 29  | watermark=False,  # Whether to apply watermark to the output video. | | 30  | )   | | 31  |     | | 32  | # rest of the code |    `

### Check for completion

The `wait_for_dubbing_completion()` function within the `dubbing_utils.py` file polls the API to check whether the dubbing process is complete. If completed, it proceeds to the next step; otherwise, it reports the status or failure.

Python

`  |     |     | | --- | --- | | 1   | def wait_for_dubbing_completion(dubbing_id: str) -> bool: | | 2   | """ | | 3   | Waits for the dubbing process to complete by periodically checking the status. | | 4   |     | | 5   | Args: | | 6   | dubbing_id (str): The dubbing project id. | | 7   |     | | 8   | Returns: | | 9   | bool: True if the dubbing is successful, False otherwise. | | 10  | """ | | 11  | MAX_ATTEMPTS = 120 | | 12  | CHECK_INTERVAL = 10  # In seconds | | 13  |     | | 14  | for _ in range(MAX_ATTEMPTS): | | 15  | metadata = client.dubbing.get_dubbing_project_metadata(dubbing_id) | | 16  | if metadata.status == "dubbed": | | 17  | return True | | 18  | elif metadata.status == "dubbing": | | 19  | print( | | 20  | "Dubbing in progress... Will check status again in", | | 21  | CHECK_INTERVAL, | | 22  | "seconds.", | | 23  | )   | | 24  | time.sleep(CHECK_INTERVAL) | | 25  | else: | | 26  | print("Dubbing failed:", metadata.error_message) | | 27  | return False | | 28  |     | | 29  | print("Dubbing timed out") | | 30  | return False |    `

### Save the video locally

Upon completion of dubbing, the `download_dubbed_file()` function in `dubbing_utils.py` will save the dubbed file to a local directory, typically under the `data/{dubbing_id}/{language_code}.mp4`.

Python

`  |     |     | | --- | --- | | 1   | def download_dubbed_file(dubbing_id: str, language_code: str) -> str: | | 2   | """ | | 3   | Downloads the dubbed file for a given dubbing ID and language code. | | 4   |     | | 5   | Args: | | 6   | dubbing_id: The ID of the dubbing project. | | 7   | language_code: The language code for the dubbing. | | 8   |     | | 9   | Returns: | | 10  | The file path to the downloaded dubbed file. | | 11  | """ | | 12  | dir_path = f"data/{dubbing_id}" | | 13  | os.makedirs(dir_path, exist_ok=True) | | 14  |     | | 15  | file_path = f"{dir_path}/{language_code}.mp4" | | 16  | with open(file_path, "wb") as file: | | 17  | for chunk in client.dubbing.get_dubbed_file(dubbing_id, language_code): | | 18  | file.write(chunk) | | 19  |     | | 20  | return file_path |    `

### Putting it together

We add the `wait_for_dubbing_completion`(`waitForDubbingCompletion`) function and the `download_dubbed_file`(`downloadDubbedFile`) function together to create the final function.

Python

`  |     |     | | --- | --- | | 1   | def create_dub_from_file( | | 2   | input_file_path: str, | | 3   | file_format: str, | | 4   | source_language: str, | | 5   | target_language: str, | | 6   | ) -> Optional[str]: | | 7   | """ | | 8   | Dubs an audio or video file from one language to another and saves the output. | | 9   |     | | 10  | Args: | | 11  | input_file_path (str): The file path of the audio or video to dub. | | 12  | file_format (str): The file format of the input file. | | 13  | source_language (str): The language of the input file. | | 14  | target_language (str): The target language to dub into. | | 15  |     | | 16  | Returns: | | 17  | Optional[str]: The file path of the dubbed file or None if operation failed. | | 18  | """ | | 19  | if not os.path.isfile(input_file_path): | | 20  | raise FileNotFoundError(f"The input file does not exist: {input_file_path}") | | 21  |     | | 22  | with open(input_file_path, "rb") as audio_file: | | 23  | response = client.dubbing.dub_a_video_or_an_audio_file( | | 24  | file=(os.path.basename(input_file_path), audio_file, file_format), | | 25  | target_lang=target_language, | | 26  | mode="automatic", | | 27  | source_lang=source_language, | | 28  | num_speakers=1, | | 29  | watermark=False,  # reduces the characters used if enabled, only works for videos not audio | | 30  | )   | | 31  |     | | 32  | dubbing_id = response.dubbing_id | | 33  | if wait_for_dubbing_completion(dubbing_id): | | 34  | output_file_path = download_dubbed_file(dubbing_id, target_language) | | 35  | return output_file_path | | 36  | else: | | 37  | return None |    `

We then use the final the function as shown below.

create_a_dub_from_file.py (Python)

`  |     |     | | --- | --- | | 1   | if __name__ == "__main__": | | 2   | result = create_dub_from_file( | | 3   | "../example_speech.mp3",  # Input file path | | 4   | "audio/mpeg",  # File format | | 5   | "en",  # Source language | | 6   | "es",  # Target language | | 7   | )   | | 8   | if result: | | 9   | print("Dubbing was successful! File saved at:", result) | | 10  | else: | | 11  | print("Dubbing failed or timed out.") |    `

## How to dub a video from YouTube, TikTok, Twitter or Vimeo

For dubbing web-based content, instead of uploading a file you can pass in a URL. This supports popular platforms like YouTube, TikTok, Twitter, and Vimeo.

Python

`  |     |     | | --- | --- | | 1   | def create_dub_from_url( | | 2   | source_url: str, | | 3   | source_language: str, | | 4   | target_language: str, | | 5   | ) -> Optional[str]: | | 6   | """ | | 7   | Downloads a video from a URL, and creates a dubbed version in the target language. | | 8   |     | | 9   | Args: | | 10  | source_url (str): The URL of the source video to dub. Can be a YouTube link, TikTok, X (Twitter) or a Vimeo link. | | 11  | source_language (str): The language of the source video. | | 12  | target_language (str): The target language to dub into. | | 13  |     | | 14  | Returns: | | 15  | Optional[str]: The file path of the dubbed file or None if operation failed. | | 16  | """ | | 17  |     | | 18  | response = client.dubbing.dub_a_video_or_an_audio_file( | | 19  | source_url=source_url, # URL of the source video/audio file. | | 20  | target_lang=target_language, # The Target language to dub the content into. Can be none if dubbing studio editor is enabled and running manual mode | | 21  | mode="automatic", # automatic or manual. | | 22  | source_lang=source_language, # Source language. | | 23  | num_speakers=1, # Number of speakers to use for the dubbing. | | 24  | watermark=True,  # Whether to apply watermark to the output video. | | 25  | )   | | 26  |     | | 27  | dubbing_id = response.dubbing_id | | 28  | if wait_for_dubbing_completion(dubbing_id): | | 29  | output_file_path = download_dubbed_file(dubbing_id, target_language) | | 30  | return output_file_path | | 31  | else: | | 32  | return None |    `

You can then call the function as shown below.

Python

`  |     |     | | --- | --- | | 1   | if __name__ == "__main__": | | 2   | source_url = "https://www.youtube.com/watch?v=0EqSXDwTq6U"  # Charlie bit my finger | | 3   | source_language = "en" | | 4   | target_language = "fr" | | 5   | result = create_dub_from_url(source_url, source_language, target_language) | | 6   | if result: | | 7   | print("Dubbing was successful! File saved at:", result) | | 8   | else: | | 9   | print("Dubbing failed or timed out.") |    `

## Conclusion

With this guide and the accompanying code structure, you now have a basic setup for dubbing audio and video content using the ElevenLabs API. Whether you’re working with local files or content from URLs, you can create multilingual versions of your media to cater to diverse audiences.

Remember to always follow the best practices when dealing with API keys and sensitive data, and consult the ElevenLabs API documentation for more advanced features and options. Happy dubbing!

For additional information on dubbing capabilities, translation services, and available languages, please refer to the [ElevenLabs API documentation](/docs/api-reference/dubbing/dub-a-video-or-an-audio-file)
.

Should you encounter any issues or have questions, our [GitHub Issues page](https://github.com/elevenlabs/elevenlabs-docs/issues)
is open for your queries and feedback.

## List of supported languages for dubbing

| No  | Language Name | Language Code |
| --- | ------------- | ------------- |
| 1   | English       | en            |
| 2   | Hindi         | hi            |
| 3   | Portuguese    | pt            |
| 4   | Chinese       | zh            |
| 5   | Spanish       | es            |
| 6   | French        | fr            |
| 7   | German        | de            |
| 8   | Japanese      | ja            |
| 9   | Arabic        | ar            |
| 10  | Russian       | ru            |
| 11  | Korean        | ko            |
| 12  | Indonesian    | id            |
| 13  | Italian       | it            |
| 14  | Dutch         | nl            |
| 15  | Turkish       | tr            |
| 16  | Polish        | pl            |
| 17  | Swedish       | sv            |
| 18  | Filipino      | fil           |
| 19  | Malay         | ms            |
| 20  | Romanian      | ro            |
| 21  | Ukrainian     | uk            |
| 22  | Greek         | el            |
| 23  | Czech         | cs            |
| 24  | Danish        | da            |
| 25  | Finnish       | fi            |
| 26  | Bulgarian     | bg            |
| 27  | Croatian      | hr            |
| 28  | Slovak        | sk            |
| 29  | Tamil         | ta            |

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Text to Speech (product guide) — ElevenLabs Documentation

![Text to Speech product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/text-to-speech/text-to-speech-product-feature.png)

## Overview

ElevenLabs’ Text to Speech technology is integral to our offerings, powering high-quality AI-generated speech across various applications worldwide. It’s likely you’ve already encountered our voices in action, delivering lifelike audio experiences.

## Guide

![Text to Speech demo](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/text-to-speech/text-to-speech-demo.png)

[1](/docs/product-guides/playground/text-to-speech#text-input)

### Text input

Type or paste your text into the input box on the Text to Speech page.

[2](/docs/product-guides/playground/text-to-speech#voice-selection)

### Voice selection

Select the voice you wish to use from your Voices at the bottom left of the screen.

[3](/docs/product-guides/playground/text-to-speech#adjust-settings-optional)

### Adjust settings (optional)

Modify the voice settings for the desired output.

[4](/docs/product-guides/playground/text-to-speech#generate)

### Generate

Click the ‘Generate’ button to create your audio file.

## Settings

Get familiar with the voices, models & settings for creating high-quality speech.

###### Voices

### Voices

![Text to Speech voice
selection](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/text-to-speech/text-to-speech-voices.png)

We offer many types of voices, including the curated Default Voices library; completely synthetic voices created using our Voice Design tool; you can create your own collection of cloned voices using our two technologies: Instant Voice Cloning and Professional Voice Cloning. Browse through our voice library to find the perfect voice for your production.

Not all voices are equal, and a lot depends on the source audio used to create that voice. Some voices will perform better than others, while some will be more stable than others. Additionally, certain voices will be more easily cloned by the AI than others, and some voices may work better with one model and one language compared to another. All of these factors are important to consider when selecting your voice.

[Learn more about voices](/docs/capabilities/voices)

###### Models

### Models

![Text to Speech model
selection](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/text-to-speech/text-to-speech-models.png)

ElevenLabs offers two families of models: standard (high-quality) models and Flash models, which are optimized for low latency. Each family includes both English-only and multilingual models, tailored for specific use cases with strengths in either speed, accuracy, or language diversity.

[Eleven Multilingual v2\
\
Our most lifelike, emotionally rich speech synthesis model\
\
Most natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Rich emotional expression](/docs/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](/docs/models#flash-v25)

[Learn more about our models](/docs/models)

###### Voice settings

### Voice settings

![Text to Speech voice
settings](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/text-to-speech/text-to-speech-settings.png)

Our users have found different workflows that work for them. The most common setting is stability around 50 and similarity near 75, with minimal changes thereafter. Of course, this all depends on the original voice and the style of performance you’re aiming for.

It’s important to note that the AI is non-deterministic; setting the sliders to specific values won’t guarantee the same results every time. Instead, the sliders function more as a range, determining how wide the randomization can be between each generation.

#### Stability

The stability slider determines how stable the voice is and the randomness between each generation. Lowering this slider introduces a broader emotional range for the voice. As mentioned before, this is also influenced heavily by the original voice. Setting the slider too low may result in odd performances that are overly random and cause the character to speak too quickly. On the other hand, setting it too high can lead to a monotonous voice with limited emotion.

For a more lively and dramatic performance, it is recommended to set the stability slider lower and generate a few times until you find a performance you like.

On the other hand, if you want a more serious performance, even bordering on monotone at very high values, it is recommended to set the stability slider higher. Since it is more consistent and stable, you usually don’t need to generate as many samples to achieve the desired result. Experiment to find what works best for you!

#### Similarity

The similarity slider dictates how closely the AI should adhere to the original voice when attempting to replicate it. If the original audio is of poor quality and the similarity slider is set too high, the AI may reproduce artifacts or background noise when trying to mimic the voice if those were present in the original recording.

#### Style exaggeration

With the introduction of the newer models, we also added a style exaggeration setting. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0. It’s important to note that using this setting has shown to make the model slightly less stable, as it strives to emphasize and imitate the style of the original voice.

In general, we recommend keeping this setting at 0 at all times.

#### Speaker Boost

This setting boosts the similarity to the original speaker. However, using this setting requires a slightly higher computational load, which in turn increases latency. The differences introduced by this setting are generally rather subtle.

## FAQ

###### Good input equals good output

The first factor, and one of the most important, is that good, high-quality, and consistent input will result in good, high-quality, and consistent output.

If you provide the AI with audio that is less than ideal—for example, audio with a lot of noise, reverb on clear speech, multiple speakers, or inconsistency in volume or performance and delivery—the AI will become more unstable, and the output will be more unpredictable.

If you plan on cloning your own voice, we strongly recommend that you go through our guidelines in the documentation for creating proper voice clones, as this will provide you with the best possible foundation to start from. Even if you intend to use only Instant Voice Clones, it is advisable to read the Professional Voice Cloning section as well. This section contains valuable information about creating voice clones, even though the requirements for these two technologies are slightly different.

###### Use the right voice

The second factor to consider is that the voice you select will have a tremendous effect on the output. Not only, as mentioned in the first factor, is the quality and consistency of the samples used to create that specific clone extremely important, but also the language and tonality of the voice.

If you want a voice that sounds happy and cheerful, you should use a voice that has been cloned using happy and cheerful samples. Conversely, if you desire a voice that sounds introspective and brooding, you should select a voice with those characteristics.

However, it is also crucial to use a voice that has been trained in the correct language. For example, all of the professional voice clones we offer as default voices are English voices and have been trained on English samples. Therefore, if you have them speak other languages, their performance in those languages can be unpredictable. It is essential to use a voice that has been cloned from samples where the voice was speaking the language you want the AI to then speak.

###### Use proper formatting

This may seem slightly trivial, but it can make a big difference. The AI tries to understand how to read something based on the context of the text itself, which means not only the words used but also how they are put together, how punctuation is applied, the grammar, and the general formatting of the text.

This can have a small but impactful influence on the AI’s delivery. If you were to misspell a word, the AI won’t correct it and will try to read it as written.

###### Nondeterministic

The settings of the AI are nondeterministic, meaning that even with the same initial conditions (voice, settings, model), it will give you slightly different output, similar to how a voice actor will deliver a slightly different performance each time.

This variability can be due to various factors, such as the options mentioned earlier: voice, settings, model. Generally, the breadth of that variability can be controlled by the stability slider. A lower stability setting means a wider range of variability between generations, but it also introduces inter-generational variability, where the AI can be a bit more performative.

A wider variability can often be desirable, as setting the stability too high can make certain voices sound monotone as it does give the AI the same leeway to generate more variable content. However, setting the stability too low can also introduce other issues where the generations become unstable, especially with certain voices that might have used less-than-ideal audio for the cloning process.

The default setting of 50 is generally a great starting point for most applications.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Sound effects (product guide) — ElevenLabs Documentation

![Sound effects product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/sound-effects/sound-effects-product-feature.png)

## Overview

**Sound effects** enhance the realism and immersion of your audio projects. ElevenLabs offers a variety of sound effects that can be easily integrated into your voiceovers and projects.

## Guide

![Sound effects demo](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/sound-effects/sound-effects-demo.png)

[1](/docs/product-guides/playground/sound-effects#navigate-to-sound-effects)

### Navigate to Sound Effects

Head over to the Sound Effects tab in the dashboard.

[2](/docs/product-guides/playground/sound-effects#describe-the-sound-effect)

### Describe the sound effect

In the text box, type a description of the sound effect you want (e.g., “person walking on grass”).

[3](/docs/product-guides/playground/sound-effects#adjust-settings)

### Adjust settings

![Sound effects settings](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/sound-effects/sound-effects-settings.png)

1.  Set the duration for the generated sound (or let it automatically pick the best length).
2.  Use the prompt influence slider to control how closely the output should matchthe prompt.

[4](/docs/product-guides/playground/sound-effects#generate-sound)

### Generate Sound

Click the “Generate” button to start generating.

[5](/docs/product-guides/playground/sound-effects#review-and-regenerate)

### Review and regenerate

You should have four different sounds generated. If you like none of them, adjust the prompt or settings as needed and regenerate.

**Exercise**: Create a Sound Effect using the following prompt: old-school funky brass stabs from an old vinyl sample, stem, 88bpm in F# minor.

## Explore the library

![Sound effects explore](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/sound-effects/sound-effects-explore.png)

Check out some of our community-made sound effects in the **Explore** tab.

For more information on prompting & how sound effects work visit our [overview page](/docs/capabilities/sound-effects)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Dubbing Overview — ElevenLabs Documentation

![Dubbing studio product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/dubbing/dubbing-product-feature.png)

**Dubbing** allows you to translate content across 29 languages in seconds with voice translation, speaker detection, and audio dubbing.

Automated Dubbing or Video Translation is a process for translating and replacing the original audio of a video with a new language, while preserving the unique characteristics of the original speakers’ voices.

We offer two different types of Dubbing: Automatic and Studio.

**Automatic Dubbing** is the default, so let’s see the step by step for this type of dubbing.

![Dubbing new project](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/dubbing/dubbing-new-project.png)

### Step by step for Automatic Dubbing

1.  Go to the Dubbing Studio in your Navigation Menu.
2.  Enter the dub name and select the original and target languages.
3.  Upload a video/audio file or import one via URL (YouTube, TikTok, etc.).
4.  Opt for watermarking if needed.
5.  Leave the Create a Dubbing Studio project box unchecked.
6.  Click on the **Advanced Settings** option:
    - Choose the number of speakers and video resolution.
    - Select the specific range for dubbing if needed.
7.  Click on **Create** and sit back while ElevenLabs does its magic.

**Exercise**: Dub the video found [here](https://www.youtube.com/watch?v=WnNFZt0qjD0)
from English to Spanish (or a language of your choosing). Select 6 speakers and keep the watermark.

[API reference\
\
See the API reference for dubbing.](/docs/api-reference/text-to-voice)
[Example app\
\
A Python flask example app for dubbing.](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/dubbing)

### Dubbing Studio Project

- This is the advanced Dubbing version, which you can access by checking the **Create a Dubbing Studio project** box. Read more about it in the [Dubbing Studio guide](/docs/product-guides/products/dubbing/dubbing-studio)
  .

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voice changer (product guide) — ElevenLabs Documentation

![Voice changer product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voice-changer/voice-changer-product-feature.png)

## Overview

Voice changer (previously Speech-to-Speech) allows you to convert one voice (source voice) into another (cloned voice) while preserving the tone and delivery of the original voice.

Voice changer can be used to complement Text-to-Speech (TTS) by fixing pronunciation errors or infusing that special performance you’ve been wanting to exude. Voice changer is especially useful for emulating those subtle, idiosyncratic characteristics of the voice that give a more emotive and human feel. Some key features include:

- Greater accuracy with whispering
- The ability to create audible sighs, laughs, or cries
- Greatly improved detection of tone and emotion
- Accurately follows the input speaking cadence
- Language/accent retention

###### Watch a video of voice changer in action

## Guide

![Voice changer demo](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voice-changer/voice-changer-demo.png)

Audio can be uploaded either directly with an audio file, or spoken live through a microphone. The audio file must be less than **50mb in size**, and either the audio file or your live recording cannot exceed **5 minutes in length**.

If you have material longer than 5 minutes, we recommend breaking it up into smaller sections and generating them separately. Additionally, if your file size is too large, you may need to compress/convert it to an mp3.

### Existing audio file

To upload an existing audio file, either click the audio box, or drag and drop your audio file directly onto it.

### Record live

Press the **Record Audio** button in the audio box, and then once you are ready to begin recording, press the **Microphone** button to start. After you’re finished recording, press the **Stop** button.

You will then see the audio file of this recording, which you can then playback to listen to - this is helpful to determine if you are happy with your performance/recording. The character cost will be displayed on the bottom-left corner, and you will not be charged this quota for recording anything - only when you press “Generate”.

**The cost for a voice changer generation is solely duration-based at 1000 characters per minute.**

## Settings

![Voice changer settings](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voice-changer/voice-changer-settings.png)

Learn more about the different voice settings [here](/docs/product-guides/playground/text-to-speech#settings)
.

Voice changer adds an additional setting to automaticaly remove background noise from your recording.

## Support languages

`eleven_english_sts_v2`

Our v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

The `eleven_english_sts_v2` model only supports English.

[Learn more about models](/docs/models)

## Best practices

Voice changer excels at **preserving accents** and **natural speech cadences** across various output voices. For instance, if you upload an audio sample with a Portuguese accent, the output will retain that language and accent. The input sample is crucial, as it determines the output characteristics. If you select a British voice like “George” but record with an American accent, the result will be George’s voice with an American accent.

- **Expression**: Be expressive in your recordings. Whether shouting, crying, or laughing, the voice changer will accurately replicate your performance. This tool is designed to enhance AI realism, allowing for creative expression.
- **Microphone gain**: Ensure the input gain is appropriate. A quiet recording may hinder AI recognition, while a loud one could cause audio clipping.
- **Background Noise**: Turn on the **Remove Background Noise** option to automatically remove background noise from your recording.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Studio overview — ElevenLabs Documentation

![Studio product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/studio/studio-product-feature.png)

## Overview

Studio is an end-to-end workflow for creating long-form content. With this tool you can upload an entire book, document or webpage and generate a voiceover narration for it. The result can then be downloaded as a single MP3 file or as individual MP3 files for each chapter.

## Guide

![Studio create](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/studio/studio-home.png)

[1](/docs/product-guides/products/studio#create-a-new-project)

### Create a new project

Select one of the start options at the top of the studio page.

[2](/docs/product-guides/products/studio#select-settings)

### Select settings

Follow the instructions in the popup and click “Create”.

[3](/docs/product-guides/products/studio#edit-text)

### Edit text

Make changes in the text editor and adjust voice settings as needed.

[4](/docs/product-guides/products/studio#download-audio-files)

### Download audio files

Click the “Convert” button to compile and download the entire project or specific chapters as a single audio file. Use the “Download” button for various download options, including project-wide or chapter-specific files.

You can use our [Audio Native](/docs/product/audio-native/overview)
feature to easily and effortlessly embed any narration project onto your website.

## Settings

###### Voices

### Voices

We offer many types of voices, including the curated Default Voices library; completely synthetic voices created using our Voice Design tool; you can create your own collection of cloned voices using our two technologies: Instant Voice Cloning and Professional Voice Cloning. Browse through our voice library to find the perfect voice for your production.

Not all voices are equal, and a lot depends on the source audio used to create that voice. Some voices will perform better than others, while some will be more stable than others. Additionally, certain voices will be more easily cloned by the AI than others, and some voices may work better with one model and one language compared to another. All of these factors are important to consider when selecting your voice.

[Learn more about voices](/docs/capabilities/voices)

###### Voice settings

### Voice settings

![Studio voice settings](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/studio/studio-voice-settings.png)

Our users have found different workflows that work for them. The most common setting is stability around 50 and similarity near 75, with minimal changes thereafter. Of course, this all depends on the original voice and the style of performance you’re aiming for.

It’s important to note that the AI is non-deterministic; setting the sliders to specific values won’t guarantee the same results every time. Instead, the sliders function more as a range, determining how wide the randomization can be between each generation.

#### Stability

The stability slider determines how stable the voice is and the randomness between each generation. Lowering this slider introduces a broader emotional range for the voice. As mentioned before, this is also influenced heavily by the original voice. Setting the slider too low may result in odd performances that are overly random and cause the character to speak too quickly. On the other hand, setting it too high can lead to a monotonous voice with limited emotion.

For a more lively and dramatic performance, it is recommended to set the stability slider lower and generate a few times until you find a performance you like.

On the other hand, if you want a more serious performance, even bordering on monotone at very high values, it is recommended to set the stability slider higher. Since it is more consistent and stable, you usually don’t need to generate as many samples to achieve the desired result. Experiment to find what works best for you!

#### Similarity

The similarity slider dictates how closely the AI should adhere to the original voice when attempting to replicate it. If the original audio is of poor quality and the similarity slider is set too high, the AI may reproduce artifacts or background noise when trying to mimic the voice if those were present in the original recording.

#### Style exaggeration

With the introduction of the newer models, we also added a style exaggeration setting. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0. It’s important to note that using this setting has shown to make the model slightly less stable, as it strives to emphasize and imitate the style of the original voice.

In general, we recommend keeping this setting at 0 at all times.

#### Speaker boost

This setting boosts the similarity to the original speaker. However, using this setting requires a slightly higher computational load, which in turn increases latency. The differences introduced by this setting are generally rather subtle.

###### Pronunciation dictionaries

### Pronunciation dictionaries

Sometimes you may want to specify the pronunciation of certain words, such as character or brand names, or specify how acronyms should be read. Pronunciation dictionaries allow this functionality by enabling you to upload a lexicon or dictionary file that includes rules about how specified words should be pronounced, either using a phonetic alphabet (phoneme tags) or word substitutions (alias tags).

Whenever one of these words is encountered in a project, the AI will pronounce the word using the specified replacement. When checking for a replacement word in a pronunciation dictionary, the dictionary is checked from start to end and only the first replacement is used.

You can add a pronunciation dictionary to your project from the General tab in Project settings.

For more information on pronunciation dictionaries, please see our [prompting best practices guide](/docs/best-practices/prompting#pronunciation-dictionaries)
.

###### Exporting

### Exporting

Within the “Export” tab under General settings you can add additional metadata such as Title, Author, ISBN and a Description to your project. This information will automatically be added to the downloaded audio files.

## FAQ

###### Free regenerations

![Studio free regenerations](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/regenerations/regenerations-projects.png)

In Studio, provided you don’t change the text, you can regenerate a selected paragraph or section of text twice for free.

If free regenerations are available for the selected paragraph or text, you will see “Regenerate”. If you hover over the “Regenerate” button, the number of free regenerations remaining will be displayed.

Once your free regenerations have been used, the button will display “Generate”, and you will be charged for subsequent generations.

###### Auto-regeneration for bulk conversions

When converting a full chapter or project, auto-regeneration automatically checks the output for volume issues, voice similarity, and mispronunciations. If ElevenLabs detects any issues, the tool will automatically regenerate the audio up to twice, at no extra cost.

This feature may increase the processing time but helps ensure higher quality output for your bulk conversions.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with Webflow — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/webflow#add-html-to-your-blog-post)

### Add HTML to your blog post

Navigate to your Webflow blog, sign in and open the editor for the blog post you wish to narrate.

[2](/docs/product-guides/audio-tools/audio-native/webflow#add-the-embed-code-to-your-blog-post)

### Add the embed code to your blog post

Click the ”+” symbol in the top left and select “Code Embed” from the Elements menu.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-webflow-1.webp)

Paste the Audio Native embed code into the HTML box and click “Save & Close”.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-webflow-2.webp)

[3](/docs/product-guides/audio-tools/audio-native/webflow#re-position-the-code-embed)

### Re-position the code embed

In the Navigator, place the code embed where you want it to appear on the page.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-webflow-3.webp)

[4](/docs/product-guides/audio-tools/audio-native/webflow#publish-your-changes)

### Publish your changes

Finally, publish your changes and navigate to the live version of the blog post. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-webflow-4.webp)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Dubbing studio — ElevenLabs Documentation

### Step by step for for using Dubbing Studio:

1.  Follow the same steps as you had with Automatic Dubbing, this time checking the **Create a Dubbing Studio project** box.
2.  Click on **Create**.
3.  The system will auto-generate a transcription of the original audio (or for more advanced users, you can manually input the transcription using the Manual upload option during the upload stage).
4.  Review the transcription in the speaker cards and edit if necessary.
5.  If needed, re-assign clips to the appropriate speaker(s) by dragging and dropping the audio clips from the track to the speaker timeline.
6.  Click on the language where you dubbed the video in at the bottom of the screen.
7.  You will see a new set of speaker cards appearing next to your transcription, as well as a new set of audio files that are highlighted in sync with the audio timeline.
    - (Optional) If you have made any edits to the transcription, you can re-translate the text by clicking the arrow between the two speaker cards.
    - (Optional) You can assign different voices and/or edit the voice settings for an audio track by clicking the cog icon available next to the speaker’s name within the audio track.
8.  Use the timeline to view and adjust the placement of voice clips.
9.  **You can edit clips**:
    - By dragging the clip edges to speed up or slow down speech within in select mode.
    - Merge clips by selecting the merge button between clips in the original audio tracks.
    - Split clips by selecting the split button in the clip panel with the split icon to save time.
10. You can export the final output in various formats and synchronize any video or audio you wish by selecting “Export”.
11. Preview the dubbed video, export when ready.
12. This allows video clips to be generated. Choose the video format and size from available options.
13. Select the dubbed file to be generated.

## Additional Features

- **Manual Import**: Allows for manual uploading of video, background audio, and speaker audio files, along with CSV files specifying details for each clip.
- **Timing Adjustments**: Choose between fixed durations to maintain video timing or dynamic durations for more natural speech flow.
- **Voiceover Tracks:** Voiceover tracks create new Speakers. You can click and add clips on the timeline wherever you like. After creating a clip, start writing your desired text on the speaker cards above. You’ll first need to translate that text, then you can press “Generate”. You can also use our voice changer tool by clicking on the microphone icon on the right side of the screen to use your own voice and then change it into the selected voice.
- **SFX Tracks:** Add a SFX track, then click anywhere on that track to create a SFX clip. Similar to our independent SFX feature, simply start writing your prompt in the Speaker card above and click “Generate” to create your new SFX audio. You can lengthen or shorten SFX clips and move them freely around your timeline to fit your project - make sure to press the “stale” button if you do so.
- **Upload Audio:** This option allows you to upload a non voiced track such as sfx, music or background track. Please keep in mind that if voices are present in this track, they won’t be detected so it will not be possible to translate or correct them.

**Exercise**: Dub the video found [here](https://www.youtube.com/watch?v=WnNFZt0qjD0)
using Dubbing Studio from English to Spanish (or a language of your choosing). Select 6 speakers and keep the watermark.

![Dubbing project](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/dubbing/dubbing-project.png)

## ”Dynamic” vs. “Fixed” Generation

In Dubbing Studio, all regenerations made to the text are “Fixed” generations by default. This means that no matter how much text is in a Speaker card, that respective clip will not change its length. This is helpful to keep the timing of the video with the speech. However, this can be problematic if there are too many or too few words within the speaker card, as this can result in sped up or slowed down speech.

This is where “Dynamic” generation can help. You can access this by right clicking on a clip and selecting “Generate Audio (Dynamic Duration). You’ll notice now that the length of the clip will more appropriately match the text spoken for that section. For example, the phrase **“I’m doing well!”** should only occupy a small clip - if the clip was very long, the speech would be slurred and drawn out. This is where Dynamic generation can be helpful.

Just note, though, that this could affect the syncing and timing of your video. Additionally, if you choose “Dynamic Duration” for a clip that has many words, the clip will need to lengthen - if there is a clip directly in front of it, it will not have enough room to generate properly, so make sure you leave some space between your clips!

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voice isolator & background sound remover (product guide) — ElevenLabs Documentation

![Voice changer product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voice-isolator/voice-isolator-product-feature.png)

## Overview

Voice isolator is a tool that allows you to remove background noise from audio recordings.

## Guide

![Voice isolator](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voice-isolator/voice-isolator.png)

To use the voice isolator app, navigate to [Voice Isolator](/app/voice-isolator)
under the Audio Tools section. Here you can upload or drag and drop your audio file into the app, or record a new audio file with your device’s microphone.

Click “Isolate voice” to start the process. The app will isolate the voice from the background noise and return a new audio file with the isolated voice. Once the process is complete, you can download the audio file or play it back in the app.

The voice isolator functionality is also available via the [API](/docs/api-reference/audio-isolation/audio-isolation)
to easily integrate this functionality into your own applications.

[Voice isolator app\
\
Use the voice isolator app.](/app/voice-isolator)
[API reference\
\
Use the voice isolator API.](/docs/api-reference/audio-isolation/audio-isolation)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# AI speech classifier — ElevenLabs Documentation

![AI speech classifier](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/ai-speech-classifier/ai-speech-classifier.png)

## Overview

The AI speech classifier is a tool that allows you to detect if an audio file was generated by ElevenLabs.

## Guide

[1](/docs/product-guides/audio-tools/ai-speech-classifier#navigate-to-the-ai-speech-classifier-page)

### Navigate to the AI speech classifier page

Select the “AI speech classifier” option from the sidebar under “Audio Tools” in the ElevenLabs dashboard.

[2](/docs/product-guides/audio-tools/ai-speech-classifier#upload-an-audio-file)

### Upload an audio file

Click the “Upload audio” button upload an audio file and begin scanning.

[3](/docs/product-guides/audio-tools/ai-speech-classifier#analyze-the-audio-file)

### Analyze the audio file

The AI speech classifier will analyze the audio file and provide a result.

## FAQ

###### How accurate is the AI speech classifier?

Our classifier maintains high accuracy (99% precision, 80% recall) for audio files generated with ElevenLabs that have not been modified. We will continue to improve this tool, while exploring other detection tools that provide transparency about how content was created.

###### Does using the tool cost me anything?

No, the tool is free for all to use.

###### Do I have to be logged in to use the tool?

A [web version](https://elevenlabs.io/ai-speech-classifier)
of the tool is available for you to use without having to log in.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with Wix — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/wix#add-html-to-your-blog-post)

### Add HTML to your blog post

Navigate to your Wix site, sign in and open the settings page for the page you wish to narrate.

[2](/docs/product-guides/audio-tools/audio-native/wix#add-the-embed-code-to-your-blog-post)

### Add the embed code to your blog post

Click the ”+” symbol at the top of your content and select “HTML Code” from the menu.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wix-1.webp)

Paste the Audio Native embed code into the HTML box and click “Save”.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wix-2.webp)

[3](/docs/product-guides/audio-tools/audio-native/wix#publish-the-page)

### Publish the page

Click the “Publish” button in the top right corner of the editor.

[4](/docs/product-guides/audio-tools/audio-native/wix#navigate-to-the-live-version-of-the-blog-post)

### Navigate to the live version of the blog post

Finally, navigate to the live version of the blog post. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wix-3.webp)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Account — ElevenLabs Documentation

To begin using ElevenLabs, you’ll need to create an account. Follow these steps:

- **Sign Up**: Visit the [ElevenLabs website](https://elevenlabs.io/sign-up)
  and click on the ‘Get started free’ button. You can register using your email or through one of the OAuth providers.
- **Verify Email**: Check your email for a verification link from ElevenLabs. Click the link to verify your account.
- **Initial Setup**: After verification, you’ll be directed to the Speech Synthesis page where you can start generating audio from text.

**Exercise**: Try out an example to get started or type something, select a voice and click generate!

![Account creation exercise](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/administration/account-creation.png)

You can sign up with traditional email and password or using popular OAuth providers like Google, Facebook, and GitHub.

If you choose to sign up with your email, you will be asked to verify your email address before you can start using the service. Once you have verified your email, you will be taken to the Speech Synthesis page, where you can immediately start using the service. Simply type anything into the box and press “generate” to convert the text into voiceover narration. Please note that each time you press “generate” anywhere on the website, it will deduct credits from your quota.

If you sign up using Google OAuth, your account will be intrinsically linked to your Google account, meaning you will not be able to change your email address, as it will always be linked to your Google email.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Billing — ElevenLabs Documentation

[Pricing\
\
View the pricing page](/pricing)
[Subscription details\
\
View your subscription details](/app/subscription)

When signing up, you will be automatically assigned to the free tier. To view your subscription, click on “My Account” in the bottom left corner and select [“Subscription”](https://elevenlabs.io/app/subscription)
. You can read more about the different plans [here](https://elevenlabs.io/pricing)
. At the bottom of the page, you will find a comparison table to understand the differences between the various plans.

We offer five public plans: Free, Starter, Creator, Pro, Scale, and Business. In addition, we also offer an Enterprise option that’s specifically tailored to the unique needs and usage of large organizations.

You can see details of all our plans on the subscription page. This includes information about the total monthly credit quota, the number of custom voices you can have saved simultaneously, and the quality of audio produced.

Cloning is only available on the Starter tier and above. The free plan offers three custom voices that you can create using our [Voice Design tool](/docs/product-guides/voices/voice-design)
, or you can add voices from the [Voice Library](/docs/product-guides/voices/voice-library)
if they are not limited to the paid tiers.

You can upgrade your subscription at any time, and any unused quota from your previous plan will roll over to the new one. As long as you don’t cancel or downgrade, unused credits at the end of the month will carry over to the next month, up to a maximum of two months’ worth of credits. For more information, please visit our Help Center articles:

- [“How does credit rollover work?"](https://help.elevenlabs.io/hc/en-us/articles/27561768104081-How-does-credit-rollover-work)
- ["What happens to my subscription and quota at the end of the month?”](https://help.elevenlabs.io/hc/en-us/articles/13514114771857-What-happens-to-my-subscription-and-quota-at-the-end-of-the-month)

From the [subscription page](/app/subscription)
, you can also downgrade your subscription at any point in time if you would like. When downgrading, it won’t take effect until the current cycle ends, ensuring that you won’t lose any of the monthly quota before your month is up.

When generating content on our paid plans, you get commercial rights to use that content. If you are on the free plan, you can use the content non-commercially with attribution. Read more about the license in our [Terms of Service](https://elevenlabs.io/terms)
and in our Help Center [here](https://help.elevenlabs.io/hc/en-us/articles/13313564601361-Can-I-publish-the-content-I-generate-on-the-platform-)
.

For more information on payment methods, please refer to the [Help Center](https://help.elevenlabs.io/)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voice design — ElevenLabs Documentation

![Voice design](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voices-voice-design.png)

## Overview

Voice Design helps creators fill the gaps when the exact voice they are looking for isn’t available in the [Voice Library](/app/voice-library)
. If you can’t find a suitable voice for your project, you can create one. Note that Voice Design is highly experimental and [Professional Voice Clones (PVC)](/docs/product-guides/voices/voice-cloning)
are currently the highest quality voices on our platform. If there is a PVC available in our library that fits your needs, we recommend using it instead.

You can find Voice Design by heading to Voices -> My Voices -> Add a new voice -> Voice Design in the [ElevenLabs app](/app/voice-lab?create=true&creationType=voiceDesign)
or via the [API](/docs/api-reference/text-to-voice)
.

When you hit generate, we’ll generate three voice options for you. The only charge for using voice design is the number of credits to generate your preview text, which you are only charged once even though we are generating three samples for you. You can see the number of characters that will be deducated in the “Text to preview” text box.

After generating, you’ll have the option to select and save one of the generations, which will take up one of your voice slots.

[API reference\
\
See the API reference for Voice Design](/docs/api-reference/text-to-voice)
[Example app\
\
A Next.js example app for Voice Design](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-voice/x-to-voice)

## Prompting guide

### Voice design types

| Type                   | Description                                                                                                                     | Example Prompts                                                                                                                                                                                                                                                                                                                                                                   |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Realistic Voice Design | Create an original, realistic voice by specifying age, accent/nationality, gender, tone, pitch, intonation, speed, and emotion. | \- “A young Indian female with a soft, high voice. Conversational, slow and calm.” <br>\- “An old British male with a raspy, deep voice. Professional, relaxed and assertive.” <br>\- “A middle-aged Australian female with a warm, low voice. Corporate, fast and happy.”                                                                                                        |
| Character Voice Design | Generate unique voices for creative characters using simpler prompts.                                                           | \- “A massive evil ogre, troll” <br>\- “A sassy little squeaky mouse” <br>\- “An angry old pirate, shouting” <br> <br>Some other characters we’ve had success with include Goblin, Vampire, Elf, Troll, Werewolf, Ghost, Alien, Giant, Witch, Wizard, Zombie, Demon, Devil, Pirate, Genie, Ogre, Orc, Knight, Samurai, Banshee, Yeti, Druid, Robot, Elf, Monkey, Monster, Dracula |

### Voice attributes

| Attribute          | Importance      | Options                                                           |
| ------------------ | --------------- | ----------------------------------------------------------------- |
| Age                | High Importance | Young, Teenage, Adult, Middle-Aged, Old, etc…                     |
| Accent/Nationality | High Importance | British, Indian, Polish, American, etc…                           |
| Gender             | High Importance | Male, Female, Gender Neutral                                      |
| Tone               | Not Needed      | Gruff, Soft, Warm, Raspy, etc…                                    |
| Pitch              | Not Needed      | Deep, Low, High, Squeaky, etc…                                    |
| Intonation         | Not Needed      | Conversational, Professional, Corporate, Urban, Posh, etc…        |
| Speed              | Not Needed      | Fast, Quick, Slow, Relaxed, etc…                                  |
| Emotion/Delivery   | Not Needed      | Angry, Calm, Scared, Happy, Assertive, Whispering, Shouting, etc… |

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Payouts — ElevenLabs Documentation

![Payouts](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/payouts-product-feature.png)

## Overview

The [Payouts](https://elevenlabs.io/payouts)
(VL) system allows you to earn rewards for sharing voices in the Voice Library. ElevenLabs uses [Stripe Connect](https://stripe.com/connect)
to process reward payouts.

## Account setup

To set up your Payouts account:

- Click on your account in the bottom left and select [“Payouts”](/app/payouts)
  .

![Payouts overview](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/payouts-overview.png)

- Follow the prompts from Stripe Connect to complete the account setup.

## Tracking usage and earnings

- You can track the usage of your voices by going to [“My Voices”](/app/voice-lab)
  , clicking “View” to open the detailed view for your voice, then clicking the sharing icon at the bottom. Once you have the Sharing Options open, click “View Metrics”.
- The rewards you earn are based on the options you selected when [sharing your voice in the Voice Library](/docs/product-guides/voices/voice-library#sharing-voices)
  .
- You can also see your all-time earnings and past payouts by going back to your Payouts page.

## Reader app rewards

- If your voice is marked as **[High-Quality](/docs/product-guides/voices/voice-library#category)
  ** and you have activated the “Available in ElevenReader” toggle, your voice will made be available in the [ElevenReader app](/text-reader)
  . Rewards for ElevenReader are reported separately – to view your Reader App rewards, check the “ElevenReader” box on your “View Metrics” screen.

## Things to know

- Rewards accumulate frequently throughout the day, but payouts typically happen once a week as long as you have more than $10 in accrued payouts. You can see your past payouts by going to your [Payouts](/app/payouts)
  page in the sidebar.

## Supported countries

- Currently, Stripe Connect is not supported in all countries. We are constantly working to expand our reach for Payouts and plan to add availability in more countries when possible.

###### Supported countries

Argentina, Australia, Austria, Belgium, Bulgaria, Canada, Chile, Colombia, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hong Kong SAR China, Hungary, Iceland, India, Indonesia, Ireland, Israel, Italy, Japan, Latvia, Liechtenstein, Lithuania, Luxembourg, Malaysia, Malta, Mexico, Monaco, Netherlands, New Zealand, Nigeria, Norway, Peru, Philippines, Poland, Portugal, Qatar, Romania, Saudi Arabia, Singapore, Slovakia, Slovenia, South Africa, South Korea, Spain, Sweden, Switzerland, Thailand, Taiwan, Turkey, United Arab Emirates, United Kingdom, United States, Uruguay & Vietnam

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native Overview — ElevenLabs Documentation

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-product-feature.png)

## Overview

Audio Native is an embedded audio player that automatically voices content of a web page using ElevenLab’s [Text to Speech](/docs/product-guides/playground/text-to-speech)
service. It can also be used to embed pre-generated content from a project into a web page. All it takes to embed on your site is a small HTML snippet. In addition, Audio Native provides built-in metrics allowing you to precisely track audience engagement via a listener dashboard.

The end result will be a Audio Native player that can narrate the content of a page, or, like in the case below, embed pre-generated content from a project:

## Guide

[1](/docs/product-guides/audio-tools/audio-native#navigate-to-audio-native)

### Navigate to Audio Native

In the ElevenLabs dashboard, under “Audio Tools” navigate to [“Audio Native”](/app/audio-native)
.

[2](/docs/product-guides/audio-tools/audio-native#configure-player-appearance)

### Configure player appearance

Customize the player apperance by selecting background and text colors.

[3](/docs/product-guides/audio-tools/audio-native#configure-allowed-sites)

### Configure allowed sites

The URL allowlist is the list of web pages that will be permitted to play your content.

You can choose to add a specific web page (e.g. `https://elevenlabs.io/blog`) or add a whole domain to the allowlist (e.g. `http://elevenlabs.io`). If a player is embedded on a page that is not in the allowlist, it will not work as intended.

[4](/docs/product-guides/audio-tools/audio-native#get-embed-code)

### Get embed code

Once you’ve finished configuring the player and allowlist, copy the embed code and paste it into your website’s source code.

## Technology-specific guides

To integrate Audio Native into your web techology of choice, see the following guides:

[React](/docs/product-guides/audio-tools/audio-native/react)
[Ghost](/docs/product-guides/audio-tools/audio-native/ghost)
[Squarespace](/docs/product-guides/audio-tools/audio-native/squarespace)
[Framer](/docs/product-guides/audio-tools/audio-native/framer)
[Webflow](/docs/product-guides/audio-tools/audio-native/webflow)
[Wordpress](/docs/product-guides/audio-tools/audio-native/word-press)
[Wix](/docs/product-guides/audio-tools/audio-native/wix)

## Using the API

You can use the [Audio Native API](/docs/api-reference/audio-native/create)
to programmatically create an Audio Native player for your existing content.

PythonJavaScript

`  |     |     | | --- | --- | | 1   | from elevenlabs import ElevenLabs | | 2   |     | | 3   | client = ElevenLabs( | | 4   | api_key="YOUR_API_KEY", | | 5   | )   | | 6   | response = client.audio_native.create( | | 7   | name="name", | | 8   | )   | | 9   |     | | 10  | # Use the snippet in response.html_snippet to embed the player on your website |    `

## Settings

###### Voice and model

### Voices

To configure the voice and model that will be used to read the content of the page, navigate to the “Settings” tab and select the voice and modelyou want to use.

###### Pronunciation dictionaries

### Pronunciation dictionaries

Sometimes you may want to specify the pronunciation of certain words, such as character or brand names, or specify how acronyms should be read. Pronunciation dictionaries allow this functionality by enabling you to upload a lexicon or dictionary file that includes rules about how specified words should be pronounced, either using a phonetic alphabet (phoneme tags) or word substitutions (alias tags).

Whenever one of these words is encountered in a project, the AI will pronounce the word using the specified replacement. When checking for a replacement word in a pronunciation dictionary, the dictionary is checked from start to end and only the first replacement is used.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with Ghost — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/ghost#add-html-to-your-blog-post)

### Add HTML to your blog post

Navigate to your Ghost blog, sign in and open the settings page for the blog post you wish to narrate.

[2](/docs/product-guides/audio-tools/audio-native/ghost#add-the-embed-code-to-your-blog-post)

### Add the embed code to your blog post

Click the ”+” symbol on the left and select “HTML” from the menu.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-ghost-1.webp)

Paste the Audio Native embed code into the HTML box and press enter.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-ghost-2.webp)

[3](/docs/product-guides/audio-tools/audio-native/ghost#update-the-blog-post)

### Update the blog post

Click the “Update” button in the top right corner of the editor, which should now be highlighted in green text.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-ghost-3.webp)

[4](/docs/product-guides/audio-tools/audio-native/ghost#navigate-to-the-live-version-of-the-blog-post)

### Navigate to the live version of the blog post

Finally, navigate to the live version of the blog post. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-ghost-4.webp)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Workspaces — ElevenLabs Documentation

![Workspaces](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/workspaces/workspace-main.png)

Workspaces are currently only available for Enterprise customers. To upgrade, [get in touch with our sales team](https://elevenlabs.io/contact-sales)
.

## Overview

For teams that want to collaborate in ElevenLabs, we offer shared workspaces. Workspaces offer the folowing benefits:

- **Shared billing** - Rather than having each of your team members individually create & manage subscriptions, all of your team’s character usage and billing is centralized under one workspace.
- **Shared resources** - Within a workspace, your team can share: voices, studio instances, conversational AI agents, dubbings and more.
- **Access management** - Your workspace admin can easily add and remove team members.
- **API Key management** - You can issue and revoke unlimited API keys for your team.

## FAQ

###### How do I create a workspace?

### Creating a workspace

Workspaces are automatically enabled for all Enterprise clients. When setting up your account, you’ll be asked to nominate a workspace admin who will have the power to add more team members as well as nominate others to be an admin.

###### How do I add a team member to a workspace?

### Adding a team member to a workspace

Only administrators can add and remove team members.

![Workspace domain verification](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/workspaces/workspace-product-feature.png)

Once you are logged in, select your profile in the bottom left of the dashboard and choose Workspace Settings and then navigate to the “Members” tab. From there you’ll be able to add team members, assign roles and remove members from the workspace.

###### What roles can I assign members?

### Roles

There are two roles, Admins and Members. Members have full access to your workspace and can generate an unlimited number of characters (within your current overall plan’s limit).

Admins have all of the access of Members, with the added ability to add/remove teammates and permissions to manage your subscription.

###### How do I manage billing?

### Managing Billing

Only admins can manage billing.

To manage your billing, select your profile in the bottom left of the dashboard and choose “Subscription”. From there, you’ll be able to update your payment information and access past invoices.

###### How do I manage API keys?

### Managing API Keys

To manage workspace API keys, select your profile in the bottom left of the dashboard and choose “Workspace settings”. Navigate to the “API keys” tab and you’ll be able to issue & revoke API keys.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Libraries & SDKs — ElevenLabs Documentation

## Official REST API libraries

ElevenLabs provides officially supported libraries that are updated with the latest features available in the [REST API](/docs/api-reference/introduction)
.

| Language          | GitHub                                                           | Package Manager                                 |
| ----------------- | ---------------------------------------------------------------- | ----------------------------------------------- |
| Python            | [GitHub README](https://github.com/elevenlabs/elevenlabs-python) | [PyPI](https://pypi.org/project/elevenlabs/)    |
| Javascript (Node) | [GitHub README](https://github.com/elevenlabs/elevenlabs-js)     | [npm](https://www.npmjs.com/package/elevenlabs) |

Test and explore all ElevenLabs API endpoints using our official [Postman collection](https://www.postman.com/elevenlabs/elevenlabs/collection/7i9rytu/elevenlabs-api-documentation?action=share&creator=39903018)
.

## Conversational AI libraries

These libraries are designed for use with ElevenLabs [Conversational AI](/docs/conversational-ai/overview)
.

| Language   | Documentation                                         | Package Manager                                         |
| ---------- | ----------------------------------------------------- | ------------------------------------------------------- |
| Javascript | [Docs](/docs/conversational-ai/libraries/java-script) | [npm](https://www.npmjs.com/package/@11labs/client)     |
| React      | [Docs](/docs/conversational-ai/libraries/react)       | [npm](https://www.npmjs.com/package/@11labs/react)      |
| Python     | [Docs](/docs/conversational-ai/libraries/python)      | [PyPI](https://pypi.org/project/elevenlabs/)            |
| Swift      | [Docs](/docs/conversational-ai/libraries/swift)       | [Github](https://github.com/elevenlabs/ElevenLabsSwift) |

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# User groups — ElevenLabs Documentation

![Group Management](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/workspaces/manage-group.png)

## Overview

Only workspace admins can create, edit, and delete user groups.

User groups allow you to manage permissions for multiple users at once.

## Creating a user group

You can create a user group from the workspace settings page. You can then [share resources](/docs/product-guides/administration/workspaces/sharing-resources)
with the group directly. If access to a user group is lost, access to resources shared with that group is also lost.

## Multiple groups

User groups cannot be nested, but you can add users to multiple groups. If a user is part of multiple groups, they will have the union of all the permissions of the groups they are part of.

For example, you can create a voice and grant the **Sales** and **Marketing** groups viewer and editor roles on the voice, respectively. If a user is part of both groups, they will have editor permissions on the voice. Losing access to the **Marketing** group will downgrade the user’s permissions to viewer.

## Disabling platform features

Permissions for groups can be revoked for specific product features, such as Professional Voice Cloning or Sound Effects. To do this, you first have to remove the relevant permissions from the **Everyone** group. Afterwards, enable the permissions for each group that should have access.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voice Cloning overview — ElevenLabs Documentation

![Voice cloning product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voice-cloning-product-feature.jpg)

## Overview

When cloning a voice, there are two main options: Instant Voice Cloning (IVC) and Professional Voice Cloning (PVC). IVC is a quick and easy way to clone your voice, while PVC is a more accurate and customizable option.

## Instant Voice Cloning

![Instant voice
cloning](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voices-ivc-creation.jpg)

IVC allows you to create voice clones form shorter samples near instantaneously. Creating an instant voice clone does not train or create a custom AI model. Instead, it relies on prior knowledge from training data to make an educated guess rather than training on the exact voice. This works extremely well for a lot of voices.

However, the biggest limitation of IVC is if you are trying to clone a very unique voice with a very unique accent where the AI might not have heard a similar voices before during training. In such cases, creating a custom model with explicit training using PVC might be the best option.

## Professional Voice Cloning

![Professional voice
cloning](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voices-pvc-creation.jpg)

A PVC is a special feature that is available to our Creator+ plans. PVC allows you to train a hyper-realistic model of a voice. This is achieved by training a dedicated model on a large set of voice data to produce a model that’s indistinguishable from the original voice.

Since the custom models require fine-tuning and training, it will take a bit longer to train these PVCs compared to an IVC. Giving an estimate is challenging as it depends on the number of people in the queue before you and a few other factors.

Here are the current estimates for PVC:

- **English:** ~3 hours
- **Multilingual:** ~6 hours

## Beginner’s guide to audio recording

If you’re new to audio recording, here are some tips to help you get started.

### Recording location

When recording audio, choose a suitable location and set up to minimize room echo/reverb. So, we want to “deaden” the room as much as possible. This is precisely what a vocal booth that is acoustically treated made for, and if you do not have a vocal booth readily available, you can experiment with some ideas for a DIY vocal booth, “blanket fort”, or closet.

Here are a few YouTube examples of DIY acoustics ideas:

- [I made a vocal booth for $0.00!](https://www.youtube.com/watch?v=j4wJMDUuHSM)
- [How to Record GOOD Vocals in a BAD Room](https://www.youtube.com/watch?v=TsxdHtu-OpU)
- [The 5 BEST Vocal Home Recording TIPS!](https://www.youtube.com/watch?v=K96mw2QBz34)

### Microphone, pop-filter, and audio interface

A good microphone is crucial. Microphones can range from $100 to $10,000, but a professional XLR microphone costing $150 to $300 is sufficient for most voiceover work.

For an affordable yet high-quality setup for voiceover work, consider a **Focusrite** interface paired with an **Audio-Technica AT2020** or **Rode NT1 microphone**. This setup, costing between $300 to $500, offers high-quality recording suitable for professional use, with minimal self-noise for clean results.

Please ensure that you have a proper **pop-filter** in front of the microphone when recording to avoid plosives as well as breaths and air hitting the diaphragm/microphone directly, as it will sound poor and will also cause issues with the cloning process.

### Digital Audio Workstation (DAW)

There are many different recording solutions out there that all accomplish the same thing: recording audio. However, they are not all created equally. As long as they can record WAV files at 44.1kHz or 48kHz with a bitrate of at least 24 bits, they should be fine. You don’t need any fancy post-processing, plugins, denoisers, or anything because we want to keep audio recording simple.

If you want a recommendation, we would suggest something like **REAPER**, which is a fantastic DAW with a tremendous amount of flexibility. It is the industry standard for a lot of audio work. Another good free option is **Audacity**.

Maintain optimal recording levels (not too loud or too quiet) to avoid digital distortion and excessive noise. Aim for peaks of -6 dB to -3 dB and an average loudness of -18 dB for voiceover work, ensuring clarity while minimizing the noise floor. Monitor closely and adjust levels as needed for the best results based on the project and recording environment.

### Positioning

One helpful guideline to follow is to maintain a distance of about two fists away from the microphone, which is approximately 20cm (7-8 in), with a pop filter placed between you and the microphone. Some people prefer to position the pop filter all the way back so that they can press it up right against it. This helps them maintain a consistent distance from the microphone more easily.

Another common technique to avoid directly breathing into the microphone or causing plosive sounds is to speak at an angle. Speaking at an angle ensures that exhaled air is less likely to hit the microphone directly and, instead, passes by it.

### Performance

The performance you give is one of the most crucial aspects of this entire recording session. The AI will try to clone everything about your voice to the best of its ability, which is very high. This means that it will attempt to replicate your cadence, tonality, performance style, the length of your pauses, whether you stutter, take deep breaths, sound breathy, or use a lot of “uhms” and “ahs” – it can even replicate those. Therefore, what we want in the audio file is precisely the performance and voice that we want to clone, nothing less and nothing more. That is also why it’s quite important to find a script that you can read that fits the tonality we are aiming for.

When recording for AI, it is very important to be consistent. if you are recording a voice either keep it very animated throughout or keep it very subdued throughout you can’t mix and match or the AI can become unstable because it doesn’t know what part of the voice to clone. same if you’re doing an accent keep the same accent throughout the recording. Consistency is key to a proper clone!

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Instant Voice Cloning — ElevenLabs Documentation

![Voice cloning product feature](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voice-cloning-product-feature.jpg)

## Creating an Instant Voice Clone

When cloning a voice, it’s important to consider what the AI has been trained on: which languages and what type of dataset. You can find more information about which languages each model has been trained on in our [help center](https://help.elevenlabs.io/hc/en-us/articles/17883183930129-What-models-do-you-offer-and-what-is-the-difference-between-them)
.

Read more about each individual model and their strengths in the [Models page](/docs/models)
).

## Guide

If you are unsure about what is permissible from a legal standpoint, please consult the [Terms of Service](https://elevenlabs.io/terms-of-use)
and our [AI Safety information](https://elevenlabs.io/safety)
for more information.

[1](/docs/product-guides/voices/voice-cloning/instant-voice-cloning#navigate-to-the-instant-voice-cloning-page)

### Navigate to the Instant Voice Cloning page

In the ElevenLabs dashboard, select the “Voices” section on the left, then click “Add a new voice”.

From the modal, select “Instant Voice Clone”.

[2](/docs/product-guides/voices/voice-cloning/instant-voice-cloning#upload-or-record-your-audio)

### Upload or record your audio

Follow the on-screen instructions to upload or record your audio.

[3](/docs/product-guides/voices/voice-cloning/instant-voice-cloning#confirm-voice-details)

### Confirm voice details

![Voice cloning IVC modal](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voice-cloning-ivc-modal.jpg)

Name and label your voice clone, confirm that you have the right and consent to clone the voice, then click “Save voice”.

[4](/docs/product-guides/voices/voice-cloning/instant-voice-cloning#use-your-voice-clone)

### Use your voice clone

Under the “Voices” section in the dashboard, select the “Personal” tab, then click on your voice clone to begin using it.

## Best practices

###### Record at least 1 minute of audio

#### Record at least 1 minute of audio

Avoid recording more than 3 minutes, this will yield little improvement and can, in some cases, even be detrimental to the clone.

How the audio was recorded is more important than the total length (total runtime) of the samples. The number of samples you use doesn’t matter; it is the total combined length (total runtime) that is the important part.

Approximately 1-2 minutes of clear audio without any reverb, artifacts, or background noise of any kind is recommended. When we speak of “audio or recording quality,” we do not mean the codec, such as MP3 or WAV; we mean how the audio was captured. However, regarding audio codecs, using MP3 at 128 kbps and above is advised. Higher bitrates don’t have a significant impact on the quality of the clone.

###### Keep the audio consistent

#### Keep the audio consistent

The AI will attempt to mimic everything it hears in the audio. This includes the speed of the person talking, the inflections, the accent, tonality, breathing pattern and strength, as well as noise and mouth clicks. Even noise and artefacts which can confuse it are factored in.

Ensure that the voice maintains a consistent tone throughout, with a consistent performance. Also, make sure that the audio quality of the voice remains consistent across all the samples. Even if you only use a single sample, ensure that it remains consistent throughout the full sample. Feeding the AI audio that is very dynamic, meaning wide fluctuations in pitch and volume, will yield less predictable results.

###### Replicate your performance

#### Replicate your performance

Another important thing to keep in mind is that the AI will try to replicate the performance of the voice you provide. If you talk in a slow, monotone voice without much emotion, that is what the AI will mimic. On the other hand, if you talk quickly with much emotion, that is what the AI will try to replicate.

It is crucial that the voice remains consistent throughout all the samples, not only in tone but also in performance. If there is too much variance, it might confuse the AI, leading to more varied output between generations.

###### Find a good balance for the volume

#### Find a good balance for the volume

Find a good balance for the volume so the audio is neither too quiet nor too loud. The ideal would be between -23 dB and -18 dB RMS with a true peak of -3 dB.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with Squarespace — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/squarespace#add-html-to-your-blog-post)

### Add HTML to your blog post

Navigate to your Squarespace site, sign in and open the page you wish to add narration to.

[2](/docs/product-guides/audio-tools/audio-native/squarespace#add-the-embed-code-to-your-blog-post)

### Add the embed code to your blog post

Click the ”+” symbol on the spot you want to place the Audio Native player and select “Code” from the menu.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-squarespace-1.png)

Paste the Audio Native embed code into the HTML box and press enter.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-squarespace-2.png)

[3](/docs/product-guides/audio-tools/audio-native/squarespace#update-the-blog-post)

### Update the blog post

Click the “Save” button in the top right corner of the editor, which should now be highlighted.

[4](/docs/product-guides/audio-tools/audio-native/squarespace#navigate-to-the-live-version-of-the-blog-post)

### Navigate to the live version of the blog post

Finally, navigate to the live version of the blog post. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-squarespace-3.png)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Sharing resources — ElevenLabs Documentation

![Sharing a project](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/workspaces/share-project.png)

## Overview

This is a new feature we are rolling out gradually to all our customers. If you don’t see the share options yet and would like to get ahead, please reach out to [success@elevenlabs.io](mailto:success@elevenlabs.io)
.

If your subscription plan includes multiple seats, you can share resources with your members. Currently, you can share:

- Voices
- Studio assets
- Conversational AI agents
- Dubbings
- Voiceover Studio assets

## Sharing

You can share a **resource** with a **principal**. A principal is one of the following:

- A user
- A user group
- A workspace API key

A resource can be shared with at most 100 principals.

Workspace API keys behave like individual users. They don’t have access to anything in the workspace when they are created, but they can be added to resources by resource admins.

## Roles

When you share a resource with a principal, you can assign them a **role**. We support the following roles:

- **Viewer**: Viewers can discover the resource and its contents. They can also “use” the resource, e.g., generate TTS with a voice or listen to the audio of a studio instance.
- **Editor**: Everything a viewer can do, plus they can also edit the contents of the resource.
- **Admin**: Everything an editor can do, plus they can also delete the resource and manage sharing permissions.

When you create a resource, you have admin permissions on it. Other resource admins cannot remove your admin permissions on the resources you created.

Workspace admins have admin permissions on all resources in the workspace. This can be removed from them only by removing their workspace admin role.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Single Sign-On (SSO) — ElevenLabs Documentation

![SSO](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/workspaces/workspace-sso.png)

## Overview

Only workspace admins can use this feature.

Single Sign-On (SSO) allows your team to log in to ElevenLabs by using your existing identity provider. This allows your team to use the same credentials they use for other services to log in to ElevenLabs.

## Guide

[1](/docs/product-guides/administration/workspaces/sso#access-your-sso-settings)

### Access your SSO settings

Click on your profile icon located at the bottom left of the dashboard, select “Workspace settings”, and then navigate to the “Security & SSO” tab.

[2](/docs/product-guides/administration/workspaces/sso#choose-identity-providers)

### Choose identity providers

You can choose from a variety of pre-configured identity providers, including Google, Apple, GitHub, etc. Custom organization SSO providers will only appear in this list after they have been configured, as shown in the “SSO Provider” section.

[3](/docs/product-guides/administration/workspaces/sso#verify-your-email-domain)

### Verify your email domain

Next, you need to verify your email domain for authentication. This lets ElevenLabs know that you own the domain you are configuring for SSO. This is a security measure to prevent unauthorized access to your workspace.

Click the “Verify domain” button and enter the domain name you want to verify. After completing this step, click on the domain pending verification. You will be prompted to add a DNS TXT record to your domain’s DNS settings. Once the DNS record has been added, click on the “Verify” button.

[4](/docs/product-guides/administration/workspaces/sso#configure-sso)

### Configure SSO

If you want to configure your own SSO provider, select the SSO provider dropdown to select between OIDC (OpenID Connect) and SAML (Security Assertion Markup Language). Note that only Service Provider (SP) initiated SSO is supported for SAML.

Once you’ve filled out the required fields, click the “Update SSO” button to save your changes.

Configuring a new SSO provider will log out all workspace members currently logged in with SSO.

## FAQ

###### Microsoft Entra Identifier / Azure AD - SAML

What shall I fill for Identifier (Entity ID)?

- Use Service Provider Entity Id

What shall I fill for Reply URL (Assertion Consumer Service) URL in SAML?

- Use Redirect URL

What is ACS URL?

- Same as Assertion Consumer Service URL

Which fields should I use to provide ElevenLabs?

- Use _Microsoft Entra Identifier_ for IdP Entity ID
- Use _Login URL_ for IdP Sign-In URL

###### I am getting the error 'Unable to login with saml.workspace...'

- One known error: Inside the `<saml:Subject>` field of the SAML response, make sure `<saml:NameID>` is set to the email address of the user.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Error messages — ElevenLabs Documentation

This guide includes an overview of error messages you might see in the ElevenLabs dashboard & API.

## Dashboard errors

| Error Message                                          | Cause                                                                                                     | Solution                                                                                                                                                        |
| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| The selected model can not be used for text-to-speech. | Occurs when switching between speech-to-speech and text-to-speech if the model does not switch correctly. | Select the desired model. If unresolved, select a different model, then switch back.                                                                            |
| Oops, something went wrong.                            | Indicates a client-side error, often due to device or browser issues.                                     | Click “Try again” or refresh the page. If unresolved, clear browser cache and cookies. Temporarily pause browser-based translation tools like Google Translate. |

If error messages persist after following these solutions, please [contact our support team](https://help.elevenlabs.io/hc/en-us/requests/new?ticket_form_id=13145996177937)
for further assistance.

## API errors

### Code 400/401

| Code                         | Overview                                                                                                                                                                                                     |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| max_character_limit_exceeded | **Cause:** You are sending too many characters in a single request. <br>**Solution:** Split the request into smaller chunks, see [character limits](/docs/models#character-limits)<br> for more information. |
| invalid_api_key              | **Cause:** You have not set your API key correctly. <br>**Solution:** Ensure the request is correctly authenticated. See [authentication](/docs/api-reference/authentication)<br> for more information.      |
| quota_exceeded               | **Cause:** You have insufficient quota to complete the request. <br>**Solution:** On the Creator plan and above, you can enable usage-based billing from your Subscription page.                             |
| voice_not_found              | **Cause:** You have entered the incorrect voice_id. <br>**Solution:** Check that you are using the correct voice_id for the voice you want to use. You can verify this in My Voices.                         |

### Code 403

| Code              | Overview                                                                                                                                                                 |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| only_for_creator+ | **Cause:** You are trying to use professional voices on a free or basic subscription. <br>**Solution:** Upgrade to Creator tier or higher to access professional voices. |

### Code 429

| Code                         | Overview                                                                                                                                                                                                  |
| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| too_many_concurrent_requests | **Cause:** You have exceeded the concurrency limit for your subscription. <br>**Solution:** See [concurrency limits and priority](/docs/models#concurrency-limits-and-priority)<br> for more information. |
| system_busy                  | **Cause:** Our services are experiencing high levels of traffic and your request could not be processed. <br>**Solution:** Retry the request. If the issue persists, please contact support.              |

If error messages persist after following these solutions, please [contact our support team](https://help.elevenlabs.io/hc/en-us/requests/new?ticket_form_id=13145996177937)
for further assistance.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voice library — ElevenLabs Documentation

![Voice library](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voices-voice-library.png)

## Overview

The [Voice Library](https://elevenlabs.io/voice-library)
(VL) is a marketplace where our community can share voices and earn rewards when they’re used. At the moment, only Professional Voice Clones (PVCs) can be shared in the library. Instant Voice Clones (IVCs) cannot be shared for safety reasons.

## Using voices from the Voice Library

You can play a sample for any voice in the Voice Library by clicking it.

To use a voice from the Voice Library, you first need to add it to My Voices. To do this, click “Add”. This will save it to My Voices using the default name for the voice. You can use it directly from the Voice Library by clicking “Use”, which will open Speech Synthesis with the voice selected.

Once the voice has been added to My Voices, it will appear in the voice selection menu across all features.

## Details view

You can find out more information about a voice by clicking “View”. This opens up a pane on the right which contains more information. Here you can see all the tags associated with the voice, including:

- the language it was trained on
- the age and gender of the voice
- the category, for example, “Conversational”
- how long the notice period is, if the voice has one
- if the voice has been labelled as High Quality
- what type of voice it is, for example, Professional Voice Clone

You can also see how many users have saved the voice to My Voices, and how many characters of audio have been generated with the voice.

Finally, you can also see suggestions of similar voices, and can play samples and add these to My Voices if you want.

### Category

Some labels tell you about the type of voice:

Voice Design voices are no longer shareable in the Voice Library; however, the legacy shared voices will remain accessible.

Voice Design

Generated voices made using **[Voice Design](/docs/product/voices/voice-lab/voice-design)
**

Professional Voice Clone

Voices made using **[Professional Voice Cloning](/docs/product/voices/voice-lab/professional-voice-cloning)
**

HQ

The HQ label stands for High Quality, and indicates that this Professional Voice Clone has been trained on audio which follows our **[Professional Recording Guidelines](/docs/product/voices/voice-lab/professional-voice-cloning)
** and has passed a quality control check on input texts of various lengths.

### Sharing Options

Other labels tell you about options the voice owner set when sharing the voice. Please see the **[Sharing](/docs/product/voices/voice-library/sharing)
** page for more details.

Notice Period

A label with a clock icon indicates that the voice has a Notice Period in place. The Notice Period lets you now how long you’ll continue to have access to the voice if the voice owner decides to remove it from the Voice Library.

Credit Multiplier

Some voices have a credit multiplier in place. This is shown by a label displaying, for example, x2 multiplier or x3 multiplier. This means that the voice owner has set a custom rate for use of their voice. Please pay close attention, as credit multipliers mean your account will be deducted >1x the number of credits when you generate using a voice that has a credit multiplier.

Live Moderation

Some voices have “Live Moderation” enabled. This is indicated with a label with a shield icon. When you generate using a voice with Live Moderation enabled, we use tools to check whether the text being generated belongs to a number of prohibited categories. This may introduce extra latency when using the voice, and voices with Live Moderation enabled cannot be used in Projects.

## Filters, Sorting, and Search

To help you find the perfect voice for you, the Voice Library is searchable and filterable.

### Search box

You can use the search box to search by name, keyword and voice ID. You can also search by dragging and dropping an audio file, or uploading a file by clicking the upload icon. This will return the voice used, if it can be found, along with similar voices.

### Sort by

You have a number of options for sorting voices in the Voice Library:

- Trending: voices are ranked by our trending algorithm
- Latest: newest voices are shown first
- Most users
- Most characters generated

### Language filter

The language filter allows you to return only voices that have been trained on audio in a specific language. While all voices are compatible with our multilingual models and can therefore be used with all 32 languages we support, voices labelled with a specific language should perform well for content in that language

### Accent filter

If you select a specific language, the Accent filter will also become available. This allows you to look for voices with specific accents.

### More filters

Click the “More filters” button to access additional filters.

#### Category

- Voice Design
- Professional
- High-Quality

#### Gender

- Male
- Female
- Neutral

#### Age

- Young
- Middle Aged
- Old

#### Use case

You can click the use case of your choice to show only voices that have been labelled with this use case.

#### Voice Notice Periods

When voice creators remove voices from the library, users who have added these voices to their “My Voices” collection receive advance notice through email and in-app notifications. These notifications: Specify when the voice will become unavailable Recommend similar alternative voices from the library This notice period, determined by the voice creator, ensures users have time to transition to new voices and maintain continuity in their projects.

## Sharing voices

![Voice sharing](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-sharing.png)

![Voice sharing overview](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-sharing-overview.png)

![Voice sharing options](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-sharing-options.png)

## How to share a voice model in the Voice Library:

**1\. Share Button:** To get started with sharing a voice model, find the voice model you want to share in [My Voices](/docs/product/voices/voice-lab/overview)
and click the share icon:

**2\. Sharing Toggle:** Next, activate sharing by enabling the “Sharing” toggle. Note that this doesn’t make your voice model automatically discoverable in the Voice Library.

**3\. Sharing Link/Email Whitelist:** Once the “Sharing” toggle is enabled, you have a few ways to share your Voice Model:

- **Sharing Link:** share this link with your audience, your friends, or anyone else that you want to be able to make a copy of your voice model in My Voices.
- **Email Whitelist:** you can specify specific emails to restrict who can make copies of your voice model in My Voices using your Sharing Link. If you leave the whitelist blank, all emails will be enabled by default.
- **Discovery in Voice Library:** this makes your voice model discoverable in the Voice Library and takes you to the sharing dialog detailed in Step 4 below.

**4\. Library Sharing Options:** if you enable “Discovery in Voice Library”, you’ll be brought to a dialog screen where you can configure a few options for sharing your voice model in the Voice Library:

Please see the [Voice Library Addendum](https://elevenlabs.io/vla)
to our Terms of Service for full descriptions of these options.

**5\. Naming Guidelines:** Please ensure the name you give your voice model adheres to the guidelines shown in the sharing dialog:

- The naming pattern should be a one-word name followed by a 2-4 word description, separated by a hyphen (-).
- Your name should NOT include the following:

  - Names of public individuals or entities (company names, band names, influencers or famous people, etc).
  - Social handles (Twitter, Instagram, you name it, etc).
  - ALL CAPS WORDS.
  - Emojis and any other non-letter characters.
  - Explicit or harmful words.
  - The word “voice”.

- Some examples of names following our guidelines:

  - Anna - calm and kind
  - Robert - friendly grandpa
  - Steve - wise teacher
  - Harmony - soothing serenader
  - Jasper - jovial storyteller
  - Maya - confident narrator

**6\. Scroll and accept terms:** Before sharing your voice model in the Voice Library, you’ll be asked to scroll and accept the [Voice Library Addendum](https://elevenlabs.io/terms#VLA)
to our [Terms of Service](https://elevenlabs.io/terms)
and provide additional consents and confirmations. Please do this carefully and ensure you fully understand our service before sharing. If you have any questions at this stage, you can reach out to us at [legal@elevenlabs.io](mailto:legal@elevenlabs.io)
.

Before you share your voice to the Voice Library, we have a few guidelines that need to be followed. These guidelines are in place to ensure better discoverability and to maintain a clean and organized appearance for everyone using the platform. Please take the time to read through the guidelines below. They will help you understand how you should name, categorize, and tag your voice to enhance the overall experience for users.

### Review

Once you’ve created, named, and shared your voice, it will be set for pending review. This means that someone from the ElevenLabs team will go through your voice to ensure that it adheres to the guidelines outlined above. If there are significant issues, your request to share the voice model will be declined. If only small changes are required, the team might make these adjustments for you and approve the voice model for sharing.

As part of the review process, our team may add labels to your voice model to make it discoverable using the filters for the Voice Library:

- Gender
- Accent
- Language (the language of the source audio used to create your PVC)
- Age
- Use case
- Descriptive

Consistently uploading voices that do not adhere to the guidelines or are highly explicit in nature might result in being barred from uploading and sharing voices altogether. Therefore, please adhere to the guidelines.

Currently, we do not have an estimate of how long the review process will take, as it is highly dependent on the length of the queue.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with React — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

This guide will show how to integrate Audio Native into React apps. The focus will be on a Next.js project, but the underlying concepts will work for any React based application.

[1](/docs/product-guides/audio-tools/audio-native/react#create-an-audio-native-react-component)

### Create an Audio Native React component

After completing the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
, you’ll have an embed code snippet. Here’s an example snippet:

Embed code snippet

`  |     |     | | --- | --- | | 1   | <div | | 2   | id="elevenlabs-audionative-widget" | | 3   | data-height="90" | | 4   | data-width="100%" | | 5   | data-frameborder="no" | | 6   | data-scrolling="no" | | 7   | data-publicuserid="public-user-id" | | 8   | data-playerurl="https://elevenlabs.io/player/index.html" | | 9   | data-projectid="project-id" | | 10  | >   | | 11  | Loading the <a href="https://elevenlabs.io/text-to-speech" target="_blank" rel="noopener">Elevenlabs Text to Speech</a> AudioNative Player... | | 12  | </div> | | 13  | <script src="https://elevenlabs.io/player/audioNativeHelper.js" type="text/javascript"></script> |    `

We can extract the data from the snippet to create a customizable React component.

ElevenLabsAudioNative.tsx

`  |     |     | | --- | --- | | 1   | // ElevenLabsAudioNative.tsx | | 2   |     | | 3   | 'use client'; | | 4   |     | | 5   | import { useEffect } from 'react'; | | 6   |     | | 7   | export type ElevenLabsProps = { | | 8   | publicUserId: string; | | 9   | textColorRgba?: string; | | 10  | backgroundColorRgba?: string; | | 11  | size?: 'small' \| 'large'; | | 12  | children?: React.ReactNode; | | 13  | };  | | 14  |     | | 15  | export const ElevenLabsAudioNative = ({ | | 16  | publicUserId, | | 17  | size, | | 18  | textColorRgba, | | 19  | backgroundColorRgba, | | 20  | children, | | 21  | }: ElevenLabsProps) => { | | 22  | useEffect(() => { | | 23  | const script = document.createElement('script'); | | 24  |     | | 25  | script.src = 'https://elevenlabs.io/player/audioNativeHelper.js'; | | 26  | script.async = true; | | 27  | document.body.appendChild(script); | | 28  |     | | 29  | return () => { | | 30  | document.body.removeChild(script); | | 31  | };  | | 32  | }, []); | | 33  |     | | 34  | return ( | | 35  | <div | | 36  | id="elevenlabs-audionative-widget" | | 37  | data-height={size === 'small' ? '90' : '120'} | | 38  | data-width="100%" | | 39  | data-frameborder="no" | | 40  | data-scrolling="no" | | 41  | data-publicuserid={publicUserId} | | 42  | data-playerurl="https://elevenlabs.io/player/index.html" | | 43  | data-small={size === 'small' ? 'True' : 'False'} | | 44  | data-textcolor={textColorRgba ?? 'rgba(0, 0, 0, 1.0)'} | | 45  | data-backgroundcolor={backgroundColorRgba ?? 'rgba(255, 255, 255, 1.0)'} | | 46  | >   | | 47  | {children ? children : 'Elevenlabs AudioNative Player'} | | 48  | </div> | | 49  | );  | | 50  | };  | | 51  |     | | 52  | export default ElevenLabsAudioNative; |    `

The above component can be found on [GitHub](https://github.com/elevenlabs/elevenlabs-examples/blob/main/examples/audio-native/react/ElevenLabsAudioNative.tsx)
.

[2](/docs/product-guides/audio-tools/audio-native/react#use-the-audio-native-component)

### Use the Audio Native component

Before using the component on your page, you need to retrieve your public user ID from the original code snippet. Copy the contents of `data-publicuserid` from the embed code snippet and insert it into the `publicUserId` prop of the component.

page.tsx

`  |     |     | | --- | --- | | 1   | import { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative'; | | 2   |     | | 3   | export default function Page() { | | 4   | return ( | | 5   | <div> | | 6   | <h1>Your Page Title</h1> | | 7   |     | | 8   | // Insert the public user ID from the embed code snippet | | 9   | <ElevenLabsAudioNative publicUserId="<your-public-user-id>" /> | | 10  |     | | 11  | <p>Your page content...</p> | | 12  | </div> | | 13  | );  | | 14  | }   |    `

[3](/docs/product-guides/audio-tools/audio-native/react#customize-the-player-with-component-props)

### Customize the player with component props

The component props can be used to customize the player. For example, you can change the size, text color, and background color.

page.tsx

`  |     |     | | --- | --- | | 1   | import { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative'; | | 2   |     | | 3   | export default function Page() { | | 4   | return ( | | 5   | <div> | | 6   | <h1>Your Page Title</h1> | | 7   |     | | 8   | <ElevenLabsAudioNative | | 9   | publicUserId="<your-public-user-id>" | | 10  | size="small" | | 11  | textColorRgba="rgba(255, 255, 255, 1.0)" | | 12  | backgroundColorRgba="rgba(0, 0, 0, 1.0)" | | 13  | />  | | 14  |     | | 15  | <p>Your page content...</p> | | 16  | </div> | | 17  | );  | | 18  | }   |    `

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Professional Voice Cloning — ElevenLabs Documentation

## Creating a Professional Voice Clone

When cloning a voice, it’s important to consider what the AI has been trained on: which languages and what type of dataset. You can find more information about which languages each model has been trained on in our [help center](https://help.elevenlabs.io/hc/en-us/articles/17883183930129-What-models-do-you-offer-and-what-is-the-difference-between-them)
.

Read more about each individual model and their strengths in the [Models page](/docs/models)
).

## Guide

If you are unsure about what is permissible from a legal standpoint, please consult the [Terms of Service](https://elevenlabs.io/terms-of-use)
and our [AI Safety information](https://elevenlabs.io/safety)
for more information.

[1](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#navigate-to-the-instant-voice-cloning-page)

### Navigate to the Instant Voice Cloning page

In the ElevenLabs dashboard, select the “Voices” section on the left, then click “Add a new voice”.

From the modal, select “Professional Voice Clone”.

[2](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#upload-your-audio)

### Upload your audio

![Voice cloning IVC modal](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voices/voice-cloning/voices-pvc-creation.jpg)

Follow the on-screen instructions to label your voice clone and upload audio samples.

[3](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#verify-your-voice)

### Verify your voice

Once everything is recorded and uploaded, you will be asked to verify your voice. To ensure a smooth experience, please try to verify your voice using the same or similar equipment used to record the samples and in a tone and delivery that is similar to what was present in the samples. If you do not have access to the same equipment, try verifying the best you can. If it fails, you will have to reach out to support.

[4](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#use-your-voice-clone)

### Use your voice clone

Under the “Voices” section in the dashboard, select the “Personal” tab, then click on your voice clone to begin using it.

There are a few things to be mindful of before you start uploading your samples, and some steps that you need to take to ensure the best possible results.

[1](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#record-high-quality-audio)

### Record high quality audio

Professional Voice Cloning is highly accurate in cloning the samples used for its training. It will create a near-perfect clone of what it hears, including all the intricacies and characteristics of that voice, but also including any artifacts and unwanted audio present in the samples. This means that if you upload low-quality samples with background noise, room reverb/echo, or any other type of unwanted sounds like music on multiple people speaking, the AI will try to replicate all of these elements in the clone as well.

[2](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#ensure-theres-only-a-single-speaking-voice)

### Ensure there’s only a single speaking voice

Make sure there’s only a single speaking voice throughout the audio, as more than one speaker or excessive noise or anything of the above can confuse the AI. This confusion can result in the AI being unable to discern which voice to clone or misinterpreting what the voice actually sounds like because it is being masked by other sounds, leading to a less-than-optimal clone.

[3](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#provide-enough-material)

### Provide enough material

Make sure you have enough material to clone the voice properly. The bare minimum we recommend is 30 minutes of audio, but for the optimal result and the most accurate clone, we recommend closer to 2+ hours of audio. You might be able to get away with less, but at that point, we can’t vouch for the quality of the resulting clone.

[4](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#keep-the-style-consistent)

### Keep the style consistent

The speaking style in the samples you provide will be replicated in the output, so depending on what delivery you are looking for, the training data should correspond to that style (e.g. if you are looking to voice an audiobook with a clone of your voice, the audio you submit for training should be a recording of you reading a book in the tone of voice you want to use). It is better to just include one style in the uploaded samples for consistencies sake.

[5](/docs/product-guides/voices/voice-cloning/professional-voice-cloning#use-samples-speaking-the-language-you-want-the-pvc-to-be-used-for)

### Use samples speaking the language you want the PVC to be used for

It is best to use samples speaking where you are speaking the language that the PVC will mainly be used for. Of course, the AI can speak any language that we currently support. However, it is worth noting that if the voice itself is not native to the language you want the AI to speak - meaning you cloned a voice speaking a different language - it might have an accent from the original language and might mispronounce words and inflections. For instance, if you clone a voice speaking English and then want it to speak Spanish, it will very likely have an English accent when speaking Spanish. We only support cloning samples recorded in one of our supported languages, and the application will reject your sample if it is recorded in an unsupported language.

See the examples below for what to expect from a good and bad recording.

For now, we only allow you to clone your own voice. You will be asked to go through a verification process before submitting your fine-tuning request.

## Tips and suggestions

###### Professional Recording Equipment

#### Professional Recording Equipment

Use high-quality recording equipment for optimal results as the AI will clone everything about the audio. High-quality input = high-quality output. Any microphone will work, but an XLR mic going into a dedicated audio interface would be our recommendation. A few general recommendations on low-end would be something like an Audio Technica AT2020 or a Rode NT1 going into a Focusrite interface or similar.

###### Use a Pop-Filter

#### Use a Pop-Filter

Use a Pop-Filter when recording. This will minimize plosives when recording.

###### Microphone Distance

#### Microphone Distance

Position yourself at the right distance from the microphone - approximately two fists away from the mic is recommended, but it also depends on what type of recording you want.

###### Noise-Free Recording

#### Noise-Free Recording

Ensure that the audio input doesn’t have any interference, like background music or noise. The AI cloning works best with clean, uncluttered audio.

###### Room Acoustics

#### Room Acoustics

Preferably, record in an acoustically-treated room. This reduces unwanted echoes and background noises, leading to clearer audio input for the AI. You can make something temporary using a thick duvet or quilt to dampen the recording space.

###### Audio Pre-processing

#### Audio Pre-processing

Consider editing your audio beforehand if you’re aiming for a specific sound you want the AI to output. For instance, if you want a polished podcast-like output, pre-process your audio to match that quality, or if you have long pauses or many “uhm”s and “ahm”s between words as the AI will mimic those as well.

###### Volume Control

#### Volume Control

Maintain a consistent volume that’s loud enough to be clear but not so loud that it causes distortion. The goal is to achieve a balanced and steady audio level. The ideal would be between -23dB and -18dB RMS with a true peak of -3dB.

###### Sufficient Audio Length

#### Sufficient Audio Length

Provide at least 30 minutes of high-quality audio that follows the above guidelines for best results - preferably closer to 2+ hours of audio. The more quality data you can feed into the AI, the better the voice clone will be. The number of samples is irrelevant; the total runtime is what matters. However, if you plan to upload multiple hours of audio, it is better to split it into multiple ~30-minute samples. This makes it easier to upload.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Conversational AI dashboard — ElevenLabs Documentation

## Overview

The Agents Dashboard provides real-time insights into your Conversational AI agents. It displays performance metrics over customizable time periods. You can review data for individual agents or across your entire workspace.

## Analytics

You can monitor activity over various daily, weekly, and monthly time periods.

![Dashboard view showing last day metrics](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/conversational-ai/lastday.png)

Dashboard view for Last Day

![Dashboard view showing last month metrics](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/conversational-ai/lastmonth.png)

Dashboard view for Last Month

The dashboard can be toggled to show different metrics, including: number of calls, average duration, total cost, and average cost.

## Language Breakdown

A key benefit of Conversational AI is the ability to support multiple languages. The Language Breakdown section shows the percentage of calls (overall, or per-agent) in each language.

![Language breakdown showing percentage of calls in each language](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/conversational-ai/dashboard-language-breakdown.png)

Language Breakdown

## Active Calls

At the top left of the dashboard, the current number of active calls is displayed. This real-time counter reflects ongoing sessions for your workspace’s agents, and is also accessible via the API.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Zero retention mode (Enterprise) — ElevenLabs Documentation

## Background

By default, we retain data, in accordance with our Privacy Policy, to enhance our services, troubleshoot issues, and ensure the security of our systems. However, for some enterprise customers, we offer a “Zero Retention Mode” option for specific products. In this Zero Retention Mode, most data in requests and responses are immediately deleted once the request is completed.

## What is zero retention mode?

Zero Retention Mode provides an additional level of security and peace of mind for especially sensitive workflows. When enabled, logging of certain data points is restricted, including:

- TTS text input
- TTS audio output
- Voice Changer audio input
- Voice Changer audio output
- Email associated with the account generating the input in our logs

This data is related to the processing of the request, and can only be seen by the user doing the request and the volatile memory of the process serving the request. None of this data is sent at any point to a database where data is stored long term.

## Who has access to zero retention mode?

Enterprise customers can use zero Retention Mode. It is primarily intended for use by our customers in the healthcare and banking sector, and other customers who may use our services to process sensitive information.

## When can a customer use zero retention mode?

Zero Retention Mode is available to select enterprise customers. However, access to this feature may be restricted if ElevenLabs determines a customer’s use case to be high risk, if an account is flagged by an automated system for additional moderation or at ElevenLabs’ sole discretion. In such cases, the enterprise administrator will be promptly notified of the restriction.

## How does zero retention mode work?

Zero Retention Mode only works for API requests, specifically:

- **Text to Speech**: this covers the Text-to-Speech (TTS) API, including all endpoints beginning with `/v1/text-to-speech/` and the TTS websocket connection.
- **Voice Changer**: this covers the Voice Changer API, including all endpoints starting with `/v1/speech-to-speech/`.

After setup, check the request history to verify zero Retention Mode is enabled. If enabled, there should be no requests in the history.

Zero retention mode can be used by sending `enable_logging=false` with the product which supports it.

For example, in Text to Speech API, you can set the query parameter [enable_logging](https://elevenlabs.io/docs/api-reference/text-to-speech#parameter-enable-logging)
to False like this:

Pythoncurl

`  |     |     | | --- | --- | | $   | from elevenlabs import ElevenLabs | | >   |     | | >   | client = ElevenLabs( | | >   | api_key="YOUR_API_KEY", | | >   | )   | | >   |     | | >   | response = client.text_to_speech.convert( | | >   | voice_id=voice_id, | | >   | output_format="mp3_22050_32", | | >   | text=text, | | >   | model_id="eleven_turbo_v2", | | >   | enable_logging=False | | >   | )   |    `

## What products are configured for zero retention mode?

| Product                    | Type                 | Default Retention | Eligible for zero Retention |
| -------------------------- | -------------------- | ----------------- | --------------------------- |
| Text to Speech             | Text Input           | Enabled           | Yes                         |
|                            | Audio Output         | Enabled           | Yes                         |
| Voice Changer              | Audio Input          | Enabled           | Yes                         |
|                            | Audio Output         | Enabled           | Yes                         |
| Instant Voice Cloning      | Audio Samples        | Enabled           | No                          |
| Professional Voice Cloning | Audio Samples        | Enabled           | No                          |
| Dubbing                    | Audio/Video Input    | Enabled           | No                          |
|                            | Audio Output         | Enabled           | No                          |
| Projects                   | Text Input           | Enabled           | No                          |
|                            | Audio Output         | Enabled           | No                          |
| Conv AI                    | All Input and Output | Enabled           | No                          |

## What are some limitations of zero retention mode?

Troubleshooting and support for zero Retention Mode is limited. Because of the configuration, we will not be able to diagnose issues with TTS/STS generations. Debugging will be more difficult as a result.

## How retention works if zero retention mode is not active?

Customers by default have history preservation enabled. All customers can use the API to delete generations at any time. This action will immediately remove the corresponding audio and text from our database; however, debugging and moderation logs may still retain data related to the generation.

## Data backup (When zero retention mode is not used)

For any retained data, we regularly back up such data to prevent data loss in the event of any unexpected incidents. Following data deletion, database items are retained in backups for up to 30 days After this period, the data expires and is not recoverable.

## Account deletion (When zero retention mode is not used)

All data is deleted from our systems permanently when you delete your account. This includes all data associated with your account, such as API keys, request history, and any other data stored in your account. We also take commercially reasonable efforts to delete debugging data related to your account.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Troubleshooting — ElevenLabs Documentation

Our models are non-deterministic, meaning outputs can vary based on inputs. While we strive to enhance predictability, some variability is inherent. This guide outlines common issues and preventive measures.

## General

###### Inconsistencies in volume and quality

If the generated voice output varies in volume or tone, it is often due to inconsistencies in the voice clone training audio.

- **Apply compression**: Compress the training audio to reduce dynamic range and ensure consistent audio. Aim for a RMS between -23 dB and -18 dB and the true peak below -3 dB.
- **Background noise**: Ensure the training audio contains only the voice you want to clone — no music, noise, or pops. Background noise, sudden bursts of energy or consistent low-frequency energy can make the AI less stable.
- **Speaker consistency**: Ensure the speaker maintains a consistent distance from the microphone and avoids whispering or shouting. Variations can lead to inconsistent volume or tonality.
- **Audio length**:
  - **Instant Voice Cloning**: Use 1–2 minutes of consistent audio. Consistency in tonality, performance, accent, and quality is crucial.
  - **Professional Voice Cloning**: Use at least 30 minutes, ideally 2+ hours, of consistent audio for best results.

To minimize issues, consider breaking your text into smaller segments. This approach helps maintain consistent volume and reduces degradation over longer audio generations. Utilize our Studio feature to generate several smaller audio segments simultaneously, ensuring better quality and consistency.

Refer to our guides for optimizing Instant and Professional Voice Clones for best practices and advice.

###### Mispronunciation

The multilingual models may rarely mispronounce certain words, even in English. This issue appears to be somewhat arbitrary but seems to be voice and text-dependent. It occurs more frequently with certain voices and text, especially when using words that also appear in other languages.

- **Use Studio**: This feature helps minimize mispronunciation issues, which are more prevalent in longer text sections when using Speech Synthesis. While it won’t completely eliminate the problem, it can help avoid it and make it easier to regenerate specific sections without redoing the entire text.
- **Properly cloned voices**: Similar to addressing inconsistency issues, using a properly cloned voice in the desired languages can help reduce mispronunciation.
- **Specify pronunciation**: When using our Studio feature, consider specifying the pronunciation of certain words, such as character names and brand names, or how acronyms should be read. For more information, refer to the Pronunciation Dictionary section of our guide to Studio.

###### Language switching and accent drift

The AI can sometimes switch languages or accents throughout a single generation, especially if that generation is longer in length. This issue is similar to the mispronunciation problem and is something we are actively working to improve.

- **Use properly cloned voices**: Using an Instant Voice Clone or a Professional Voice Clone trained on high-quality, consistent audio in the desired language can help mitigate this issue. Pairing this with the Studio feature can further enhance stability.
- **Understand voice limitations**: Default and generated voices are primarily English and may carry an English accent when used for other languages. Cloning a voice that speaks the target language with the desired accent provides the AI with better context, reducing the likelihood of language switching.
- **Language selection**: Currently, the AI determines the language based on the input text. Writing in the desired language is crucial, especially when using pre-made voices that are English-based, as they may introduce an English accent.
- **Optimal text length**: The AI tends to maintain a consistent accent over shorter text segments. For best results, keep text generations under 800-900 characters when using Text-to-Speech. The Studio workflow can help manage longer texts by breaking them into smaller, more manageable segments.

###### Mispronounced numbers, symbols or acronyms

The models may mispronounce certain numbers, symbols and acronyms. For example, the numbers “1, 2, 3” might be pronounced as “one,” “two,” “three” in English. To ensure correct pronunciation in another language, write them out phonetically or in words as you want them to be spoken.

- **Example**: For the number “1” to be pronounced in French, write “un.”
- **Symbols**: Specify how symbols should be read, e.g., ”$” as “dollar” or “euro.”
- **Acronyms**: Spell out acronyms phonetically.

###### Corrupt speech

Corrupt speech is a rare issue where the model generates muffled or distorted audio. This occurs unpredictably, and we have not identified a cause. If encountered, regenerate the section to resolve the issue.

###### Audio degradation over longer generations

Audio quality may degrade during extended text-to-speech conversions, especially with the Multilingual v1 model. To mitigate this, break text into sections under 800 characters.

- **Voice Selection**: Some voices are more susceptible to degradation. Use high-quality samples for cloned voices to minimize artifacts.
- **Stability and Similarity**: Adjust these settings to influence voice behavior and artifact prominence. Hover over each setting for more details.

###### Style exaggeration

For some voices, this voice setting can lead to instability, including inconsistent speed, mispronunciation and the addition of extra sounds. We recommend keeping this setting at 0, especially if you find you are experiencing these issues in your generated audio.

## Studio (formerly Projects)

###### File imports

The import function attempts to import the file you provide to the website. Given the variability in website structures and book formatting, including images, always verify the import for accuracy.

- **Chapter images**: If a book’s chapters start with an image as the first letter, the AI may not recognize the letter. Manually add the letter to each chapter.
- **Paragraph structure**: If text imports as a single long paragraph instead of following the original book’s structure, it may not function correctly. Ensure the text maintains its original line breaks. If issues persist, try copying and pasting. If this fails, the text format may need conversion or rewriting.
- **Preferred format**: EPUB is the recommended file format for creating a project in Studio. A well-structured EPUB will automatically split each chapter in Studio, facilitating navigation. Ensure each chapter heading is formatted as “Heading 1” for proper recognition.

Always double-check imported content for accuracy and structure.

###### Glitches between paragraphs

Occasionally, glitches or sharp breaths may occur between paragraphs. This is rare and differs from standard Text to Speech issues. If encountered, regenerate the preceding paragraph, as the problem often originates there.

If an issue persists after following this troubleshooting guide, please [contact our support team](https://help.elevenlabs.io/hc/en-us/requests/new?ticket_form_id=13145996177937)
.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with WordPress — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/word-press#install-the-wpcode-plugin)

### Install the WPCode plugin

Install the [WPCode plugin](https://wpcode.com/)
into your WordPress website to embed HTML code.

[2](/docs/product-guides/audio-tools/audio-native/word-press#create-a-new-code-snippet)

### Create a new code snippet

In the WordPress admin console, click on “Code Snippets”. Add the Audio Native embed code to the new code snippet.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wordpress-1.webp)

Pick “Auto Insert” for the insert method and set the location to be “Insert Before Content”.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wordpress-2.webp)

[3](/docs/product-guides/audio-tools/audio-native/word-press#publish-your-changes)

### Publish your changes

Finally, publish your changes and navigate to the live version of the blog post. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-wordpress-3.webp)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Audio Native with Framer — ElevenLabs Documentation

Follow the steps in the [Audio Native overview](/docs/product-guides/audio-tools/audio-native)
to get started with Audio Native before continuing with this guide.

[1](/docs/product-guides/audio-tools/audio-native/framer#add-audio-native-script-to-your-page)

### Add Audio Native script to your page

Navigate to your Framer page, sign in and go to your site settings. From the Audio Native embed code, extract the `<script>` tag and paste it in the “End of `<body>` tag” field.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-framer-1.webp)

[2](/docs/product-guides/audio-tools/audio-native/framer#add-an-embed-element)

### Add an Embed Element

On your Framer blog page, add an Embed Element from Utilities.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-framer-2.webp)

In the config for the Embed Element, switch the type to HTML and paste the `<div>` snippet from the Audio Native embed code into the HTML box.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-framer-3.webp)

[3](/docs/product-guides/audio-tools/audio-native/framer#publish-your-changes)

### Publish your changes

Finally, publish your changes and navigate to the live version of your page. You should see a message to let you know that the Audio Native project is being created. After a few minutes the text in your blog will be converted to an audio article and the embedded audio player will appear.

![Audio Native](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/audio-native/audio-native-framer-4.webp)

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# Voiceover studio — ElevenLabs Documentation

![Voiceover studio](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/product-guides/voiceover-studio/voiceover-studio.png)

## Overview

Voiceover Studio combines the audio timeline with our Sound Effects feature, giving you the ability to write a dialogue between any number of speakers, choose those speakers, and intertwine your own creative sound effects anywhere you like.

## Guide

[1](/docs/product-guides/audio-tools/voiceover-studio#navigate-to-the-voiceover-studio)

### Navigate to the Voiceover studio

In the ElevenLabs dashboard, click on the “Voiceover Studio” option in the sidebar under “Audio Tools”.

[2](/docs/product-guides/audio-tools/voiceover-studio#create-a-new-voiceover)

### Create a new voiceover

Click the “Create a new voiceover” button to begin. You can optionally upload video or audio to create a voiceover from.

[3](/docs/product-guides/audio-tools/voiceover-studio#modify-the-voiceover-with-the-timeline)

### Modify the voiceover with the timeline

On the bottom half of your screen, use the timeline to add and edit voiceover clips plus add sound effects.

[4](/docs/product-guides/audio-tools/voiceover-studio#export-your-voiceover)

### Export your voiceover

Once you’re happy with your voiceover, click the “Export” button in the bottom right, choose the format you want and either view or download your voiceover.

## FAQ

###### How does the timeline work?

### Timeline

The timeline is a linear representation of your Voiceover project. Each row represents a track, and on the far left section you have the track information for voiceover or SFX tracks. In the middle, you can create the clips that represent when a voice is speaking or a SFX is playing. On the right-hand side, you have the settings for the currently selected clip.

###### How do I add tracks?

### Adding Tracks

To add a track, click the “Add Track” button in the bottom left of the timeline. You can choose to add a voiceover track or an SFX track.

There are three types of tracks you can add in the studio: Voiceover tracks, SFX tracks and uploaded audio.

- **Voiceover Tracks:** Voiceover tracks create new Speakers. You can click and add clips on the timeline wherever you like. After creating a clip, start writing your desired text on the speaker cards above and click “Generate”. Similar to Dubbing Studio, you will also see a little cogwheel on each Speaker track - simply click on it to adjust the voice settings or replace any speaker with a voice directly from your Voices - including your own Professional Voice Clone if you have created one.
- **SFX Tracks:** Add a SFX track, then click anywhere on that track to create a SFX clip. Similar to our independent SFX feature, simply start writing your prompt in the Speaker card above and click “Generate” to create your new SFX audio. You can lengthen or shorten SFX clips and move them freely around your timeline to fit your project - make sure to press the “stale” button if you do so.
- **Uploaded Audio:** Add an audio track including background music or sound effects. It’s best to avoid uploading audio with speakers, as any speakers in this track will not be detected, so you won’t be able to translate or correct them.

###### How does this differ from Dubbing Studio?

### Key Differences from Dubbing Studio

If you chose not to upload a video when you created your Voiceover project, then the entire timeline is yours to work with and there are no time constraints. This differs from Dubbing Studio as it gives you a lot more freedom to create what you want and adjust the timing more easily.

When you Add a Voiceover Track, you will instantly be able to create clips on your timeline. Once you create a Voiceover clip, begin by writing in the Speaker Card above. After generating that audio, you will notice your clip on the timeline will automatically adjust its length based on the text prompt - this is called “Dynamic Generation”. This option is also available in Dubbing Studio by right-clicking specific clips, but because syncing is more important with dubbed videos, the default generation type there is “Fixed Generation,” meaning the clips’ lengths are not affected.

###### How are credits deducted with Voiceover Studio?

### Credit Costs

Voiceover Studio does not deduct credits to create your initial project. Credits are deducted every time material is generated. Similar to Speech-Synthesis, credit costs for Voiceover Clips are based on the length of the text prompt. SFX clips will deduct 80 credits per generation.

If you choose to Dub (translate) your Voiceover Project into different languages, this will also cost additional credits depending on how much material needs to be generated. The cost is 1 credit per character for the translation, plus the cost of generating the new audio for the additional languages.

###### How do I upload a script?

### Uploading Scripts

With Voiceover Studio, you have the option to upload a script for your project as a CSV file. You can either include speaker name and line, or speaker name, line, start time and end time. To upload a script, click on the cog icon in the top right hand corner of the page and select “Import Script”.

Scripts should be provided in the following format:

`  speaker,line,  `

Example input:

`  |     | | --- | | speaker,line, | | Joe,"Hey!" | | Maria,"Oh, hi Joe! It's been a while." |    `

You can also provide start and end times for each line in the following format:

`  speaker,line,start_time,end_time  `

Example input:

`  |     | | --- | | speaker,line,start_time,end_time | | Joe,"Hey!",0.1,1.5 | | Maria,"Oh, hi Joe! It's been a while.",1.6,2.0 |    `

Once your script has imported, make sure to assign voices to each speaker before you generate the audio. To do this, click the cog icon in the information for each track, on the left.

If you don’t specify start and end times for your clips, Voiceover Studio will estimate how long each clip will be, and distribute them along your timeline.

###### What's the difference between Dynamic Duration and Fixed Duration?

### Dynamic Duration

By default, Voiceover Studio uses Dynamic Duration, which means that the length of the clip will vary depending on the text input and the voice used. This ensures that the audio sounds as natural as possible, but it means that the length of the clip might change after the audio has been generated. You can easily reposition your clips along the timeline once they have been generated to get a natural sounding flow. If you click “Generate Stale Audio”, or use the generate button on the clip, the audio will be generated using Dynamic Duration.

This also applies if you do specify the start and end time for your clips. The clips will generate based on the start time you specify, but if you use the default Dynamic Duration, the end time is likely to change once you generate the audio.

### Fixed Duration

If you need the clip to remain the length specified, you can choose to generate with Fixed Duration instead. To do this, you need to right click on the clip and select “Generate Audio Fixed Duration”. This will adjust the length of the generated audio to fit the specified length of the clip. This could lead to the audio sounding unnaturally quick or slow, depending on the length of your clip.

If you want to generate multiple clips at once, you can use shift + click to select multiple clips for a speaker at once, then right click on one of them to select “Generate Audio Fixed Duration” for all selected clips.

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---

# End call — ElevenLabs Documentation

The **End Call** tool is added to agents created in the ElevenLabs dashboard by default. For agents created via API or SDK, if you would like to enable the End Call tool, you must add it manually as a system tool in your agent configuration. [See API Implementation below](/docs/conversational-ai/customization/tools/server-tools/end-call#api-implementation)
for details.

![End call](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/2025-02-22T07:18:06.652Z/assets/images/conversational-ai/end-call-tool.png)

## Overview

The **End Call** tool allows your conversational agent to terminate a call with the user. This is a system tool that provides flexibility in how and when calls are ended.

## Functionality

- **Default behavior**: The tool can operate without any user-defined prompts, ending the call when the conversation naturally concludes.
- **Custom prompts**: Users can specify conditions under which the call should end. For example:
  - End the call if the user says “goodbye.”
  - Conclude the call when a specific task is completed.

### API Implementation

When creating an agent via API, you can add the End Call tool to your agent configuration. It should be defined as a system tool:

PythonJavaScriptBash

`  |     |     | | --- | --- | | 1   | from elevenlabs import ( | | 2   | ConversationalConfig, | | 3   | ElevenLabs, | | 4   | AgentConfig, | | 5   | PromptAgent, | | 6   | PromptAgentToolsItem_System | | 7   | )   | | 8   |     | | 9   | # Initialize the client | | 10  | client = ElevenLabs(api_key="YOUR_API_KEY") | | 11  |     | | 12  | # Create the end call tool | | 13  | end_call_tool = PromptAgentToolsItem_System( | | 14  | name="end_call", | | 15  | description=""  # Optional: Customize when the tool should be triggered | | 16  | )   | | 17  |     | | 18  | # Create the agent configuration | | 19  | conversation_config = ConversationalConfig( | | 20  | agent=AgentConfig( | | 21  | prompt=PromptAgent( | | 22  | tools=[end_call_tool] | | 23  | )   | | 24  | )   | | 25  | )   | | 26  |     | | 27  | # Create the agent | | 28  | response = client.conversational_ai.create_agent( | | 29  | conversation_config=conversation_config | | 30  | )   |    `

Leave the description blank to use the default end call prompt.

## Example prompts

**Example 1**

[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io)

---
